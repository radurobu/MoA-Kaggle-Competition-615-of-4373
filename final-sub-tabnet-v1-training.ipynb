{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.030665,
     "end_time": "2020-11-26T20:37:05.489641",
     "exception": false,
     "start_time": "2020-11-26T20:37:05.458976",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Source \n",
    "\n",
    "- Pytorch 1.6 : https://pytorch.org/docs/stable/\n",
    "- iterative-stratification : https://github.com/trent-b/iterative-stratification for stratified K fold multilabel\n",
    "- TabNet : https://arxiv.org/pdf/1908.07442.pdf https://github.com/dreamquark-ai/tabnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.029898,
     "end_time": "2020-11-26T20:37:05.548740",
     "exception": false,
     "start_time": "2020-11-26T20:37:05.518842",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Approach :\n",
    "Inference script : \n",
    "https://www.kaggle.com/ludovick/introduction-to-tabnet-kfold-10-inference\n",
    "\n",
    "The Regressor class of TabNet is used as it allows multilabels outputs.\n",
    "- Stratified K Fold (10 folds) or shufflesplit\n",
    "- BCE Loss\n",
    "- TabNet Regressor Model\n",
    "- Supervised learning\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.028767,
     "end_time": "2020-11-26T20:37:05.606174",
     "exception": false,
     "start_time": "2020-11-26T20:37:05.577407",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Radu Approach:\n",
    "* Label Smoothing: 0.001\n",
    "* Folds: 10\n",
    "* Patience: 50\n",
    "* Seed: 3\n",
    "* **REMOVED** Added Frequency Encoding Variables\n",
    "* **REMOVED** PCA for Frequency variables (10 components)\n",
    "* Removed Label Smoothing from vaidation inference\n",
    "* Added Min Max Clipping\n",
    "* **REMOVED** Added Extra_features (mean sum std etc.)\n",
    "* **REMOVED** Drop c62\n",
    "* **REMOVED** Added Kmeans Clustres\n",
    "* Stratified validation by drug id (Chris Deotte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-11-26T20:37:05.673801Z",
     "iopub.status.busy": "2020-11-26T20:37:05.673106Z",
     "iopub.status.idle": "2020-11-26T20:37:05.699810Z",
     "shell.execute_reply": "2020-11-26T20:37:05.699139Z"
    },
    "papermill": {
     "duration": 0.063883,
     "end_time": "2020-11-26T20:37:05.699949",
     "exception": false,
     "start_time": "2020-11-26T20:37:05.636066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/iterative-stratification/iterative_stratification-0.1.6-py3-none-any.whl\n",
      "/kaggle/input/lish-moa/train_features.csv\n",
      "/kaggle/input/lish-moa/train_drug.csv\n",
      "/kaggle/input/lish-moa/test_features.csv\n",
      "/kaggle/input/lish-moa/train_targets_nonscored.csv\n",
      "/kaggle/input/lish-moa/sample_submission.csv\n",
      "/kaggle/input/lish-moa/train_targets_scored.csv\n",
      "/kaggle/input/pytorch16gpu/torch-1.6.0cu101-cp37-cp37m-linux_x86_64.whl\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/.dockerignore\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/forest_example.ipynb\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/.gitatttributes\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/.gitignore\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/CHANGELOG.md\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/renovate.json\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/census_example.ipynb\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/Dockerfile\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/pyproject.toml\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/LICENSE\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/Makefile\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/regression_example.ipynb\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/poetry.lock\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/.editorconfig\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/.flake8\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/Dockerfile_gpu\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/multi_regression_example.ipynb\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/README.md\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/release-script/Dockerfile_changelog\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/release-script/do-release.sh\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/release-script/prepare-release.sh\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/.github/PULL_REQUEST_TEMPLATE.md\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/.github/ISSUE_TEMPLATE/bug_report.md\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/.github/ISSUE_TEMPLATE/feature_request.md\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/pytorch_tabnet/tab_network.py\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/pytorch_tabnet/sparsemax.py\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/pytorch_tabnet/multiclass_utils.py\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/pytorch_tabnet/tab_model.py\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/pytorch_tabnet/utils.py\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/.circleci/config.yml\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.044542,
     "end_time": "2020-11-26T20:37:05.786023",
     "exception": false,
     "start_time": "2020-11-26T20:37:05.741481",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Install package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-11-26T20:37:05.856282Z",
     "iopub.status.busy": "2020-11-26T20:37:05.855402Z",
     "iopub.status.idle": "2020-11-26T20:38:15.844126Z",
     "shell.execute_reply": "2020-11-26T20:38:15.843256Z"
    },
    "papermill": {
     "duration": 70.023707,
     "end_time": "2020-11-26T20:38:15.844274",
     "exception": false,
     "start_time": "2020-11-26T20:37:05.820567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/pytorch16gpu/torch-1.6.0cu101-cp37-cp37m-linux_x86_64.whl\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch==1.6.0cu101) (0.18.2)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch==1.6.0cu101) (1.18.5)\r\n",
      "Installing collected packages: torch\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 1.5.1\r\n",
      "    Uninstalling torch-1.5.1:\r\n",
      "      Successfully uninstalled torch-1.5.1\r\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\r\n",
      "\r\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\r\n",
      "\r\n",
      "kornia 0.3.2 requires torch<1.6.0,>=1.5.0, but you'll have torch 1.6.0+cu101 which is incompatible.\r\n",
      "allennlp 1.0.0 requires torch<1.6.0,>=1.5.0, but you'll have torch 1.6.0+cu101 which is incompatible.\u001b[0m\r\n",
      "Successfully installed torch-1.6.0+cu101\r\n"
     ]
    }
   ],
   "source": [
    "! pip install ../input/pytorch16gpu/torch-1.6.0cu101-cp37-cp37m-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-11-26T20:38:15.948511Z",
     "iopub.status.busy": "2020-11-26T20:38:15.947606Z",
     "iopub.status.idle": "2020-11-26T20:38:45.232742Z",
     "shell.execute_reply": "2020-11-26T20:38:45.231758Z"
    },
    "papermill": {
     "duration": 29.340954,
     "end_time": "2020-11-26T20:38:45.232869",
     "exception": false,
     "start_time": "2020-11-26T20:38:15.891915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/iterative-stratification/iterative_stratification-0.1.6-py3-none-any.whl\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.6) (1.4.1)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.6) (0.23.2)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.6) (1.18.5)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->iterative-stratification==0.1.6) (2.1.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->iterative-stratification==0.1.6) (0.14.1)\r\n",
      "Installing collected packages: iterative-stratification\r\n",
      "Successfully installed iterative-stratification-0.1.6\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ../input/iterative-stratification/iterative_stratification-0.1.6-py3-none-any.whl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.035737,
     "end_time": "2020-11-26T20:38:45.305954",
     "exception": false,
     "start_time": "2020-11-26T20:38:45.270217",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "[](http://)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-11-26T20:38:45.380952Z",
     "iopub.status.busy": "2020-11-26T20:38:45.380030Z",
     "iopub.status.idle": "2020-11-26T20:38:45.382636Z",
     "shell.execute_reply": "2020-11-26T20:38:45.382059Z"
    },
    "papermill": {
     "duration": 0.041349,
     "end_time": "2020-11-26T20:38:45.382755",
     "exception": false,
     "start_time": "2020-11-26T20:38:45.341406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../input/tabnetdevelop/tabnet-develop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.052033,
     "end_time": "2020-11-26T20:38:45.476806",
     "exception": false,
     "start_time": "2020-11-26T20:38:45.424773",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is TabNet ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.035025,
     "end_time": "2020-11-26T20:38:45.546960",
     "exception": false,
     "start_time": "2020-11-26T20:38:45.511935",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "TabNet is a Deep Neural Network for tabular data and was designed to learn in a similar way than decision tree based models, in order to have their benefits : ***interpretability*** and ***sparse feature selection***. TabNet uses ***sequential attention to choose which features to reason from at each decision step***, enabling interpretability and better learning (as the learning capacity is used for the most salient/important features). The feature selection is instancewise, so it can be different for each input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.034429,
     "end_time": "2020-11-26T20:38:45.616053",
     "exception": false,
     "start_time": "2020-11-26T20:38:45.581624",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "![Example of features selections with multiples steps](https://miro.medium.com/max/700/0*xGrnkDqPkDmC_MWS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.034927,
     "end_time": "2020-11-26T20:38:45.687271",
     "exception": false,
     "start_time": "2020-11-26T20:38:45.652344",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "TabNet can use *categoricals* and *numericals features*. There are no global normalization of the inputs, instead of that, a batch normalization is applied. The features obtained is then used at each step. Two parts can be identified at each step (fig a) :\n",
    "- **<font size=\"3\">the feature selection</font>** : the feature selection is based on a **learnable mask M[i]** which allows soft selection of the important feature. The mask is obtained with the help of **an attentive transformer** (fig d), using the processed features from the previous step a[i-1]. A sparsemax function is used to encourage the sparsity : \n",
    "**M[i] = sparsemax(P[i-1] . $h_i$ a[i-1])** where P[i] is the prior scale term and allow to control how much a feature has been used in previous steps; $h_i$ is a trainable function (FC layer) . (More details in the paper)\n",
    "- **<font size=\"3\">the feature processing</font>** : the features are processed with a feature transformer block (fig c) and then split for the decision step outputs and the information for the subsequence step, such as : \n",
    "**[ d[i], a[i]] = $f_i$ (M[i] . f)** with f the inputs data \"normalized\" by the BN, **M[i]** the learnable mask computed by the attentive transformer, $f_i$ the feature transformer block. **a[i]** is then used in the next step by the attentive transformer, **d[i]** is used to compute the overall decision embedding $d_{out}$, such as **$d_{out}$ = $\\sum_{i=0}^{steps}(ReLU(d[i])$** . \n",
    "\n",
    "The output is then computed such as : **Outputs = F( $W_{final}$ $d_{out}$ )** with *F activation function (softmax, sigmoid, etc) and $W_{final}$ a FC layer.*\n",
    "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F2234817%2Fa08525247089146ce7e1bfdcfe256a2f%2FArchitecture.PNG?generation=1590581898108061&alt=media)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.033632,
     "end_time": "2020-11-26T20:38:45.755217",
     "exception": false,
     "start_time": "2020-11-26T20:38:45.721585",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# For this competition : \n",
    "TabNet can have multiple advantages in this competition:\n",
    "- it can use gpu, so the training can be quite fast, if we limits the number of epochs and steps\n",
    "- it can use multilabel prediction, so only one model can be trained contrary to boosting methods.\n",
    "\n",
    "Even if the model does not give you the best results in CV/LB compared to others approaches, it can still be a good model to add in an ensembling because it is quite fast to train and can add different capabilities, so make the ensembling more exhaustif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-11-26T20:38:45.836541Z",
     "iopub.status.busy": "2020-11-26T20:38:45.835627Z",
     "iopub.status.idle": "2020-11-26T20:38:47.359992Z",
     "shell.execute_reply": "2020-11-26T20:38:47.361121Z"
    },
    "papermill": {
     "duration": 1.571151,
     "end_time": "2020-11-26T20:38:47.361315",
     "exception": false,
     "start_time": "2020-11-26T20:38:45.790164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T20:38:47.863285Z",
     "iopub.status.busy": "2020-11-26T20:38:47.862307Z",
     "iopub.status.idle": "2020-11-26T20:38:47.866923Z",
     "shell.execute_reply": "2020-11-26T20:38:47.867592Z"
    },
    "papermill": {
     "duration": 0.447014,
     "end_time": "2020-11-26T20:38:47.867807",
     "exception": false,
     "start_time": "2020-11-26T20:38:47.420793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        \n",
    "seed_everything(62)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.050773,
     "end_time": "2020-11-26T20:38:47.969962",
     "exception": false,
     "start_time": "2020-11-26T20:38:47.919189",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T20:38:48.082568Z",
     "iopub.status.busy": "2020-11-26T20:38:48.081727Z",
     "iopub.status.idle": "2020-11-26T20:38:54.771757Z",
     "shell.execute_reply": "2020-11-26T20:38:54.770629Z"
    },
    "papermill": {
     "duration": 6.750167,
     "end_time": "2020-11-26T20:38:54.771882",
     "exception": false,
     "start_time": "2020-11-26T20:38:48.021715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/lish-moa/train_features.csv')\n",
    "train_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n",
    "train_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n",
    "test_features = pd.read_csv('../input/lish-moa/test_features.csv')\n",
    "submission = pd.read_csv('../input/lish-moa/sample_submission.csv')\n",
    "\n",
    "remove_vehicle = True\n",
    "\n",
    "if remove_vehicle:\n",
    "    train_features = train.loc[train['cp_type']=='trt_cp'].reset_index(drop=True)\n",
    "    train_targets_scored = train_targets_scored.loc[train['cp_type']=='trt_cp'].reset_index(drop=True)\n",
    "    train_targets_nonscored = train_targets_nonscored.loc[train['cp_type']=='trt_cp'].reset_index(drop=True)\n",
    "else:\n",
    "    train_features = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T20:38:54.852224Z",
     "iopub.status.busy": "2020-11-26T20:38:54.849069Z",
     "iopub.status.idle": "2020-11-26T20:38:54.893841Z",
     "shell.execute_reply": "2020-11-26T20:38:54.894409Z"
    },
    "papermill": {
     "duration": 0.087417,
     "end_time": "2020-11-26T20:38:54.894557",
     "exception": false,
     "start_time": "2020-11-26T20:38:54.807140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>cp_type</th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-1</th>\n",
       "      <th>g-2</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-5</th>\n",
       "      <th>...</th>\n",
       "      <th>c-90</th>\n",
       "      <th>c-91</th>\n",
       "      <th>c-92</th>\n",
       "      <th>c-93</th>\n",
       "      <th>c-94</th>\n",
       "      <th>c-95</th>\n",
       "      <th>c-96</th>\n",
       "      <th>c-97</th>\n",
       "      <th>c-98</th>\n",
       "      <th>c-99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_000644bb2</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D1</td>\n",
       "      <td>1.0620</td>\n",
       "      <td>0.5577</td>\n",
       "      <td>-0.2479</td>\n",
       "      <td>-0.6208</td>\n",
       "      <td>-0.1944</td>\n",
       "      <td>-1.0120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2862</td>\n",
       "      <td>0.2584</td>\n",
       "      <td>0.8076</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>-0.1912</td>\n",
       "      <td>0.6584</td>\n",
       "      <td>-0.3981</td>\n",
       "      <td>0.2139</td>\n",
       "      <td>0.3801</td>\n",
       "      <td>0.4176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_000779bfc</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>72</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.0743</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.2991</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>1.0190</td>\n",
       "      <td>0.5207</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4265</td>\n",
       "      <td>0.7543</td>\n",
       "      <td>0.4708</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.2957</td>\n",
       "      <td>0.4899</td>\n",
       "      <td>0.1522</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>0.6077</td>\n",
       "      <td>0.7371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_000a6266a</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>48</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.6280</td>\n",
       "      <td>0.5817</td>\n",
       "      <td>1.5540</td>\n",
       "      <td>-0.0764</td>\n",
       "      <td>-0.0323</td>\n",
       "      <td>1.2390</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.7250</td>\n",
       "      <td>-0.6297</td>\n",
       "      <td>0.6103</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>-1.3240</td>\n",
       "      <td>-0.3174</td>\n",
       "      <td>-0.6417</td>\n",
       "      <td>-0.2187</td>\n",
       "      <td>-1.4080</td>\n",
       "      <td>0.6931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_0015fd391</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>48</td>\n",
       "      <td>D1</td>\n",
       "      <td>-0.5138</td>\n",
       "      <td>-0.2491</td>\n",
       "      <td>-0.2656</td>\n",
       "      <td>0.5288</td>\n",
       "      <td>4.0620</td>\n",
       "      <td>-0.8095</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0990</td>\n",
       "      <td>-0.6441</td>\n",
       "      <td>-5.6300</td>\n",
       "      <td>-1.3780</td>\n",
       "      <td>-0.8632</td>\n",
       "      <td>-1.2880</td>\n",
       "      <td>-1.6210</td>\n",
       "      <td>-0.8784</td>\n",
       "      <td>-0.3876</td>\n",
       "      <td>-0.8154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_001626bd3</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>72</td>\n",
       "      <td>D2</td>\n",
       "      <td>-0.3254</td>\n",
       "      <td>-0.4009</td>\n",
       "      <td>0.9700</td>\n",
       "      <td>0.6919</td>\n",
       "      <td>1.4180</td>\n",
       "      <td>-0.8244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.6670</td>\n",
       "      <td>1.0690</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>-0.3031</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>0.2885</td>\n",
       "      <td>-0.3786</td>\n",
       "      <td>0.7125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21943</th>\n",
       "      <td>id_fff8c2444</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>72</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.1608</td>\n",
       "      <td>-1.0500</td>\n",
       "      <td>0.2551</td>\n",
       "      <td>-0.2239</td>\n",
       "      <td>-0.2431</td>\n",
       "      <td>0.4256</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0789</td>\n",
       "      <td>0.3538</td>\n",
       "      <td>0.0558</td>\n",
       "      <td>0.3377</td>\n",
       "      <td>-0.4753</td>\n",
       "      <td>-0.2504</td>\n",
       "      <td>-0.7415</td>\n",
       "      <td>0.8413</td>\n",
       "      <td>-0.4259</td>\n",
       "      <td>0.2434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21944</th>\n",
       "      <td>id_fffb1ceed</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D2</td>\n",
       "      <td>0.1394</td>\n",
       "      <td>-0.0636</td>\n",
       "      <td>-0.1112</td>\n",
       "      <td>-0.5080</td>\n",
       "      <td>-0.4713</td>\n",
       "      <td>0.7201</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1969</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>-0.8121</td>\n",
       "      <td>0.3434</td>\n",
       "      <td>0.5372</td>\n",
       "      <td>-0.3246</td>\n",
       "      <td>0.0631</td>\n",
       "      <td>0.9171</td>\n",
       "      <td>0.5258</td>\n",
       "      <td>0.4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21945</th>\n",
       "      <td>id_fffb70c0c</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D2</td>\n",
       "      <td>-1.3260</td>\n",
       "      <td>0.3478</td>\n",
       "      <td>-0.3743</td>\n",
       "      <td>0.9905</td>\n",
       "      <td>-0.7178</td>\n",
       "      <td>0.6621</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4286</td>\n",
       "      <td>0.4426</td>\n",
       "      <td>0.0423</td>\n",
       "      <td>-0.3195</td>\n",
       "      <td>-0.8086</td>\n",
       "      <td>-0.9798</td>\n",
       "      <td>-0.2084</td>\n",
       "      <td>-0.1224</td>\n",
       "      <td>-0.2715</td>\n",
       "      <td>0.3689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21946</th>\n",
       "      <td>id_fffcb9e7c</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.6660</td>\n",
       "      <td>0.2324</td>\n",
       "      <td>0.4392</td>\n",
       "      <td>0.2044</td>\n",
       "      <td>0.8531</td>\n",
       "      <td>-0.0343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1105</td>\n",
       "      <td>0.4258</td>\n",
       "      <td>-0.2012</td>\n",
       "      <td>0.1506</td>\n",
       "      <td>1.5230</td>\n",
       "      <td>0.7101</td>\n",
       "      <td>0.1732</td>\n",
       "      <td>0.7015</td>\n",
       "      <td>-0.6290</td>\n",
       "      <td>0.0740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21947</th>\n",
       "      <td>id_ffffdd77b</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>72</td>\n",
       "      <td>D1</td>\n",
       "      <td>-0.8598</td>\n",
       "      <td>1.0240</td>\n",
       "      <td>-0.1361</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>-0.3611</td>\n",
       "      <td>-3.6750</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.3890</td>\n",
       "      <td>-1.7450</td>\n",
       "      <td>-6.6300</td>\n",
       "      <td>-4.0950</td>\n",
       "      <td>-7.3860</td>\n",
       "      <td>-1.4160</td>\n",
       "      <td>-3.5770</td>\n",
       "      <td>-0.4775</td>\n",
       "      <td>-2.1500</td>\n",
       "      <td>-4.2520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21948 rows Ã— 876 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             sig_id cp_type  cp_time cp_dose     g-0     g-1     g-2     g-3  \\\n",
       "0      id_000644bb2  trt_cp       24      D1  1.0620  0.5577 -0.2479 -0.6208   \n",
       "1      id_000779bfc  trt_cp       72      D1  0.0743  0.4087  0.2991  0.0604   \n",
       "2      id_000a6266a  trt_cp       48      D1  0.6280  0.5817  1.5540 -0.0764   \n",
       "3      id_0015fd391  trt_cp       48      D1 -0.5138 -0.2491 -0.2656  0.5288   \n",
       "4      id_001626bd3  trt_cp       72      D2 -0.3254 -0.4009  0.9700  0.6919   \n",
       "...             ...     ...      ...     ...     ...     ...     ...     ...   \n",
       "21943  id_fff8c2444  trt_cp       72      D1  0.1608 -1.0500  0.2551 -0.2239   \n",
       "21944  id_fffb1ceed  trt_cp       24      D2  0.1394 -0.0636 -0.1112 -0.5080   \n",
       "21945  id_fffb70c0c  trt_cp       24      D2 -1.3260  0.3478 -0.3743  0.9905   \n",
       "21946  id_fffcb9e7c  trt_cp       24      D1  0.6660  0.2324  0.4392  0.2044   \n",
       "21947  id_ffffdd77b  trt_cp       72      D1 -0.8598  1.0240 -0.1361  0.7952   \n",
       "\n",
       "          g-4     g-5  ...    c-90    c-91    c-92    c-93    c-94    c-95  \\\n",
       "0     -0.1944 -1.0120  ...  0.2862  0.2584  0.8076  0.5523 -0.1912  0.6584   \n",
       "1      1.0190  0.5207  ... -0.4265  0.7543  0.4708  0.0230  0.2957  0.4899   \n",
       "2     -0.0323  1.2390  ... -0.7250 -0.6297  0.6103  0.0223 -1.3240 -0.3174   \n",
       "3      4.0620 -0.8095  ... -2.0990 -0.6441 -5.6300 -1.3780 -0.8632 -1.2880   \n",
       "4      1.4180 -0.8244  ...  0.0042  0.0048  0.6670  1.0690  0.5523 -0.3031   \n",
       "...       ...     ...  ...     ...     ...     ...     ...     ...     ...   \n",
       "21943 -0.2431  0.4256  ...  0.0789  0.3538  0.0558  0.3377 -0.4753 -0.2504   \n",
       "21944 -0.4713  0.7201  ...  0.1969  0.0262 -0.8121  0.3434  0.5372 -0.3246   \n",
       "21945 -0.7178  0.6621  ...  0.4286  0.4426  0.0423 -0.3195 -0.8086 -0.9798   \n",
       "21946  0.8531 -0.0343  ... -0.1105  0.4258 -0.2012  0.1506  1.5230  0.7101   \n",
       "21947 -0.3611 -3.6750  ... -3.3890 -1.7450 -6.6300 -4.0950 -7.3860 -1.4160   \n",
       "\n",
       "         c-96    c-97    c-98    c-99  \n",
       "0     -0.3981  0.2139  0.3801  0.4176  \n",
       "1      0.1522  0.1241  0.6077  0.7371  \n",
       "2     -0.6417 -0.2187 -1.4080  0.6931  \n",
       "3     -1.6210 -0.8784 -0.3876 -0.8154  \n",
       "4      0.1094  0.2885 -0.3786  0.7125  \n",
       "...       ...     ...     ...     ...  \n",
       "21943 -0.7415  0.8413 -0.4259  0.2434  \n",
       "21944  0.0631  0.9171  0.5258  0.4680  \n",
       "21945 -0.2084 -0.1224 -0.2715  0.3689  \n",
       "21946  0.1732  0.7015 -0.6290  0.0740  \n",
       "21947 -3.5770 -0.4775 -2.1500 -4.2520  \n",
       "\n",
       "[21948 rows x 876 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T20:38:54.982986Z",
     "iopub.status.busy": "2020-11-26T20:38:54.981724Z",
     "iopub.status.idle": "2020-11-26T20:38:55.488095Z",
     "shell.execute_reply": "2020-11-26T20:38:55.488615Z"
    },
    "papermill": {
     "duration": 0.554433,
     "end_time": "2020-11-26T20:38:55.488782",
     "exception": false,
     "start_time": "2020-11-26T20:38:54.934349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove 0 columns\n",
      "remove 71 columns\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ratio for each label\n",
    "\n",
    "def get_ratio_labels(df):\n",
    "    columns = list(df.columns)\n",
    "    columns.pop(0)\n",
    "    ratios = []\n",
    "    toremove = []\n",
    "    for c in columns:\n",
    "        counts = df[c].value_counts()\n",
    "        if len(counts) != 1:\n",
    "            ratios.append(counts[0]/counts[1])\n",
    "        else:\n",
    "            toremove.append(c)\n",
    "    print(f\"remove {len(toremove)} columns\")\n",
    "    \n",
    "    for t in toremove:\n",
    "        columns.remove(t)\n",
    "    return columns, np.array(ratios).astype(np.int32)\n",
    "\n",
    "columns, ratios = get_ratio_labels(train_targets_scored)\n",
    "columns_nonscored, ratios_nonscored = get_ratio_labels(train_targets_nonscored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T20:38:55.570832Z",
     "iopub.status.busy": "2020-11-26T20:38:55.568984Z",
     "iopub.status.idle": "2020-11-26T20:38:55.571465Z",
     "shell.execute_reply": "2020-11-26T20:38:55.571952Z"
    },
    "papermill": {
     "duration": 0.046137,
     "end_time": "2020-11-26T20:38:55.572076",
     "exception": false,
     "start_time": "2020-11-26T20:38:55.525939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "drop_c62=False\n",
    "if drop_c62:\n",
    "    train_features = train_features.drop(['c-62'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.037322,
     "end_time": "2020-11-26T20:38:55.646086",
     "exception": false,
     "start_time": "2020-11-26T20:38:55.608764",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T20:38:55.743716Z",
     "iopub.status.busy": "2020-11-26T20:38:55.733348Z",
     "iopub.status.idle": "2020-11-26T20:38:55.908408Z",
     "shell.execute_reply": "2020-11-26T20:38:55.907863Z"
    },
    "papermill": {
     "duration": 0.224959,
     "end_time": "2020-11-26T20:38:55.908515",
     "exception": false,
     "start_time": "2020-11-26T20:38:55.683556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "extra_features_=False\n",
    "add_clusters_= False\n",
    "\n",
    "def create_cluster(train, test, features, kind = 'g', n_clusters = 35):\n",
    "    train_ = train[features].copy()\n",
    "    test_ = test[features].copy()\n",
    "    data = pd.concat([train_, test_], axis = 0)\n",
    "    kmeans = KMeans(n_clusters = n_clusters, random_state = 62).fit(data)\n",
    "    train[f'clusters_{kind}'] = kmeans.labels_[:train.shape[0]]\n",
    "    test[f'clusters_{kind}'] = kmeans.labels_[train.shape[0]:]\n",
    "    #train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n",
    "    #test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n",
    "    return train, test\n",
    "    \n",
    "def transform_data(train, test, col, normalize=True, removed_vehicle=False, extra_features=False, add_clusters=False):\n",
    "    \"\"\"\n",
    "        the first 3 columns represents categories, the others numericals features\n",
    "    \"\"\"\n",
    "    mapping = {\"cp_type\":{\"trt_cp\": 0, \"ctl_vehicle\":1},\n",
    "               \"cp_time\":{48:0, 72:1, 24:2},\n",
    "               \"cp_dose\":{\"D1\":0, \"D2\":1}}\n",
    "    \n",
    "    if extra_features:\n",
    "        features_g = list(train.columns[4:774])\n",
    "        features_c = list(train.columns[776:876])\n",
    "\n",
    "        for df in train, test:\n",
    "            df['g_sum'] = df[features_g].sum(axis = 1)\n",
    "            df['g_mean'] = df[features_g].mean(axis = 1)\n",
    "            df['g_std'] = df[features_g].std(axis = 1)\n",
    "            df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n",
    "            df['g_skew'] = df[features_g].skew(axis = 1)\n",
    "            df['c_sum'] = df[features_c].sum(axis = 1)\n",
    "            df['c_mean'] = df[features_c].mean(axis = 1)\n",
    "            df['c_std'] = df[features_c].std(axis = 1)\n",
    "            df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n",
    "            df['c_skew'] = df[features_c].skew(axis = 1)\n",
    "            df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n",
    "            df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n",
    "            df['gc_std'] = df[features_g + features_c].std(axis = 1)\n",
    "            df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n",
    "            df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n",
    "        \n",
    "    if add_clusters:\n",
    "        features_g = list(train.columns[4:774])\n",
    "        features_c = list(train.columns[776:876])\n",
    "    \n",
    "        train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = 35)\n",
    "        train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = 5)\n",
    "        \n",
    "    \n",
    "    if removed_vehicle:\n",
    "        categories_tr = np.stack([ train[c].apply(lambda x: mapping[c][x]).values for c in col[1:3]], axis=1)\n",
    "        categories_test = np.stack([ test[c].apply(lambda x: mapping[c][x]).values for c in col[1:3]], axis=1)\n",
    "    else:\n",
    "        categories_tr = np.stack([ train[c].apply(lambda x: mapping[c][x]).values for c in col[:3]], axis=1)\n",
    "        categories_test = np.stack([ test[c].apply(lambda x: mapping[c][x]).values for c in col[:3]], axis=1)\n",
    "    if add_clusters:\n",
    "        categories_tr = np.append(categories_tr, train[['clusters_g', 'clusters_c']].values, axis=1)\n",
    "        categories_test = np.append(categories_test, test[['clusters_g', 'clusters_c']].values, axis=1)\n",
    "        \n",
    "    #categories_tr = np.append(categories_tr, train[['sig_id']].values, axis=1) \n",
    "    #categories_test = np.append(categories_test, test[['sig_id']].values, axis=1)\n",
    "    \n",
    "    max_ = 10.\n",
    "    min_ = -10.\n",
    "   \n",
    "    if removed_vehicle:\n",
    "        numerical_tr = train[col[3:]].values\n",
    "        numerical_test = test[col[3:]].values\n",
    "    else:\n",
    "        numerical_tr = train[col[3:]].values\n",
    "        numerical_test = test[col[3:]].values\n",
    "    if normalize:\n",
    "        numerical_tr = (numerical_tr-min_)/(max_ - min_)\n",
    "        numerical_test = (numerical_test-min_)/(max_ - min_)\n",
    "    return categories_tr, categories_test, numerical_tr, numerical_test\n",
    "if extra_features_:\n",
    "    col_features = list(train_features.columns)[1:] + ['g_sum', 'g_mean', 'g_std', 'g_kurt', 'g_skew', 'c_sum', 'c_mean', 'c_std', 'c_kurt', 'c_skew', 'gc_sum', 'gc_mean', 'gc_std', 'gc_kurt', 'gc_skew']  \n",
    "else:\n",
    "    col_features = list(train_features.columns)[1:]\n",
    "cat_tr, cat_test, numerical_tr, numerical_test = transform_data(train_features, test_features, col_features, normalize=False, removed_vehicle=remove_vehicle, extra_features=extra_features_, add_clusters=add_clusters_)\n",
    "targets_tr = train_targets_scored[columns].values.astype(np.float32)\n",
    "targets2_tr = train_targets_nonscored[columns_nonscored].values.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T20:38:56.005345Z",
     "iopub.status.busy": "2020-11-26T20:38:56.003989Z",
     "iopub.status.idle": "2020-11-26T20:38:56.103621Z",
     "shell.execute_reply": "2020-11-26T20:38:56.104335Z"
    },
    "papermill": {
     "duration": 0.157358,
     "end_time": "2020-11-26T20:38:56.104525",
     "exception": false,
     "start_time": "2020-11-26T20:38:55.947167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21948, 874)\n",
      "(3982, 874)\n"
     ]
    }
   ],
   "source": [
    "freq_encodeing = False\n",
    "if freq_encodeing:\n",
    "    df_numerical_tr = pd.DataFrame(numerical_tr)\n",
    "    df_numerical_test = pd.DataFrame(numerical_test)\n",
    "    df_numerical_count_tr = pd.DataFrame()\n",
    "    df_numerical_count_test = pd.DataFrame()\n",
    "    df_numerical_tr_map = pd.DataFrame()\n",
    "    df_numerical_test_map = pd.DataFrame()\n",
    "\n",
    "    def myround(x, prec=2, base=.05):\n",
    "      return round(base * round(float(x)/base),prec)\n",
    "\n",
    "    for col in df_numerical_tr.columns:\n",
    "        df_numerical_count_tr.loc[:,col] = df_numerical_tr.loc[:,col].apply(lambda x: myround(x)).value_counts()\n",
    "        df_numerical_tr_map.loc[:,col] = df_numerical_tr.loc[:,col].apply(lambda x: myround(x)).map(df_numerical_count_tr.loc[:,col])\n",
    "        df_numerical_count_test.loc[:,col] = df_numerical_test.loc[:,col].apply(lambda x: myround(x)).value_counts()\n",
    "        df_numerical_test_map.loc[:,col] = df_numerical_test.loc[:,col].apply(lambda x: myround(x)).map(df_numerical_count_test.loc[:,col])\n",
    "    df_numerical_tr_map = df_numerical_tr_map.fillna(0)\n",
    "    df_numerical_test_map = df_numerical_test_map.fillna(0)\n",
    "\n",
    "    train_data = np.concatenate([cat_tr, numerical_tr, df_numerical_tr_map.values], axis=1)\n",
    "    print(train_data.shape)\n",
    "    test_data = np.concatenate([cat_test, numerical_test, df_numerical_test_map.values], axis=1)\n",
    "    print(test_data.shape)\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    train_data[:,cat_tr.shape[1]:] = scaler.fit_transform(train_data[:,cat_tr.shape[1]:])\n",
    "    test_data[:,cat_tr.shape[1]:]= scaler.transform(test_data[:,cat_tr.shape[1]:])\n",
    "else:\n",
    "    train_data = np.concatenate([cat_tr, numerical_tr], axis=1)\n",
    "    print(train_data.shape)\n",
    "    test_data = np.concatenate([cat_test, numerical_test], axis=1)\n",
    "    print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T20:38:56.193027Z",
     "iopub.status.busy": "2020-11-26T20:38:56.191040Z",
     "iopub.status.idle": "2020-11-26T20:38:56.193787Z",
     "shell.execute_reply": "2020-11-26T20:38:56.194253Z"
    },
    "papermill": {
     "duration": 0.051421,
     "end_time": "2020-11-26T20:38:56.194404",
     "exception": false,
     "start_time": "2020-11-26T20:38:56.142983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Trans_PCA=False\n",
    "if Trans_PCA:\n",
    "    from sklearn.decomposition import PCA\n",
    "    n_comp = 10\n",
    "    PCAF = PCA(n_components=n_comp, random_state=62)\n",
    "    numerical_tr_PCA = PCAF.fit_transform(train_data[:,-df_numerical_tr_map.shape[1]:])\n",
    "    numerical_test_PCA = PCAF.transform(test_data[:,-df_numerical_tr_map.shape[1]:])\n",
    "    \n",
    "    train_data = train_data[:,:-df_numerical_tr_map.shape[1]]\n",
    "    test_data = test_data[:,:-df_numerical_tr_map.shape[1]]\n",
    "    \n",
    "    train_data = np.concatenate([train_data, numerical_tr_PCA], axis=1)\n",
    "    print(train_data.shape)\n",
    "    test_data = np.concatenate([test_data, numerical_test_PCA], axis=1)\n",
    "    print(test_data.shape)\n",
    "    print(PCAF.explained_variance_ratio_)\n",
    "    print(PCAF.explained_variance_ratio_.sum())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.046913,
     "end_time": "2020-11-26T20:38:56.279841",
     "exception": false,
     "start_time": "2020-11-26T20:38:56.232928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.038732,
     "end_time": "2020-11-26T20:38:56.357701",
     "exception": false,
     "start_time": "2020-11-26T20:38:56.318969",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T20:38:56.442221Z",
     "iopub.status.busy": "2020-11-26T20:38:56.440172Z",
     "iopub.status.idle": "2020-11-26T20:38:56.442972Z",
     "shell.execute_reply": "2020-11-26T20:38:56.443491Z"
    },
    "papermill": {
     "duration": 0.047684,
     "end_time": "2020-11-26T20:38:56.443618",
     "exception": false,
     "start_time": "2020-11-26T20:38:56.395934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "           \n",
    "def inference_fn(model, X ,verbose=True):\n",
    "    with torch.no_grad():\n",
    "        y_preds = model.predict( X )\n",
    "        y_preds = torch.sigmoid(torch.as_tensor(y_preds)).numpy()\n",
    "    return y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T20:38:56.693503Z",
     "iopub.status.busy": "2020-11-26T20:38:56.692462Z",
     "iopub.status.idle": "2020-11-26T20:38:56.697958Z",
     "shell.execute_reply": "2020-11-26T20:38:56.698786Z"
    },
    "papermill": {
     "duration": 0.122053,
     "end_time": "2020-11-26T20:38:56.698989",
     "exception": false,
     "start_time": "2020-11-26T20:38:56.576936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_loss_score(actual, predicted,  eps=1e-15):\n",
    "\n",
    "        \"\"\"\n",
    "        :param predicted:   The predicted probabilities as floats between 0-1\n",
    "        :param actual:      The binary labels. Either 0 or 1.\n",
    "        :param eps:         Log(0) is equal to infinity, so we need to offset our predicted values slightly by eps from 0 or 1\n",
    "        :return:            The logarithmic loss between between the predicted probability assigned to the possible outcomes for item i, and the actual outcome.\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        p1 = actual * np.log(predicted+eps)\n",
    "        p0 = (1-actual) * np.log(1-predicted+eps)\n",
    "        loss = p0 + p1\n",
    "\n",
    "        return -loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T20:38:56.837414Z",
     "iopub.status.busy": "2020-11-26T20:38:56.836354Z",
     "iopub.status.idle": "2020-11-26T20:38:56.841724Z",
     "shell.execute_reply": "2020-11-26T20:38:56.842432Z"
    },
    "papermill": {
     "duration": 0.079805,
     "end_time": "2020-11-26T20:38:56.842617",
     "exception": false,
     "start_time": "2020-11-26T20:38:56.762812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_loss_multi(y_true, y_pred):\n",
    "    M = y_true.shape[1]\n",
    "    results = np.zeros(M)\n",
    "    for i in range(M):\n",
    "        results[i] = log_loss_score(y_true[:,i], y_pred[:,i])\n",
    "    return results.mean()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T20:38:56.979964Z",
     "iopub.status.busy": "2020-11-26T20:38:56.978882Z",
     "iopub.status.idle": "2020-11-26T20:38:56.983286Z",
     "shell.execute_reply": "2020-11-26T20:38:56.984140Z"
    },
    "papermill": {
     "duration": 0.077626,
     "end_time": "2020-11-26T20:38:56.984383",
     "exception": false,
     "start_time": "2020-11-26T20:38:56.906757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_targets(targets):\n",
    "    ### check if targets are all binary in training set\n",
    "    \n",
    "    for i in range(targets.shape[1]):\n",
    "        if len(np.unique(targets[:,i])) != 2:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T20:38:57.120486Z",
     "iopub.status.busy": "2020-11-26T20:38:57.119540Z",
     "iopub.status.idle": "2020-11-26T20:38:57.124531Z",
     "shell.execute_reply": "2020-11-26T20:38:57.125232Z"
    },
    "papermill": {
     "duration": 0.07651,
     "end_time": "2020-11-26T20:38:57.125413",
     "exception": false,
     "start_time": "2020-11-26T20:38:57.048903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def auc_multi(y_true, y_pred):\n",
    "    M = y_true.shape[1]\n",
    "    results = np.zeros(M)\n",
    "    for i in range(M):\n",
    "        try:\n",
    "            results[i] = roc_auc_score(y_true[:,i], y_pred[:,i])\n",
    "        except:\n",
    "            pass\n",
    "    return results.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.056902,
     "end_time": "2020-11-26T20:38:57.241432",
     "exception": false,
     "start_time": "2020-11-26T20:38:57.184530",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039196,
     "end_time": "2020-11-26T20:38:57.321504",
     "exception": false,
     "start_time": "2020-11-26T20:38:57.282308",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Cross Entrpoy with Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-11-26T20:38:57.442128Z",
     "iopub.status.busy": "2020-11-26T20:38:57.415965Z",
     "iopub.status.idle": "2020-11-26T20:38:57.571148Z",
     "shell.execute_reply": "2020-11-26T20:38:57.570569Z"
    },
    "papermill": {
     "duration": 0.210829,
     "end_time": "2020-11-26T20:38:57.571255",
     "exception": false,
     "start_time": "2020-11-26T20:38:57.360426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## TABNET\n",
    "#from pytorch_tabnet.tab_model import TabModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.sparse import csc_matrix\n",
    "import time\n",
    "from abc import abstractmethod\n",
    "from pytorch_tabnet import tab_network\n",
    "from pytorch_tabnet.multiclass_utils import unique_labels\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, accuracy_score\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from pytorch_tabnet.utils import (PredictDataset,\n",
    "                                  create_dataloaders,\n",
    "                                  create_explain_matrix)\n",
    "from sklearn.base import BaseEstimator\n",
    "from torch.utils.data import DataLoader\n",
    "from copy import deepcopy\n",
    "import io\n",
    "import json\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "\n",
    "class TabModel(BaseEstimator):\n",
    "    def __init__(self, n_d=8, n_a=8, n_steps=3, gamma=1.3, cat_idxs=[], cat_dims=[], cat_emb_dim=1,\n",
    "                 n_independent=2, n_shared=2, epsilon=1e-15,  momentum=0.02,\n",
    "                 lambda_sparse=1e-3, seed=0,\n",
    "                 clip_value=1, verbose=1,\n",
    "                 optimizer_fn=torch.optim.Adam,\n",
    "                 optimizer_params=dict(lr=2e-2),\n",
    "                 scheduler_params=None, scheduler_fn=None,\n",
    "                 mask_type=\"sparsemax\",\n",
    "                 input_dim=None, output_dim=None,\n",
    "                 device_name='auto',\n",
    "                 label_smoothing = 0.001):\n",
    "        \"\"\" Class for TabNet model\n",
    "        Parameters\n",
    "        ----------\n",
    "            device_name: str\n",
    "                'cuda' if running on GPU, 'cpu' if not, 'auto' to autodetect\n",
    "        \"\"\"\n",
    "\n",
    "        self.n_d = n_d\n",
    "        self.n_a = n_a\n",
    "        self.n_steps = n_steps\n",
    "        self.gamma = gamma\n",
    "        self.cat_idxs = cat_idxs\n",
    "        self.cat_dims = cat_dims\n",
    "        self.cat_emb_dim = cat_emb_dim\n",
    "        self.n_independent = n_independent\n",
    "        self.n_shared = n_shared\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.lambda_sparse = lambda_sparse\n",
    "        self.clip_value = clip_value\n",
    "        self.verbose = verbose\n",
    "        self.optimizer_fn = optimizer_fn\n",
    "        self.optimizer_params = optimizer_params\n",
    "        self.device_name = device_name\n",
    "        self.scheduler_params = scheduler_params\n",
    "        self.scheduler_fn = scheduler_fn\n",
    "        self.mask_type = mask_type\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.batch_size = 1024\n",
    "        \n",
    "        #Adaugat de Radu:\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "        self.seed = seed\n",
    "        torch.manual_seed(self.seed)\n",
    "        # Defining device\n",
    "        if device_name == 'auto':\n",
    "            if torch.cuda.is_available():\n",
    "                device_name = 'cuda'\n",
    "            else:\n",
    "                device_name = 'cpu'\n",
    "        self.device = torch.device(device_name)\n",
    "        print(f\"Device used : {self.device}\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def construct_loaders(self, X_train, y_train, X_valid, y_valid,\n",
    "                          weights, batch_size, num_workers, drop_last):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        train_dataloader, valid_dataloader : torch.DataLoader, torch.DataLoader\n",
    "            Training and validation dataloaders\n",
    "        -------\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define construct_loaders to use this base class')\n",
    "\n",
    "    def init_network(\n",
    "                     self,\n",
    "                     input_dim,\n",
    "                     output_dim,\n",
    "                     n_d,\n",
    "                     n_a,\n",
    "                     n_steps,\n",
    "                     gamma,\n",
    "                     cat_idxs,\n",
    "                     cat_dims,\n",
    "                     cat_emb_dim,\n",
    "                     n_independent,\n",
    "                     n_shared,\n",
    "                     epsilon,\n",
    "                     virtual_batch_size,\n",
    "                     momentum,\n",
    "                     device_name,\n",
    "                     mask_type,\n",
    "                     ):\n",
    "        self.network = tab_network.TabNet(\n",
    "            input_dim,\n",
    "            output_dim,\n",
    "            n_d=n_d,\n",
    "            n_a=n_a,\n",
    "            n_steps=n_steps,\n",
    "            gamma=gamma,\n",
    "            cat_idxs=cat_idxs,\n",
    "            cat_dims=cat_dims,\n",
    "            cat_emb_dim=cat_emb_dim,\n",
    "            n_independent=n_independent,\n",
    "            n_shared=n_shared,\n",
    "            epsilon=epsilon,\n",
    "            virtual_batch_size=virtual_batch_size,\n",
    "            momentum=momentum,\n",
    "            device_name=device_name,\n",
    "            mask_type=mask_type).to(self.device)\n",
    "\n",
    "        self.reducing_matrix = create_explain_matrix(\n",
    "            self.network.input_dim,\n",
    "            self.network.cat_emb_dim,\n",
    "            self.network.cat_idxs,\n",
    "            self.network.post_embed_dim)\n",
    "\n",
    "    def fit(self, X_train, y_train, X_valid=None, y_valid=None, loss_fn=None,\n",
    "            weights=0, max_epochs=100, patience=10, batch_size=1024,\n",
    "            virtual_batch_size=128, num_workers=0, drop_last=False, label_smoothing=0.001):\n",
    "        \"\"\"Train a neural network stored in self.network\n",
    "        Using train_dataloader for training data and\n",
    "        valid_dataloader for validation.\n",
    "        Parameters\n",
    "        ----------\n",
    "            X_train: np.ndarray\n",
    "                Train set\n",
    "            y_train : np.array\n",
    "                Train targets\n",
    "            X_train: np.ndarray\n",
    "                Train set\n",
    "            y_train : np.array\n",
    "                Train targets\n",
    "            weights : bool or dictionnary\n",
    "                0 for no balancing\n",
    "                1 for automated balancing\n",
    "                dict for custom weights per class\n",
    "            max_epochs : int\n",
    "                Maximum number of epochs during training\n",
    "            patience : int\n",
    "                Number of consecutive non improving epoch before early stopping\n",
    "            batch_size : int\n",
    "                Training batch size\n",
    "            virtual_batch_size : int\n",
    "                Batch size for Ghost Batch Normalization (virtual_batch_size < batch_size)\n",
    "            num_workers : int\n",
    "                Number of workers used in torch.utils.data.DataLoader\n",
    "            drop_last : bool\n",
    "                Whether to drop last batch during training\n",
    "        \"\"\"\n",
    "        # update model name\n",
    "\n",
    "        self.update_fit_params(X_train, y_train, X_valid, y_valid, loss_fn,\n",
    "                               weights, max_epochs, patience, batch_size,\n",
    "                               virtual_batch_size, num_workers, drop_last, label_smoothing)\n",
    "\n",
    "        train_dataloader, valid_dataloader = self.construct_loaders(X_train,\n",
    "                                                                    y_train,\n",
    "                                                                    X_valid,\n",
    "                                                                    y_valid,\n",
    "                                                                    self.updated_weights,\n",
    "                                                                    self.batch_size,\n",
    "                                                                    self.num_workers,\n",
    "                                                                    self.drop_last)\n",
    "\n",
    "        self.init_network(\n",
    "            input_dim=self.input_dim,\n",
    "            output_dim=self.output_dim,\n",
    "            n_d=self.n_d,\n",
    "            n_a=self.n_a,\n",
    "            n_steps=self.n_steps,\n",
    "            gamma=self.gamma,\n",
    "            cat_idxs=self.cat_idxs,\n",
    "            cat_dims=self.cat_dims,\n",
    "            cat_emb_dim=self.cat_emb_dim,\n",
    "            n_independent=self.n_independent,\n",
    "            n_shared=self.n_shared,\n",
    "            epsilon=self.epsilon,\n",
    "            virtual_batch_size=self.virtual_batch_size,\n",
    "            momentum=self.momentum,\n",
    "            device_name=self.device_name,\n",
    "            mask_type=self.mask_type\n",
    "        )\n",
    "\n",
    "        self.optimizer = self.optimizer_fn(self.network.parameters(),\n",
    "                                           **self.optimizer_params)\n",
    "\n",
    "        if self.scheduler_fn:\n",
    "            self.scheduler = self.scheduler_fn(self.optimizer, **self.scheduler_params)\n",
    "        else:\n",
    "            self.scheduler = None\n",
    "\n",
    "        self.losses_train = []\n",
    "        self.losses_valid = []\n",
    "        self.learning_rates = []\n",
    "        self.metrics_train = []\n",
    "        self.metrics_valid = []\n",
    "\n",
    "        if self.verbose > 0:\n",
    "            print(\"Will train until validation stopping metric\",\n",
    "                  f\"hasn't improved in {self.patience} rounds.\")\n",
    "            msg_epoch = f'| EPOCH |  train  |   valid  | total time (s)'\n",
    "            print('---------------------------------------')\n",
    "            print(msg_epoch)\n",
    "\n",
    "        total_time = 0\n",
    "        while (self.epoch < self.max_epochs and\n",
    "               self.patience_counter < self.patience):\n",
    "            starting_time = time.time()\n",
    "            # updates learning rate history\n",
    "            self.learning_rates.append(self.optimizer.param_groups[-1][\"lr\"])\n",
    "\n",
    "            fit_metrics = self.fit_epoch(train_dataloader, valid_dataloader)\n",
    "\n",
    "            # leaving it here, may be used for callbacks later\n",
    "            self.losses_train.append(fit_metrics['train']['loss_avg'])\n",
    "            self.losses_valid.append(fit_metrics['valid']['total_loss'])\n",
    "            self.metrics_train.append(fit_metrics['train']['stopping_loss'])\n",
    "            self.metrics_valid.append(fit_metrics['valid']['stopping_loss'])\n",
    "\n",
    "            stopping_loss = fit_metrics['valid']['stopping_loss']\n",
    "            if stopping_loss < self.best_cost:\n",
    "                self.best_cost = stopping_loss\n",
    "                self.patience_counter = 0\n",
    "                # Saving model\n",
    "                self.best_network = deepcopy(self.network)\n",
    "                has_improved = True\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "                has_improved=False\n",
    "            self.epoch += 1\n",
    "            total_time += time.time() - starting_time\n",
    "            if self.verbose > 0:\n",
    "                if self.epoch % self.verbose == 0:\n",
    "                    separator = \"|\"\n",
    "                    msg_epoch = f\"| {self.epoch:<5} | \"\n",
    "                    msg_epoch += f\" {fit_metrics['train']['stopping_loss']:.5f}\"\n",
    "                    msg_epoch += f' {separator:<2} '\n",
    "                    msg_epoch += f\" {fit_metrics['valid']['stopping_loss']:.5f}\"\n",
    "                    msg_epoch += f' {separator:<2} '\n",
    "                    msg_epoch += f\" {np.round(total_time, 1):<10}\"\n",
    "                    msg_epoch += f\" {has_improved}\"\n",
    "                    print(msg_epoch)\n",
    "\n",
    "        if self.verbose > 0:\n",
    "            if self.patience_counter == self.patience:\n",
    "                print(f\"Early stopping occured at epoch {self.epoch}\")\n",
    "            print(f\"Training done in {total_time:.3f} seconds.\")\n",
    "            print('---------------------------------------')\n",
    "\n",
    "        self.history = {\"train\": {\"loss\": self.losses_train,\n",
    "                                  \"metric\": self.metrics_train,\n",
    "                                  \"lr\": self.learning_rates},\n",
    "                        \"valid\": {\"loss\": self.losses_valid,\n",
    "                                  \"metric\": self.metrics_valid}}\n",
    "        # load best models post training\n",
    "        self.load_best_model()\n",
    "\n",
    "        # compute feature importance once the best model is defined\n",
    "        self._compute_feature_importances(train_dataloader)\n",
    "\n",
    "    def save_model(self, path):\n",
    "        \"\"\"\n",
    "        Saving model with two distinct files.\n",
    "        \"\"\"\n",
    "        saved_params = {}\n",
    "        for key, val in self.get_params().items():\n",
    "            if isinstance(val, type):\n",
    "                # Don't save torch specific params\n",
    "                continue\n",
    "            else:\n",
    "                saved_params[key] = val\n",
    "\n",
    "        # Create folder\n",
    "        Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Save models params\n",
    "        with open(Path(path).joinpath(\"model_params.json\"), \"w\", encoding=\"utf8\") as f:\n",
    "            json.dump(saved_params, f)\n",
    "\n",
    "        # Save state_dict\n",
    "        torch.save(self.network.state_dict(), Path(path).joinpath(\"network.pt\"))\n",
    "        shutil.make_archive(path, 'zip', path)\n",
    "        shutil.rmtree(path)\n",
    "        print(f\"Successfully saved model at {path}.zip\")\n",
    "        return f\"{path}.zip\"\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "\n",
    "        try:\n",
    "            with zipfile.ZipFile(filepath) as z:\n",
    "                with z.open(\"model_params.json\") as f:\n",
    "                    loaded_params = json.load(f)\n",
    "                with z.open(\"network.pt\") as f:\n",
    "                    try:\n",
    "                        saved_state_dict = torch.load(f)\n",
    "                    except io.UnsupportedOperation:\n",
    "                        # In Python <3.7, the returned file object is not seekable (which at least\n",
    "                        # some versions of PyTorch require) - so we'll try buffering it in to a\n",
    "                        # BytesIO instead:\n",
    "                        saved_state_dict = torch.load(io.BytesIO(f.read()))\n",
    "        except KeyError:\n",
    "            raise KeyError(\"Your zip file is missing at least one component\")\n",
    "\n",
    "        self.__init__(**loaded_params)\n",
    "\n",
    "        self.init_network(\n",
    "            input_dim=self.input_dim,\n",
    "            output_dim=self.output_dim,\n",
    "            n_d=self.n_d,\n",
    "            n_a=self.n_a,\n",
    "            n_steps=self.n_steps,\n",
    "            gamma=self.gamma,\n",
    "            cat_idxs=self.cat_idxs,\n",
    "            cat_dims=self.cat_dims,\n",
    "            cat_emb_dim=self.cat_emb_dim,\n",
    "            n_independent=self.n_independent,\n",
    "            n_shared=self.n_shared,\n",
    "            epsilon=self.epsilon,\n",
    "            virtual_batch_size=1024,\n",
    "            momentum=self.momentum,\n",
    "            device_name=self.device_name,\n",
    "            mask_type=self.mask_type\n",
    "        )\n",
    "        self.network.load_state_dict(saved_state_dict)\n",
    "        self.network.eval()\n",
    "        return\n",
    "\n",
    "    def fit_epoch(self, train_dataloader, valid_dataloader):\n",
    "        \"\"\"\n",
    "        Evaluates and updates network for one epoch.\n",
    "        Parameters\n",
    "        ----------\n",
    "            train_dataloader: a :class: `torch.utils.data.Dataloader`\n",
    "                DataLoader with train set\n",
    "            valid_dataloader: a :class: `torch.utils.data.Dataloader`\n",
    "                DataLoader with valid set\n",
    "        \"\"\"\n",
    "        train_metrics = self.train_epoch(train_dataloader)\n",
    "        valid_metrics = self.predict_epoch(valid_dataloader)\n",
    "\n",
    "        fit_metrics = {'train': train_metrics,\n",
    "                       'valid': valid_metrics}\n",
    "\n",
    "        return fit_metrics\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_epoch(self, train_loader):\n",
    "        \"\"\"\n",
    "        Trains one epoch of the network in self.network\n",
    "        Parameters\n",
    "        ----------\n",
    "            train_loader: a :class: `torch.utils.data.Dataloader`\n",
    "                DataLoader with train set\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define train_epoch to use this base class')\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_batch(self, data, targets):\n",
    "        \"\"\"\n",
    "        Trains one batch of data\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.tensor`\n",
    "                Target data\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define train_batch to use this base class')\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict_epoch(self, loader):\n",
    "        \"\"\"\n",
    "        Validates one epoch of the network in self.network\n",
    "        Parameters\n",
    "        ----------\n",
    "            loader: a :class: `torch.utils.data.Dataloader`\n",
    "                    DataLoader with validation set\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define predict_epoch to use this base class')\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict_batch(self, data, targets):\n",
    "        \"\"\"\n",
    "        Make predictions on a batch (valid)\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            batch_outs: dict\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define predict_batch to use this base class')\n",
    "\n",
    "    def load_best_model(self):\n",
    "        if self.best_network is not None:\n",
    "            self.network = self.best_network\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on a batch (valid)\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            predictions: np.array\n",
    "                Predictions of the regression problem or the last class\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define predict to use this base class')\n",
    "\n",
    "    def explain(self, X):\n",
    "        \"\"\"\n",
    "        Return local explanation\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            M_explain: matrix\n",
    "                Importance per sample, per columns.\n",
    "            masks: matrix\n",
    "                Sparse matrix showing attention masks used by network.\n",
    "        \"\"\"\n",
    "        self.network.eval()\n",
    "\n",
    "        dataloader = DataLoader(PredictDataset(X),\n",
    "                                batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        for batch_nb, data in enumerate(dataloader):\n",
    "            data = data.to(self.device).float()\n",
    "\n",
    "            M_explain, masks = self.network.forward_masks(data)\n",
    "            for key, value in masks.items():\n",
    "                masks[key] = csc_matrix.dot(value.cpu().detach().numpy(),\n",
    "                                            self.reducing_matrix)\n",
    "\n",
    "            if batch_nb == 0:\n",
    "                res_explain = csc_matrix.dot(M_explain.cpu().detach().numpy(),\n",
    "                                             self.reducing_matrix)\n",
    "                res_masks = masks\n",
    "            else:\n",
    "                res_explain = np.vstack([res_explain,\n",
    "                                         csc_matrix.dot(M_explain.cpu().detach().numpy(),\n",
    "                                                        self.reducing_matrix)])\n",
    "                for key, value in masks.items():\n",
    "                    res_masks[key] = np.vstack([res_masks[key], value])\n",
    "        return res_explain, res_masks\n",
    "\n",
    "    def _compute_feature_importances(self, loader):\n",
    "        self.network.eval()\n",
    "        feature_importances_ = np.zeros((self.network.post_embed_dim))\n",
    "        for data, targets in loader:\n",
    "            data = data.to(self.device).float()\n",
    "            M_explain, masks = self.network.forward_masks(data)\n",
    "            feature_importances_ += M_explain.sum(dim=0).cpu().detach().numpy()\n",
    "\n",
    "        feature_importances_ = csc_matrix.dot(feature_importances_,\n",
    "                                              self.reducing_matrix)\n",
    "        self.feature_importances_ = feature_importances_ / np.sum(feature_importances_)\n",
    "        \n",
    "        \n",
    "class TabNetRegressor(TabModel):\n",
    "\n",
    "    def construct_loaders(self, X_train, y_train, X_valid, y_valid, weights,\n",
    "                          batch_size, num_workers, drop_last):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        train_dataloader, valid_dataloader : torch.DataLoader, torch.DataLoader\n",
    "            Training and validation dataloaders\n",
    "        -------\n",
    "        \"\"\"\n",
    "        if isinstance(weights, int):\n",
    "            if weights == 1:\n",
    "                raise ValueError(\"Please provide a list of weights for regression.\")\n",
    "        if isinstance(weights, dict):\n",
    "            raise ValueError(\"Please provide a list of weights for regression.\")\n",
    "\n",
    "        train_dataloader, valid_dataloader = create_dataloaders(X_train,\n",
    "                                                                y_train,\n",
    "                                                                X_valid,\n",
    "                                                                y_valid,\n",
    "                                                                weights,\n",
    "                                                                batch_size,\n",
    "                                                                num_workers,\n",
    "                                                                drop_last)\n",
    "        return train_dataloader, valid_dataloader\n",
    "\n",
    "    def update_fit_params(self, X_train, y_train, X_valid, y_valid, loss_fn,\n",
    "                          weights, max_epochs, patience,\n",
    "                          batch_size, virtual_batch_size, num_workers, drop_last,label_smoothing):\n",
    "\n",
    "        if loss_fn is None:\n",
    "            self.loss_fn = torch.nn.functional.mse_loss\n",
    "        else:\n",
    "            self.loss_fn = loss_fn\n",
    "\n",
    "        assert X_train.shape[1] == X_valid.shape[1], \"Dimension mismatch X_train X_valid\"\n",
    "        self.input_dim = X_train.shape[1]\n",
    "\n",
    "        if len(y_train.shape) == 1:\n",
    "            raise ValueError(\"\"\"Please apply reshape(-1, 1) to your targets\n",
    "                                if doing single regression.\"\"\")\n",
    "        assert y_train.shape[1] == y_valid.shape[1], \"Dimension mismatch y_train y_valid\"\n",
    "        self.output_dim = y_train.shape[1]\n",
    "\n",
    "        self.updated_weights = weights\n",
    "\n",
    "        self.max_epochs = max_epochs\n",
    "        self.patience = patience\n",
    "        self.batch_size = batch_size\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        # Initialize counters and histories.\n",
    "        self.patience_counter = 0\n",
    "        self.epoch = 0\n",
    "        self.best_cost = np.inf\n",
    "        self.num_workers = num_workers\n",
    "        self.drop_last = drop_last\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "    def train_epoch(self, train_loader):\n",
    "        \"\"\"\n",
    "        Trains one epoch of the network in self.network\n",
    "        Parameters\n",
    "        ----------\n",
    "            train_loader: a :class: `torch.utils.data.Dataloader`\n",
    "                DataLoader with train set\n",
    "        \"\"\"\n",
    "\n",
    "        self.network.train()\n",
    "        y_preds = []\n",
    "        ys = []\n",
    "        total_loss = 0\n",
    "\n",
    "        for data, targets in train_loader:\n",
    "            batch_outs = self.train_batch(data, targets)\n",
    "            y_preds.append(batch_outs[\"y_preds\"].cpu().detach().numpy())\n",
    "            ys.append(batch_outs[\"y\"].cpu().detach().numpy())\n",
    "            total_loss += batch_outs[\"loss\"]\n",
    "\n",
    "        y_preds = np.vstack(y_preds)\n",
    "        ys = np.vstack(ys)\n",
    "\n",
    "        #stopping_loss = mean_squared_error(y_true=ys, y_pred=y_preds)\n",
    "        stopping_loss =log_loss_multi(ys, torch.sigmoid(torch.as_tensor(y_preds)).numpy()  )\n",
    "        total_loss = total_loss / len(train_loader)\n",
    "        epoch_metrics = {'loss_avg': total_loss,\n",
    "                         'stopping_loss': total_loss,\n",
    "                         }\n",
    "\n",
    "        if self.scheduler is not None:\n",
    "            self.scheduler.step()\n",
    "        return epoch_metrics\n",
    "\n",
    "    def train_batch(self, data, targets):\n",
    "        \"\"\"\n",
    "        Trains one batch of data\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.tensor`\n",
    "                Target data\n",
    "        \"\"\"\n",
    "        self.network.train()\n",
    "        data = data.to(self.device).float()\n",
    "\n",
    "        targets = targets.to(self.device).float()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        output, M_loss = self.network(data)\n",
    "        \n",
    "        #Added by Radu:\n",
    "        y_smo = targets * (1 - self.label_smoothing) + 0.5 * self.label_smoothing\n",
    "\n",
    "        loss = self.loss_fn(output, y_smo)\n",
    "        \n",
    "        loss -= self.lambda_sparse*M_loss\n",
    "\n",
    "        loss.backward()\n",
    "        if self.clip_value:\n",
    "            clip_grad_norm_(self.network.parameters(), self.clip_value)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        loss_value = loss.item()\n",
    "        batch_outs = {'loss': loss_value,\n",
    "                      'y_preds': output,\n",
    "                      'y': targets}\n",
    "        return batch_outs\n",
    "\n",
    "    def predict_epoch(self, loader):\n",
    "        \"\"\"\n",
    "        Validates one epoch of the network in self.network\n",
    "        Parameters\n",
    "        ----------\n",
    "            loader: a :class: `torch.utils.data.Dataloader`\n",
    "                    DataLoader with validation set\n",
    "        \"\"\"\n",
    "        y_preds = []\n",
    "        ys = []\n",
    "        self.network.eval()\n",
    "        total_loss = 0\n",
    "\n",
    "        for data, targets in loader:\n",
    "            batch_outs = self.predict_batch(data, targets)\n",
    "            total_loss += batch_outs[\"loss\"]\n",
    "            y_preds.append(batch_outs[\"y_preds\"].cpu().detach().numpy())\n",
    "            ys.append(batch_outs[\"y\"].cpu().detach().numpy())\n",
    "\n",
    "        y_preds = np.vstack(y_preds)\n",
    "        ys = np.vstack(ys)\n",
    "\n",
    "        stopping_loss =log_loss_multi(ys, torch.sigmoid(torch.as_tensor(y_preds)).numpy()  ) #mean_squared_error(y_true=ys, y_pred=y_preds)\n",
    "\n",
    "        total_loss = total_loss / len(loader)\n",
    "        epoch_metrics = {'total_loss': total_loss,\n",
    "                         'stopping_loss': stopping_loss}\n",
    "\n",
    "        return epoch_metrics\n",
    "\n",
    "    def predict_batch(self, data, targets):\n",
    "        \"\"\"\n",
    "        Make predictions on a batch (valid)\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            batch_outs: dict\n",
    "        \"\"\"\n",
    "        self.network.eval()\n",
    "        data = data.to(self.device).float()\n",
    "        targets = targets.to(self.device).float()\n",
    "\n",
    "        output, M_loss = self.network(data)\n",
    "       \n",
    "        #y_smo = targets * (1 - self.label_smoothing) + 0.5 * self.label_smoothing\n",
    "        loss = self.loss_fn(output, targets)\n",
    "        #print(self.loss_fn, loss)\n",
    "        loss -= self.lambda_sparse*M_loss\n",
    "        #print(loss)\n",
    "        loss_value = loss.item()\n",
    "        batch_outs = {'loss': loss_value,\n",
    "                      'y_preds': output,\n",
    "                      'y': targets}\n",
    "        return batch_outs\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on a batch (valid)\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            predictions: np.array\n",
    "                Predictions of the regression problem\n",
    "        \"\"\"\n",
    "        self.network.eval()\n",
    "        dataloader = DataLoader(PredictDataset(X),\n",
    "                                batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        results = []\n",
    "        for batch_nb, data in enumerate(dataloader):\n",
    "            data = data.to(self.device).float()\n",
    "\n",
    "            output, M_loss = self.network(data)\n",
    "            predictions = output.cpu().detach().numpy()\n",
    "            results.append(predictions)\n",
    "        res = np.vstack(results)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039253,
     "end_time": "2020-11-26T20:38:57.649303",
     "exception": false,
     "start_time": "2020-11-26T20:38:57.610050",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T20:38:57.733326Z",
     "iopub.status.busy": "2020-11-26T20:38:57.732065Z",
     "iopub.status.idle": "2020-11-26T20:38:57.736409Z",
     "shell.execute_reply": "2020-11-26T20:38:57.735793Z"
    },
    "papermill": {
     "duration": 0.047651,
     "end_time": "2020-11-26T20:38:57.736503",
     "exception": false,
     "start_time": "2020-11-26T20:38:57.688852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold, MultilabelStratifiedShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T20:38:57.824968Z",
     "iopub.status.busy": "2020-11-26T20:38:57.823563Z",
     "iopub.status.idle": "2020-11-26T20:38:57.827307Z",
     "shell.execute_reply": "2020-11-26T20:38:57.827758Z"
    },
    "papermill": {
     "duration": 0.052829,
     "end_time": "2020-11-26T20:38:57.827901",
     "exception": false,
     "start_time": "2020-11-26T20:38:57.775072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        self.num_class = targets_tr.shape[1]\n",
    "        self.verbose=False\n",
    "        self.seed = 64\n",
    "        self.SPLITS= 10\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.EPOCHS = 200\n",
    "        self.num_ensembling = 1\n",
    "        # Parameters model\n",
    "        self.cat_emb_dim=[1] * cat_tr.shape[1] #to choose\n",
    "        self.cats_idx = list(range(cat_tr.shape[1]))\n",
    "        self.cat_dims = [len(np.unique(cat_tr[:, i])) for i in range(cat_tr.shape[1])]\n",
    "        #self.num_numericals= numerical_tr.shape[1]\n",
    "    \n",
    "        # save\n",
    "        self.save_name = \"tabnet_raw_step1\"\n",
    "        \n",
    "        self.strategy = \"KFOLD\" # \n",
    "cfg = Config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T20:38:57.918943Z",
     "iopub.status.busy": "2020-11-26T20:38:57.917864Z",
     "iopub.status.idle": "2020-11-26T20:38:57.919958Z",
     "shell.execute_reply": "2020-11-26T20:38:57.920459Z"
    },
    "papermill": {
     "duration": 0.054133,
     "end_time": "2020-11-26T20:38:57.920588",
     "exception": false,
     "start_time": "2020-11-26T20:38:57.866455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#X_test = np.concatenate([cat_test, numerical_test ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T20:38:58.018701Z",
     "iopub.status.busy": "2020-11-26T20:38:58.017291Z",
     "iopub.status.idle": "2020-11-26T20:38:58.593214Z",
     "shell.execute_reply": "2020-11-26T20:38:58.593891Z"
    },
    "papermill": {
     "duration": 0.633056,
     "end_time": "2020-11-26T20:38:58.594082",
     "exception": false,
     "start_time": "2020-11-26T20:38:57.961026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass shuffle=True, random_state=64 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass shuffle=True, random_state=64 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "SEED = 64\n",
    "NFOLDS = cfg.SPLITS\n",
    "\n",
    "# LOAD LIBRARIES (from PIP or Kaggle Dataset) \n",
    "from sklearn.model_selection import KFold\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "# LOAD FILES\n",
    "scored = train_targets_scored.copy()\n",
    "drug = pd.read_csv('/kaggle/input/lish-moa/train_drug.csv')\n",
    "targets = scored.columns[1:]\n",
    "scored = scored.merge(drug, on='sig_id', how='left') \n",
    "\n",
    "# LOCATE DRUGS\n",
    "vc = scored.drug_id.value_counts()\n",
    "vc1 = vc.loc[vc<=18].index.sort_values()\n",
    "vc2 = vc.loc[vc>18].index.sort_values()\n",
    "\n",
    "# STRATIFY DRUGS 18X OR LESS\n",
    "dct1 = {}; dct2 = {}\n",
    "skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, \n",
    "          random_state=SEED)\n",
    "tmp = scored.groupby('drug_id')[targets].mean().loc[vc1]\n",
    "for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n",
    "    dd = {k:fold for k in tmp.index[idxV].values}\n",
    "    dct1.update(dd)\n",
    "\n",
    "# STRATIFY DRUGS MORE THAN 18X\n",
    "skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, \n",
    "          random_state=SEED)\n",
    "tmp = scored.loc[scored.drug_id.isin(vc2)].reset_index(drop=True)\n",
    "for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n",
    "    dd = {k:fold for k in tmp.sig_id[idxV].values}\n",
    "    dct2.update(dd)\n",
    "\n",
    "# ASSIGN FOLDS\n",
    "scored['kfold'] = scored.drug_id.map(dct1)\n",
    "scored.loc[scored.kfold.isna(),'kfold'] = scored.loc[scored.kfold.isna(),'sig_id'].map(dct2)\n",
    "scored.kfold = scored.kfold.astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T20:38:58.681304Z",
     "iopub.status.busy": "2020-11-26T20:38:58.680512Z",
     "iopub.status.idle": "2020-11-26T20:38:58.683547Z",
     "shell.execute_reply": "2020-11-26T20:38:58.684032Z"
    },
    "papermill": {
     "duration": 0.049196,
     "end_time": "2020-11-26T20:38:58.684155",
     "exception": false,
     "start_time": "2020-11-26T20:38:58.634959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21948, 209)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scored.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T20:38:58.775928Z",
     "iopub.status.busy": "2020-11-26T20:38:58.775198Z",
     "iopub.status.idle": "2020-11-26T20:38:58.902460Z",
     "shell.execute_reply": "2020-11-26T20:38:58.901910Z"
    },
    "papermill": {
     "duration": 0.176542,
     "end_time": "2020-11-26T20:38:58.902587",
     "exception": false,
     "start_time": "2020-11-26T20:38:58.726045",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = pd.merge(pd.DataFrame(train_data), scored[['kfold']], how='left', left_index=True, right_index=True)\n",
    "targets_tr = pd.merge(pd.DataFrame(targets_tr), scored[['kfold']], how='left', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T20:38:58.989733Z",
     "iopub.status.busy": "2020-11-26T20:38:58.989007Z",
     "iopub.status.idle": "2020-11-26T20:38:58.993164Z",
     "shell.execute_reply": "2020-11-26T20:38:58.993592Z"
    },
    "papermill": {
     "duration": 0.049991,
     "end_time": "2020-11-26T20:38:58.993737",
     "exception": false,
     "start_time": "2020-11-26T20:38:58.943746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for seed in range(2):\n",
    "    print(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T20:38:59.106412Z",
     "iopub.status.busy": "2020-11-26T20:38:59.105339Z",
     "iopub.status.idle": "2020-11-26T21:35:43.475268Z",
     "shell.execute_reply": "2020-11-26T21:35:43.474280Z"
    },
    "papermill": {
     "duration": 3404.438927,
     "end_time": "2020-11-26T21:35:43.475383",
     "exception": false,
     "start_time": "2020-11-26T20:38:59.036456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## SEED :  0\n",
      "FOLDS :  0\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 50 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     |  0.38890 |   0.05573 |   2.5        True\n",
      "| 2     |  0.03345 |   0.02831 |   4.6        True\n",
      "| 3     |  0.02625 |   0.02198 |   6.7        True\n",
      "| 4     |  0.02459 |   0.02138 |   8.8        True\n",
      "| 5     |  0.02412 |   0.02124 |   10.7       True\n",
      "| 6     |  0.02392 |   0.02098 |   12.9       True\n",
      "| 7     |  0.02366 |   0.02078 |   14.9       True\n",
      "| 8     |  0.02343 |   0.02064 |   17.0       True\n",
      "| 9     |  0.02317 |   0.02042 |   19.3       True\n",
      "| 10    |  0.02285 |   0.02026 |   21.4       True\n",
      "| 11    |  0.02260 |   0.01997 |   23.6       True\n",
      "| 12    |  0.02244 |   0.01997 |   25.6       True\n",
      "| 13    |  0.02214 |   0.01948 |   27.7       True\n",
      "| 14    |  0.02206 |   0.01949 |   29.8       False\n",
      "| 15    |  0.02182 |   0.01948 |   31.9       True\n",
      "| 16    |  0.02168 |   0.02086 |   34.1       False\n",
      "| 17    |  0.02150 |   0.01915 |   36.1       True\n",
      "| 18    |  0.02130 |   0.02150 |   38.3       False\n",
      "| 19    |  0.02122 |   0.01908 |   40.3       True\n",
      "| 20    |  0.02103 |   0.01915 |   42.3       False\n",
      "| 21    |  0.02095 |   0.01884 |   44.5       True\n",
      "| 22    |  0.02087 |   0.02072 |   46.6       False\n",
      "| 23    |  0.02072 |   0.01887 |   48.6       False\n",
      "| 24    |  0.02071 |   0.02105 |   50.9       False\n",
      "| 25    |  0.02060 |   0.02081 |   53.1       False\n",
      "| 26    |  0.02043 |   0.02011 |   55.4       False\n",
      "| 27    |  0.02038 |   0.01835 |   57.9       True\n",
      "| 28    |  0.02037 |   0.01994 |   60.3       False\n",
      "| 29    |  0.02030 |   0.02035 |   62.3       False\n",
      "| 30    |  0.02014 |   0.01835 |   64.5       True\n",
      "| 31    |  0.02008 |   0.02084 |   66.7       False\n",
      "| 32    |  0.02028 |   0.01897 |   68.8       False\n",
      "| 33    |  0.02018 |   0.01991 |   70.8       False\n",
      "| 34    |  0.02009 |   0.01868 |   72.8       False\n",
      "| 35    |  0.01993 |   0.01849 |   74.9       False\n",
      "| 36    |  0.01988 |   0.01830 |   76.9       True\n",
      "| 37    |  0.01983 |   0.01838 |   79.2       False\n",
      "| 38    |  0.01979 |   0.01804 |   81.4       True\n",
      "| 39    |  0.01977 |   0.01851 |   83.5       False\n",
      "| 40    |  0.01975 |   0.01860 |   85.8       False\n",
      "| 41    |  0.01981 |   0.01808 |   87.9       False\n",
      "| 42    |  0.01989 |   0.01812 |   90.1       False\n",
      "| 43    |  0.01986 |   0.01855 |   92.2       False\n",
      "| 44    |  0.01979 |   0.01832 |   94.2       False\n",
      "| 45    |  0.01965 |   0.01823 |   96.2       False\n",
      "| 46    |  0.01961 |   0.01807 |   98.3       False\n",
      "| 47    |  0.01965 |   0.01857 |   100.5      False\n",
      "| 48    |  0.01970 |   0.01842 |   102.6      False\n",
      "| 49    |  0.01970 |   0.01817 |   104.6      False\n",
      "| 50    |  0.01956 |   0.01785 |   106.6      True\n",
      "| 51    |  0.01955 |   0.01782 |   108.7      True\n",
      "| 52    |  0.01946 |   0.02024 |   110.9      False\n",
      "| 53    |  0.01965 |   0.01815 |   113.0      False\n",
      "| 54    |  0.01953 |   0.01820 |   115.2      False\n",
      "| 55    |  0.01964 |   0.01931 |   118.1      False\n",
      "| 56    |  0.01947 |   0.01805 |   120.2      False\n",
      "| 57    |  0.01946 |   0.01923 |   122.6      False\n",
      "| 58    |  0.01954 |   0.01796 |   124.8      False\n",
      "| 59    |  0.01950 |   0.01805 |   126.8      False\n",
      "| 60    |  0.01947 |   0.01803 |   128.8      False\n",
      "| 61    |  0.01942 |   0.01854 |   130.9      False\n",
      "| 62    |  0.01934 |   0.01822 |   133.1      False\n",
      "| 63    |  0.01944 |   0.01802 |   135.2      False\n",
      "| 64    |  0.01946 |   0.01972 |   137.1      False\n",
      "| 65    |  0.01952 |   0.01859 |   139.1      False\n",
      "| 66    |  0.01942 |   0.01812 |   141.2      False\n",
      "| 67    |  0.01949 |   0.01815 |   143.2      False\n",
      "| 68    |  0.01937 |   0.01813 |   145.6      False\n",
      "| 69    |  0.01944 |   0.01795 |   147.7      False\n",
      "| 70    |  0.01942 |   0.01797 |   149.8      False\n",
      "| 71    |  0.01936 |   0.01891 |   152.2      False\n",
      "| 72    |  0.01937 |   0.01831 |   154.2      False\n",
      "| 73    |  0.01929 |   0.01838 |   156.4      False\n",
      "| 74    |  0.01944 |   0.01810 |   158.4      False\n",
      "| 75    |  0.01946 |   0.01809 |   160.4      False\n",
      "| 76    |  0.01939 |   0.01811 |   162.5      False\n",
      "| 77    |  0.01935 |   0.01797 |   164.5      False\n",
      "| 78    |  0.01938 |   0.01782 |   166.8      True\n",
      "| 79    |  0.01934 |   0.01813 |   168.8      False\n",
      "| 80    |  0.01928 |   0.01790 |   170.8      False\n",
      "| 81    |  0.01918 |   0.01805 |   172.9      False\n",
      "| 82    |  0.01939 |   0.01800 |   174.9      False\n",
      "| 83    |  0.01936 |   0.01783 |   177.5      False\n",
      "| 84    |  0.01929 |   0.01815 |   179.9      False\n",
      "| 85    |  0.01927 |   0.01823 |   182.0      False\n",
      "| 86    |  0.01931 |   0.01807 |   184.5      False\n",
      "| 87    |  0.01943 |   0.01810 |   186.5      False\n",
      "| 88    |  0.01930 |   0.01792 |   188.8      False\n",
      "| 89    |  0.01918 |   0.01787 |   190.8      False\n",
      "| 90    |  0.01931 |   0.01781 |   192.8      True\n",
      "| 91    |  0.01927 |   0.01795 |   194.9      False\n",
      "| 92    |  0.01931 |   0.01808 |   196.9      False\n",
      "| 93    |  0.01945 |   0.01788 |   199.3      False\n",
      "| 94    |  0.01924 |   0.01800 |   201.5      False\n",
      "| 95    |  0.01936 |   0.01804 |   203.5      False\n",
      "| 96    |  0.01925 |   0.01787 |   205.6      False\n",
      "| 97    |  0.01913 |   0.01808 |   207.7      False\n",
      "| 98    |  0.01917 |   0.01788 |   210.1      False\n",
      "| 99    |  0.01906 |   0.01794 |   212.1      False\n",
      "| 100   |  0.01918 |   0.01804 |   214.1      False\n",
      "| 101   |  0.01938 |   0.01815 |   216.3      False\n",
      "| 102   |  0.01922 |   0.01782 |   218.6      False\n",
      "| 103   |  0.01909 |   0.01812 |   220.9      False\n",
      "| 104   |  0.01909 |   0.01803 |   222.9      False\n",
      "| 105   |  0.01907 |   0.01792 |   224.9      False\n",
      "| 106   |  0.01912 |   0.01780 |   227.0      True\n",
      "| 107   |  0.01912 |   0.01802 |   229.0      False\n",
      "| 108   |  0.01910 |   0.01830 |   231.1      False\n",
      "| 109   |  0.01912 |   0.01803 |   233.3      False\n",
      "| 110   |  0.01907 |   0.01810 |   235.6      False\n",
      "| 111   |  0.01900 |   0.01838 |   238.1      False\n",
      "| 112   |  0.01914 |   0.01805 |   240.2      False\n",
      "| 113   |  0.01914 |   0.01804 |   242.3      False\n",
      "| 114   |  0.01908 |   0.01779 |   244.6      True\n",
      "| 115   |  0.01915 |   0.01799 |   246.7      False\n",
      "| 116   |  0.01916 |   0.01849 |   248.8      False\n",
      "| 117   |  0.01931 |   0.01832 |   251.1      False\n",
      "| 118   |  0.01923 |   0.01815 |   253.4      False\n",
      "| 119   |  0.01926 |   0.01802 |   255.5      False\n",
      "| 120   |  0.01907 |   0.01804 |   257.5      False\n",
      "| 121   |  0.01905 |   0.01825 |   259.6      False\n",
      "| 122   |  0.01908 |   0.01805 |   261.6      False\n",
      "| 123   |  0.01919 |   0.01816 |   263.6      False\n",
      "| 124   |  0.01925 |   0.01815 |   265.9      False\n",
      "| 125   |  0.01915 |   0.01787 |   267.9      False\n",
      "| 126   |  0.01906 |   0.01809 |   269.9      False\n",
      "| 127   |  0.01917 |   0.01800 |   272.0      False\n",
      "| 128   |  0.01937 |   0.01829 |   274.0      False\n",
      "| 129   |  0.01933 |   0.01835 |   276.3      False\n",
      "| 130   |  0.01916 |   0.01796 |   278.4      False\n",
      "| 131   |  0.01905 |   0.01797 |   280.5      False\n",
      "| 132   |  0.01909 |   0.01800 |   282.7      False\n",
      "| 133   |  0.01930 |   0.01836 |   284.9      False\n",
      "| 134   |  0.01916 |   0.01797 |   287.1      False\n",
      "| 135   |  0.01913 |   0.01809 |   289.2      False\n",
      "| 136   |  0.01901 |   0.01803 |   291.2      False\n",
      "| 137   |  0.01917 |   0.01802 |   293.1      False\n",
      "| 138   |  0.01913 |   0.01788 |   295.5      False\n",
      "| 139   |  0.01913 |   0.01808 |   297.7      False\n",
      "| 140   |  0.01910 |   0.01805 |   300.1      False\n",
      "| 141   |  0.01910 |   0.01810 |   302.3      False\n",
      "| 142   |  0.01907 |   0.01783 |   304.3      False\n",
      "| 143   |  0.01907 |   0.01780 |   306.4      False\n",
      "| 144   |  0.01906 |   0.01798 |   308.7      False\n",
      "| 145   |  0.01911 |   0.01816 |   310.8      False\n",
      "| 146   |  0.01919 |   0.01865 |   312.8      False\n",
      "| 147   |  0.01917 |   0.01798 |   315.1      False\n",
      "| 148   |  0.01902 |   0.01825 |   317.3      False\n",
      "| 149   |  0.01917 |   0.01829 |   319.4      False\n",
      "| 150   |  0.01914 |   0.01834 |   321.6      False\n",
      "| 151   |  0.01903 |   0.01810 |   323.6      False\n",
      "| 152   |  0.01909 |   0.01815 |   325.6      False\n",
      "| 153   |  0.01910 |   0.01817 |   327.7      False\n",
      "| 154   |  0.01925 |   0.01825 |   329.7      False\n",
      "| 155   |  0.01906 |   0.01802 |   332.0      False\n",
      "| 156   |  0.01903 |   0.01809 |   334.1      False\n",
      "| 157   |  0.01899 |   0.01781 |   336.1      False\n",
      "| 158   |  0.01896 |   0.01820 |   338.2      False\n",
      "| 159   |  0.01901 |   0.01797 |   340.3      False\n",
      "| 160   |  0.01897 |   0.01793 |   342.6      False\n",
      "| 161   |  0.01896 |   0.01801 |   344.6      False\n",
      "| 162   |  0.01903 |   0.01783 |   346.7      False\n",
      "| 163   |  0.01890 |   0.01801 |   349.0      False\n",
      "| 164   |  0.01900 |   0.01785 |   351.1      False\n",
      "Early stopping occured at epoch 164\n",
      "Training done in 351.061 seconds.\n",
      "---------------------------------------\n",
      "Successfully saved model at tabnet_raw_step1_fold0_0.zip\n",
      "validation fold 0 : 0.017789696181052844\n",
      "FOLDS :  1\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 50 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     |  0.39515 |   0.05436 |   2.4        True\n",
      "| 2     |  0.03346 |   0.02667 |   4.4        True\n",
      "| 3     |  0.02682 |   0.02214 |   7.0        True\n",
      "| 4     |  0.02469 |   0.02176 |   9.1        True\n",
      "| 5     |  0.02421 |   0.02121 |   11.6       True\n",
      "| 6     |  0.02396 |   0.02101 |   13.6       True\n",
      "| 7     |  0.02384 |   0.02095 |   15.7       True\n",
      "| 8     |  0.02357 |   0.02176 |   17.7       False\n",
      "| 9     |  0.02332 |   0.02049 |   19.8       True\n",
      "| 10    |  0.02299 |   0.02093 |   22.1       False\n",
      "| 11    |  0.02262 |   0.02007 |   24.1       True\n",
      "| 12    |  0.02230 |   0.01979 |   26.2       True\n",
      "| 13    |  0.02206 |   0.01987 |   28.5       False\n",
      "| 14    |  0.02181 |   0.02010 |   30.6       False\n",
      "| 15    |  0.02168 |   0.01971 |   32.9       True\n",
      "| 16    |  0.02149 |   0.01940 |   35.0       True\n",
      "| 17    |  0.02137 |   0.01949 |   37.0       False\n",
      "| 18    |  0.02117 |   0.01920 |   39.0       True\n",
      "| 19    |  0.02100 |   0.01895 |   41.1       True\n",
      "| 20    |  0.02094 |   0.01882 |   43.4       True\n",
      "| 21    |  0.02072 |   0.01895 |   45.5       False\n",
      "| 22    |  0.02074 |   0.01902 |   47.6       False\n",
      "| 23    |  0.02065 |   0.01883 |   49.6       False\n",
      "| 24    |  0.02050 |   0.01861 |   51.7       True\n",
      "| 25    |  0.02051 |   0.01839 |   53.8       True\n",
      "| 26    |  0.02039 |   0.01914 |   56.0       False\n",
      "| 27    |  0.02038 |   0.01884 |   58.1       False\n",
      "| 28    |  0.02029 |   0.01953 |   60.2       False\n",
      "| 29    |  0.02031 |   0.01882 |   62.9       False\n",
      "| 30    |  0.02017 |   0.01946 |   65.0       False\n",
      "| 31    |  0.02005 |   0.02151 |   67.8       False\n",
      "| 32    |  0.02001 |   0.01825 |   69.8       True\n",
      "| 33    |  0.01999 |   0.01971 |   72.0       False\n",
      "| 34    |  0.01999 |   0.02194 |   74.1       False\n",
      "| 35    |  0.01990 |   0.01842 |   76.3       False\n",
      "| 36    |  0.01990 |   0.01861 |   78.5       False\n",
      "| 37    |  0.01982 |   0.01904 |   80.5       False\n",
      "| 38    |  0.01984 |   0.01819 |   82.7       True\n",
      "| 39    |  0.01972 |   0.01850 |   84.7       False\n",
      "| 40    |  0.01969 |   0.01845 |   86.8       False\n",
      "| 41    |  0.01958 |   0.02139 |   89.1       False\n",
      "| 42    |  0.01966 |   0.01982 |   91.2       False\n",
      "| 43    |  0.01968 |   0.01830 |   93.2       False\n",
      "| 44    |  0.01957 |   0.01837 |   95.6       False\n",
      "| 45    |  0.01957 |   0.01863 |   97.5       False\n",
      "| 46    |  0.01958 |   0.01851 |   99.9       False\n",
      "| 47    |  0.01947 |   0.02043 |   101.8      False\n",
      "| 48    |  0.01952 |   0.01855 |   103.8      False\n",
      "| 49    |  0.01954 |   0.01877 |   105.9      False\n",
      "| 50    |  0.01957 |   0.01830 |   107.9      False\n",
      "| 51    |  0.01940 |   0.01811 |   110.1      True\n",
      "| 52    |  0.01936 |   0.01846 |   112.3      False\n",
      "| 53    |  0.01932 |   0.01820 |   114.3      False\n",
      "| 54    |  0.01942 |   0.01833 |   116.3      False\n",
      "| 55    |  0.01957 |   0.01838 |   118.3      False\n",
      "| 56    |  0.01937 |   0.01894 |   120.6      False\n",
      "| 57    |  0.01929 |   0.02077 |   123.0      False\n",
      "| 58    |  0.01927 |   0.01839 |   125.3      False\n",
      "| 59    |  0.01928 |   0.01876 |   127.7      False\n",
      "| 60    |  0.01931 |   0.01819 |   129.8      False\n",
      "| 61    |  0.01936 |   0.01814 |   132.1      False\n",
      "| 62    |  0.01928 |   0.01811 |   134.2      False\n",
      "| 63    |  0.01922 |   0.01852 |   136.3      False\n",
      "| 64    |  0.01931 |   0.01809 |   138.3      True\n",
      "| 65    |  0.01933 |   0.01834 |   140.5      False\n",
      "| 66    |  0.01930 |   0.01808 |   142.7      True\n",
      "| 67    |  0.01923 |   0.01798 |   144.8      True\n",
      "| 68    |  0.01915 |   0.01834 |   146.9      False\n",
      "| 69    |  0.01926 |   0.01808 |   148.9      False\n",
      "| 70    |  0.01915 |   0.01800 |   151.0      False\n",
      "| 71    |  0.01926 |   0.01812 |   153.1      False\n",
      "| 72    |  0.01918 |   0.01818 |   155.3      False\n",
      "| 73    |  0.01919 |   0.01833 |   157.6      False\n",
      "| 74    |  0.01926 |   0.01801 |   159.8      False\n",
      "| 75    |  0.01916 |   0.01838 |   162.2      False\n",
      "| 76    |  0.01911 |   0.01822 |   164.5      False\n",
      "| 77    |  0.01940 |   0.01810 |   166.6      False\n",
      "| 78    |  0.01926 |   0.01811 |   168.8      False\n",
      "| 79    |  0.01918 |   0.01926 |   170.9      False\n",
      "| 80    |  0.01918 |   0.01818 |   173.1      False\n",
      "| 81    |  0.01920 |   0.01814 |   175.4      False\n",
      "| 82    |  0.01913 |   0.01895 |   177.5      False\n",
      "| 83    |  0.01920 |   0.01821 |   179.6      False\n",
      "| 84    |  0.01917 |   0.01811 |   181.9      False\n",
      "| 85    |  0.01913 |   0.01865 |   184.5      False\n",
      "| 86    |  0.01916 |   0.01822 |   186.7      False\n",
      "| 87    |  0.01925 |   0.01810 |   189.1      False\n",
      "| 88    |  0.01920 |   0.01784 |   191.3      True\n",
      "| 89    |  0.01912 |   0.01809 |   193.8      False\n",
      "| 90    |  0.01914 |   0.01806 |   195.9      False\n",
      "| 91    |  0.01909 |   0.01814 |   198.2      False\n",
      "| 92    |  0.01910 |   0.01865 |   200.2      False\n",
      "| 93    |  0.01920 |   0.01830 |   202.4      False\n",
      "| 94    |  0.01917 |   0.01801 |   204.5      False\n",
      "| 95    |  0.01921 |   0.01865 |   206.6      False\n",
      "| 96    |  0.01954 |   0.01815 |   209.1      False\n",
      "| 97    |  0.01919 |   0.01816 |   211.2      False\n",
      "| 98    |  0.01902 |   0.01903 |   213.2      False\n",
      "| 99    |  0.01916 |   0.01952 |   215.4      False\n",
      "| 100   |  0.01934 |   0.01952 |   217.5      False\n",
      "| 101   |  0.01914 |   0.01822 |   219.8      False\n",
      "| 102   |  0.01893 |   0.01803 |   222.0      False\n",
      "| 103   |  0.01893 |   0.01818 |   224.1      False\n",
      "| 104   |  0.01893 |   0.01817 |   226.5      False\n",
      "| 105   |  0.01892 |   0.01830 |   228.7      False\n",
      "| 106   |  0.01901 |   0.01824 |   231.0      False\n",
      "| 107   |  0.01899 |   0.01860 |   233.1      False\n",
      "| 108   |  0.01905 |   0.01811 |   235.2      False\n",
      "| 109   |  0.01894 |   0.01848 |   237.4      False\n",
      "| 110   |  0.01898 |   0.01800 |   239.5      False\n",
      "| 111   |  0.01922 |   0.01791 |   242.1      False\n",
      "| 112   |  0.01902 |   0.01829 |   244.3      False\n",
      "| 113   |  0.01899 |   0.01838 |   246.5      False\n",
      "| 114   |  0.01893 |   0.01826 |   249.0      False\n",
      "| 115   |  0.01898 |   0.01812 |   251.1      False\n",
      "| 116   |  0.01906 |   0.01843 |   253.5      False\n",
      "| 117   |  0.01901 |   0.01880 |   255.7      False\n",
      "| 118   |  0.01897 |   0.01995 |   257.8      False\n",
      "| 119   |  0.01908 |   0.01824 |   260.3      False\n",
      "| 120   |  0.01897 |   0.01849 |   262.5      False\n",
      "| 121   |  0.01888 |   0.01839 |   264.9      False\n",
      "| 122   |  0.01890 |   0.01860 |   267.1      False\n",
      "| 123   |  0.01895 |   0.01862 |   269.3      False\n",
      "| 124   |  0.01899 |   0.01849 |   271.4      False\n",
      "| 125   |  0.01899 |   0.01831 |   273.4      False\n",
      "| 126   |  0.01899 |   0.01793 |   275.8      False\n",
      "| 127   |  0.01900 |   0.01831 |   277.9      False\n",
      "| 128   |  0.01902 |   0.01853 |   280.0      False\n",
      "| 129   |  0.01893 |   0.01821 |   282.1      False\n",
      "| 130   |  0.01896 |   0.01851 |   284.2      False\n",
      "| 131   |  0.01902 |   0.02079 |   286.5      False\n",
      "| 132   |  0.01909 |   0.01787 |   288.7      False\n",
      "| 133   |  0.01891 |   0.01823 |   290.8      False\n",
      "| 134   |  0.01893 |   0.01826 |   293.2      False\n",
      "| 135   |  0.01913 |   0.01805 |   295.4      False\n",
      "| 136   |  0.01908 |   0.01809 |   297.7      False\n",
      "| 137   |  0.01890 |   0.01794 |   299.9      False\n",
      "| 138   |  0.01890 |   0.01811 |   302.3      False\n",
      "Early stopping occured at epoch 138\n",
      "Training done in 302.339 seconds.\n",
      "---------------------------------------\n",
      "Successfully saved model at tabnet_raw_step1_fold1_0.zip\n",
      "validation fold 1 : 0.017838912915994494\n",
      "FOLDS :  2\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 50 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     |  0.39058 |   0.05405 |   2.2        True\n",
      "| 2     |  0.03332 |   0.02873 |   5.1        True\n",
      "| 3     |  0.02659 |   0.02153 |   7.2        True\n",
      "| 4     |  0.02445 |   0.02105 |   9.3        True\n",
      "| 5     |  0.02397 |   0.02084 |   11.6       True\n",
      "| 6     |  0.02381 |   0.02082 |   13.9       True\n",
      "| 7     |  0.02369 |   0.02066 |   16.2       True\n",
      "| 8     |  0.02345 |   0.02060 |   18.3       True\n",
      "| 9     |  0.02310 |   0.02171 |   20.6       False\n",
      "| 10    |  0.02267 |   0.01996 |   22.9       True\n",
      "| 11    |  0.02243 |   0.01976 |   25.3       True\n",
      "| 12    |  0.02222 |   0.01984 |   27.5       False\n",
      "| 13    |  0.02190 |   0.01954 |   29.6       True\n",
      "| 14    |  0.02180 |   0.02007 |   31.7       False\n",
      "| 15    |  0.02155 |   0.01960 |   33.8       False\n",
      "| 16    |  0.02155 |   0.01927 |   36.2       True\n",
      "| 17    |  0.02135 |   0.01910 |   38.3       True\n",
      "| 18    |  0.02129 |   0.01899 |   40.4       True\n",
      "| 19    |  0.02118 |   0.01881 |   42.6       True\n",
      "| 20    |  0.02107 |   0.01864 |   44.7       True\n",
      "| 21    |  0.02097 |   0.01879 |   47.0       False\n",
      "| 22    |  0.02096 |   0.01889 |   49.2       False\n",
      "| 23    |  0.02085 |   0.01914 |   51.3       False\n",
      "| 24    |  0.02072 |   0.01828 |   53.6       True\n",
      "| 25    |  0.02057 |   0.01837 |   55.9       False\n",
      "| 26    |  0.02040 |   0.01970 |   58.8       False\n",
      "| 27    |  0.02044 |   0.01832 |   61.3       False\n",
      "| 28    |  0.02034 |   0.02242 |   63.4       False\n",
      "| 29    |  0.02047 |   0.01857 |   65.5       False\n",
      "| 30    |  0.02024 |   0.02136 |   67.8       False\n",
      "| 31    |  0.02009 |   0.01942 |   70.2       False\n",
      "| 32    |  0.02003 |   0.01826 |   72.4       True\n",
      "| 33    |  0.02014 |   0.01863 |   74.5       False\n",
      "| 34    |  0.02018 |   0.01935 |   76.6       False\n",
      "| 35    |  0.02006 |   0.01862 |   78.7       False\n",
      "| 36    |  0.01996 |   0.01820 |   81.0       True\n",
      "| 37    |  0.01986 |   0.01841 |   83.2       False\n",
      "| 38    |  0.01994 |   0.01798 |   85.3       True\n",
      "| 39    |  0.01998 |   0.01816 |   87.7       False\n",
      "| 40    |  0.01988 |   0.01802 |   89.9       False\n",
      "| 41    |  0.01972 |   0.01807 |   92.2       False\n",
      "| 42    |  0.01971 |   0.01805 |   94.4       False\n",
      "| 43    |  0.01972 |   0.01990 |   96.5       False\n",
      "| 44    |  0.01956 |   0.01839 |   98.5       False\n",
      "| 45    |  0.01968 |   0.01945 |   100.7      False\n",
      "| 46    |  0.01965 |   0.01847 |   103.0      False\n",
      "| 47    |  0.01977 |   0.01799 |   105.2      False\n",
      "| 48    |  0.01961 |   0.01802 |   107.2      False\n",
      "| 49    |  0.01952 |   0.01794 |   109.3      True\n",
      "| 50    |  0.01963 |   0.01803 |   111.5      False\n",
      "| 51    |  0.01960 |   0.01813 |   113.9      False\n",
      "| 52    |  0.01943 |   0.02001 |   116.1      False\n",
      "| 53    |  0.01946 |   0.01988 |   118.5      False\n",
      "| 54    |  0.01949 |   0.01802 |   121.1      False\n",
      "| 55    |  0.01964 |   0.01799 |   123.5      False\n",
      "| 56    |  0.01948 |   0.01791 |   125.9      True\n",
      "| 57    |  0.01942 |   0.01821 |   128.1      False\n",
      "| 58    |  0.01938 |   0.01788 |   130.2      True\n",
      "| 59    |  0.01946 |   0.01808 |   132.3      False\n",
      "| 60    |  0.01947 |   0.01815 |   134.5      False\n",
      "| 61    |  0.01942 |   0.01814 |   136.7      False\n",
      "| 62    |  0.01938 |   0.01798 |   138.9      False\n",
      "| 63    |  0.01916 |   0.01791 |   140.9      False\n",
      "| 64    |  0.01927 |   0.01768 |   143.0      True\n",
      "| 65    |  0.01929 |   0.01821 |   145.1      False\n",
      "| 66    |  0.01935 |   0.01805 |   147.4      False\n",
      "| 67    |  0.01937 |   0.01771 |   149.6      False\n",
      "| 68    |  0.01930 |   0.01810 |   151.7      False\n",
      "| 69    |  0.01932 |   0.01801 |   154.1      False\n",
      "| 70    |  0.01932 |   0.01792 |   156.4      False\n",
      "| 71    |  0.01922 |   0.01796 |   158.6      False\n",
      "| 72    |  0.01929 |   0.01874 |   160.8      False\n",
      "| 73    |  0.01936 |   0.01807 |   162.9      False\n",
      "| 74    |  0.01918 |   0.01791 |   165.0      False\n",
      "| 75    |  0.01923 |   0.01799 |   167.1      False\n",
      "| 76    |  0.01928 |   0.01778 |   169.6      False\n",
      "| 77    |  0.01930 |   0.01806 |   171.9      False\n",
      "| 78    |  0.01926 |   0.01854 |   174.0      False\n",
      "| 79    |  0.01934 |   0.01800 |   176.0      False\n",
      "| 80    |  0.01928 |   0.01788 |   178.6      False\n",
      "| 81    |  0.01927 |   0.01798 |   181.3      False\n",
      "| 82    |  0.01934 |   0.01829 |   183.6      False\n",
      "| 83    |  0.01930 |   0.01836 |   186.1      False\n",
      "| 84    |  0.01932 |   0.01814 |   188.3      False\n",
      "| 85    |  0.01930 |   0.01805 |   190.7      False\n",
      "| 86    |  0.01912 |   0.01817 |   192.8      False\n",
      "| 87    |  0.01921 |   0.01819 |   194.9      False\n",
      "| 88    |  0.01922 |   0.01816 |   196.9      False\n",
      "| 89    |  0.01926 |   0.01778 |   199.0      False\n",
      "| 90    |  0.01922 |   0.01809 |   201.3      False\n",
      "| 91    |  0.01923 |   0.01833 |   203.4      False\n",
      "| 92    |  0.01919 |   0.01802 |   205.6      False\n",
      "| 93    |  0.01932 |   0.01802 |   207.6      False\n",
      "| 94    |  0.01922 |   0.01816 |   209.7      False\n",
      "| 95    |  0.01916 |   0.01807 |   212.1      False\n",
      "| 96    |  0.01918 |   0.01808 |   214.3      False\n",
      "| 97    |  0.01933 |   0.01802 |   216.4      False\n",
      "| 98    |  0.01978 |   0.01835 |   218.6      False\n",
      "| 99    |  0.01963 |   0.01797 |   220.9      False\n",
      "| 100   |  0.01929 |   0.01784 |   223.3      False\n",
      "| 101   |  0.01908 |   0.01788 |   225.4      False\n",
      "| 102   |  0.01911 |   0.01797 |   227.5      False\n",
      "| 103   |  0.01911 |   0.01798 |   229.5      False\n",
      "| 104   |  0.01919 |   0.01804 |   231.6      False\n",
      "| 105   |  0.01909 |   0.01790 |   233.9      False\n",
      "| 106   |  0.01911 |   0.01804 |   236.0      False\n",
      "| 107   |  0.01910 |   0.01801 |   238.5      False\n",
      "| 108   |  0.01907 |   0.01779 |   240.9      False\n",
      "| 109   |  0.01899 |   0.01819 |   243.0      False\n",
      "| 110   |  0.01933 |   0.01834 |   245.5      False\n",
      "| 111   |  0.01909 |   0.01819 |   247.7      False\n",
      "| 112   |  0.01908 |   0.01835 |   249.8      False\n",
      "| 113   |  0.01917 |   0.01793 |   252.1      False\n",
      "| 114   |  0.01910 |   0.01811 |   254.4      False\n",
      "Early stopping occured at epoch 114\n",
      "Training done in 254.364 seconds.\n",
      "---------------------------------------\n",
      "Successfully saved model at tabnet_raw_step1_fold2_0.zip\n",
      "validation fold 2 : 0.017682345672476656\n",
      "FOLDS :  3\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 50 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     |  0.39643 |   0.05418 |   2.1        True\n",
      "| 2     |  0.03300 |   0.02663 |   4.3        True\n",
      "| 3     |  0.02613 |   0.02141 |   6.4        True\n",
      "| 4     |  0.02448 |   0.02091 |   8.5        True\n",
      "| 5     |  0.02405 |   0.02074 |   11.0       True\n",
      "| 6     |  0.02374 |   0.02056 |   13.1       True\n",
      "| 7     |  0.02363 |   0.02045 |   15.2       True\n",
      "| 8     |  0.02335 |   0.02024 |   17.4       True\n",
      "| 9     |  0.02303 |   0.01985 |   19.7       True\n",
      "| 10    |  0.02269 |   0.02062 |   22.1       False\n",
      "| 11    |  0.02234 |   0.01910 |   24.4       True\n",
      "| 12    |  0.02208 |   0.01913 |   26.5       False\n",
      "| 13    |  0.02188 |   0.01905 |   29.0       True\n",
      "| 14    |  0.02167 |   0.01864 |   31.1       True\n",
      "| 15    |  0.02129 |   0.01853 |   33.4       True\n",
      "| 16    |  0.02126 |   0.01863 |   35.6       False\n",
      "| 17    |  0.02112 |   0.01852 |   37.7       True\n",
      "| 18    |  0.02094 |   0.01830 |   39.9       True\n",
      "| 19    |  0.02093 |   0.01903 |   42.5       False\n",
      "| 20    |  0.02081 |   0.01985 |   45.4       False\n",
      "| 21    |  0.02068 |   0.02001 |   47.7       False\n",
      "| 22    |  0.02062 |   0.02000 |   49.9       False\n",
      "| 23    |  0.02061 |   0.01976 |   52.1       False\n",
      "| 24    |  0.02054 |   0.01898 |   54.5       False\n",
      "| 25    |  0.02040 |   0.01801 |   56.7       True\n",
      "| 26    |  0.02036 |   0.02102 |   59.0       False\n",
      "| 27    |  0.02034 |   0.01828 |   61.4       False\n",
      "| 28    |  0.02020 |   0.01801 |   63.6       True\n",
      "| 29    |  0.02002 |   0.02033 |   66.0       False\n",
      "| 30    |  0.02005 |   0.01820 |   68.2       False\n",
      "| 31    |  0.02004 |   0.01849 |   70.3       False\n",
      "| 32    |  0.01996 |   0.01769 |   72.5       True\n",
      "| 33    |  0.01986 |   0.01987 |   74.6       False\n",
      "| 34    |  0.01983 |   0.01866 |   76.9       False\n",
      "| 35    |  0.01983 |   0.01837 |   79.2       False\n",
      "| 36    |  0.01974 |   0.02010 |   81.4       False\n",
      "| 37    |  0.01977 |   0.01765 |   83.7       True\n",
      "| 38    |  0.01968 |   0.01770 |   85.8       False\n",
      "| 39    |  0.01963 |   0.01943 |   88.2       False\n",
      "| 40    |  0.01978 |   0.01871 |   90.4       False\n",
      "| 41    |  0.01973 |   0.01773 |   92.5       False\n",
      "| 42    |  0.01965 |   0.01771 |   95.0       False\n",
      "| 43    |  0.01966 |   0.01773 |   97.2       False\n",
      "| 44    |  0.01961 |   0.01749 |   99.6       True\n",
      "| 45    |  0.01961 |   0.01792 |   102.2      False\n",
      "| 46    |  0.01962 |   0.01761 |   104.3      False\n",
      "| 47    |  0.01962 |   0.01885 |   106.8      False\n",
      "| 48    |  0.01949 |   0.01768 |   109.0      False\n",
      "| 49    |  0.01931 |   0.01795 |   111.4      False\n",
      "| 50    |  0.01949 |   0.01777 |   113.6      False\n",
      "| 51    |  0.01947 |   0.01800 |   115.8      False\n",
      "| 52    |  0.01961 |   0.01770 |   118.0      False\n",
      "| 53    |  0.01954 |   0.01769 |   120.3      False\n",
      "| 54    |  0.01938 |   0.01821 |   122.5      False\n",
      "| 55    |  0.01950 |   0.01751 |   124.7      False\n",
      "| 56    |  0.01933 |   0.01857 |   127.0      False\n",
      "| 57    |  0.01939 |   0.01784 |   129.2      False\n",
      "| 58    |  0.01931 |   0.01829 |   131.6      False\n",
      "| 59    |  0.01941 |   0.01784 |   133.8      False\n",
      "| 60    |  0.01950 |   0.01807 |   136.0      False\n",
      "| 61    |  0.01959 |   0.01766 |   138.1      False\n",
      "| 62    |  0.01943 |   0.01759 |   140.2      False\n",
      "| 63    |  0.01935 |   0.01790 |   142.6      False\n",
      "| 64    |  0.01933 |   0.01774 |   144.7      False\n",
      "| 65    |  0.01924 |   0.01825 |   147.1      False\n",
      "| 66    |  0.01925 |   0.01817 |   149.2      False\n",
      "| 67    |  0.01918 |   0.01750 |   151.3      False\n",
      "| 68    |  0.01931 |   0.01781 |   153.7      False\n",
      "| 69    |  0.01936 |   0.01770 |   155.8      False\n",
      "| 70    |  0.01936 |   0.01846 |   158.0      False\n",
      "| 71    |  0.01931 |   0.01929 |   160.7      False\n",
      "| 72    |  0.01926 |   0.01808 |   163.2      False\n",
      "| 73    |  0.01923 |   0.01772 |   165.8      False\n",
      "| 74    |  0.01916 |   0.01759 |   168.0      False\n",
      "| 75    |  0.01940 |   0.01760 |   170.3      False\n",
      "| 76    |  0.01931 |   0.01775 |   172.4      False\n",
      "| 77    |  0.01942 |   0.01737 |   174.6      True\n",
      "| 78    |  0.01929 |   0.01778 |   176.9      False\n",
      "| 79    |  0.01920 |   0.01981 |   179.2      False\n",
      "| 80    |  0.01936 |   0.01775 |   181.4      False\n",
      "| 81    |  0.01924 |   0.01770 |   183.5      False\n",
      "| 82    |  0.01926 |   0.01798 |   185.7      False\n",
      "| 83    |  0.01925 |   0.01783 |   188.1      False\n",
      "| 84    |  0.01916 |   0.01814 |   190.2      False\n",
      "| 85    |  0.01925 |   0.01754 |   192.7      False\n",
      "| 86    |  0.01922 |   0.01738 |   194.8      False\n",
      "| 87    |  0.01910 |   0.01736 |   197.2      True\n",
      "| 88    |  0.01914 |   0.01777 |   199.4      False\n",
      "| 89    |  0.01921 |   0.01782 |   201.5      False\n",
      "| 90    |  0.01927 |   0.01814 |   203.8      False\n",
      "| 91    |  0.01921 |   0.01755 |   206.0      False\n",
      "| 92    |  0.01936 |   0.01836 |   208.3      False\n",
      "| 93    |  0.01946 |   0.01775 |   210.6      False\n",
      "| 94    |  0.01926 |   0.01772 |   212.7      False\n",
      "| 95    |  0.01926 |   0.01794 |   214.9      False\n",
      "| 96    |  0.01941 |   0.01782 |   217.1      False\n",
      "| 97    |  0.01925 |   0.01807 |   219.4      False\n",
      "| 98    |  0.01946 |   0.01770 |   222.2      False\n",
      "| 99    |  0.01934 |   0.01775 |   224.8      False\n",
      "| 100   |  0.01931 |   0.01762 |   227.2      False\n",
      "| 101   |  0.01924 |   0.01749 |   229.4      False\n",
      "| 102   |  0.01907 |   0.01768 |   231.7      False\n",
      "| 103   |  0.01910 |   0.01759 |   233.8      False\n",
      "| 104   |  0.01915 |   0.01758 |   235.9      False\n",
      "| 105   |  0.01905 |   0.01784 |   238.1      False\n",
      "| 106   |  0.01913 |   0.01813 |   240.2      False\n",
      "| 107   |  0.01914 |   0.01772 |   242.7      False\n",
      "| 108   |  0.01905 |   0.01773 |   244.9      False\n",
      "| 109   |  0.01906 |   0.01776 |   247.0      False\n",
      "| 110   |  0.01907 |   0.01772 |   249.2      False\n",
      "| 111   |  0.01901 |   0.01808 |   251.3      False\n",
      "| 112   |  0.01899 |   0.01773 |   253.7      False\n",
      "| 113   |  0.01927 |   0.01829 |   256.1      False\n",
      "| 114   |  0.01940 |   0.01768 |   258.2      False\n",
      "| 115   |  0.01923 |   0.01766 |   260.4      False\n",
      "| 116   |  0.01915 |   0.01769 |   262.6      False\n",
      "| 117   |  0.01920 |   0.01763 |   264.8      False\n",
      "| 118   |  0.01907 |   0.01774 |   267.0      False\n",
      "| 119   |  0.01907 |   0.01739 |   269.1      False\n",
      "| 120   |  0.01903 |   0.01752 |   271.4      False\n",
      "| 121   |  0.01899 |   0.01768 |   273.7      False\n",
      "| 122   |  0.01905 |   0.01778 |   275.9      False\n",
      "| 123   |  0.01910 |   0.01766 |   278.1      False\n",
      "| 124   |  0.01902 |   0.01774 |   280.2      False\n",
      "| 125   |  0.01910 |   0.01739 |   282.8      False\n",
      "| 126   |  0.01922 |   0.01777 |   284.9      False\n",
      "| 127   |  0.01927 |   0.01777 |   288.2      False\n",
      "| 128   |  0.01922 |   0.01767 |   290.5      False\n",
      "| 129   |  0.01900 |   0.01754 |   292.6      False\n",
      "| 130   |  0.01894 |   0.01754 |   294.8      False\n",
      "| 131   |  0.01915 |   0.01771 |   297.2      False\n",
      "| 132   |  0.01912 |   0.01765 |   299.3      False\n",
      "| 133   |  0.01903 |   0.01765 |   301.5      False\n",
      "| 134   |  0.01899 |   0.01753 |   303.6      False\n",
      "| 135   |  0.01897 |   0.01780 |   305.9      False\n",
      "| 136   |  0.01905 |   0.01738 |   308.2      False\n",
      "| 137   |  0.01904 |   0.01770 |   310.3      False\n",
      "Early stopping occured at epoch 137\n",
      "Training done in 310.331 seconds.\n",
      "---------------------------------------\n",
      "Successfully saved model at tabnet_raw_step1_fold3_0.zip\n",
      "validation fold 3 : 0.017358052611545414\n",
      "FOLDS :  4\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 50 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     |  0.38805 |   0.05331 |   2.1        True\n",
      "| 2     |  0.03356 |   0.02751 |   4.2        True\n",
      "| 3     |  0.02680 |   0.02221 |   6.9        True\n",
      "| 4     |  0.02469 |   0.02182 |   9.1        True\n",
      "| 5     |  0.02414 |   0.02161 |   11.3       True\n",
      "| 6     |  0.02378 |   0.02143 |   13.4       True\n",
      "| 7     |  0.02356 |   0.02122 |   15.5       True\n",
      "| 8     |  0.02326 |   0.02104 |   18.0       True\n",
      "| 9     |  0.02301 |   0.02073 |   20.1       True\n",
      "| 10    |  0.02271 |   0.02054 |   22.2       True\n",
      "| 11    |  0.02237 |   0.02004 |   24.5       True\n",
      "| 12    |  0.02194 |   0.01977 |   26.5       True\n",
      "| 13    |  0.02179 |   0.01966 |   29.3       True\n",
      "| 14    |  0.02150 |   0.01954 |   31.6       True\n",
      "| 15    |  0.02146 |   0.02045 |   33.9       False\n",
      "| 16    |  0.02137 |   0.01954 |   36.1       False\n",
      "| 17    |  0.02121 |   0.01960 |   38.4       False\n",
      "| 18    |  0.02096 |   0.01961 |   40.9       False\n",
      "| 19    |  0.02084 |   0.01990 |   43.1       False\n",
      "| 20    |  0.02074 |   0.01900 |   45.2       True\n",
      "| 21    |  0.02064 |   0.01936 |   47.2       False\n",
      "| 22    |  0.02061 |   0.02025 |   49.6       False\n",
      "| 23    |  0.02045 |   0.01915 |   51.7       False\n",
      "| 24    |  0.02040 |   0.01891 |   53.8       True\n",
      "| 25    |  0.02037 |   0.01988 |   56.0       False\n",
      "| 26    |  0.02028 |   0.01890 |   58.1       True\n",
      "| 27    |  0.02011 |   0.01996 |   60.3       False\n",
      "| 28    |  0.02009 |   0.01879 |   62.6       True\n",
      "| 29    |  0.01996 |   0.02071 |   64.6       False\n",
      "| 30    |  0.01998 |   0.01855 |   66.7       True\n",
      "| 31    |  0.01985 |   0.01845 |   68.9       True\n",
      "| 32    |  0.01976 |   0.01885 |   71.1       False\n",
      "| 33    |  0.01977 |   0.01961 |   73.8       False\n",
      "| 34    |  0.01974 |   0.01849 |   75.9       False\n",
      "| 35    |  0.01975 |   0.01943 |   77.9       False\n",
      "| 36    |  0.01975 |   0.01973 |   80.0       False\n",
      "| 37    |  0.01970 |   0.01856 |   82.2       False\n",
      "| 38    |  0.01970 |   0.01852 |   84.4       False\n",
      "| 39    |  0.01961 |   0.01841 |   86.7       True\n",
      "| 40    |  0.01954 |   0.01849 |   89.1       False\n",
      "| 41    |  0.01951 |   0.01853 |   91.3       False\n",
      "| 42    |  0.01954 |   0.01918 |   93.7       False\n",
      "| 43    |  0.01946 |   0.01846 |   96.1       False\n",
      "| 44    |  0.01949 |   0.01827 |   98.3       True\n",
      "| 45    |  0.01968 |   0.01860 |   100.4      False\n",
      "| 46    |  0.01972 |   0.01982 |   102.5      False\n",
      "| 47    |  0.01962 |   0.01859 |   105.4      False\n",
      "| 48    |  0.01938 |   0.01909 |   107.4      False\n",
      "| 49    |  0.01930 |   0.02091 |   109.5      False\n",
      "| 50    |  0.01936 |   0.01820 |   111.6      True\n",
      "| 51    |  0.01932 |   0.01840 |   113.6      False\n",
      "| 52    |  0.01939 |   0.01968 |   116.0      False\n",
      "| 53    |  0.01948 |   0.01829 |   118.2      False\n",
      "| 54    |  0.01939 |   0.01832 |   120.3      False\n",
      "| 55    |  0.01940 |   0.01825 |   122.4      False\n",
      "| 56    |  0.01930 |   0.01846 |   124.4      False\n",
      "| 57    |  0.01935 |   0.01847 |   126.8      False\n",
      "| 58    |  0.01932 |   0.01846 |   128.8      False\n",
      "| 59    |  0.01930 |   0.01832 |   131.0      False\n",
      "| 60    |  0.01933 |   0.01824 |   133.0      False\n",
      "| 61    |  0.01929 |   0.01823 |   135.1      False\n",
      "| 62    |  0.01922 |   0.01808 |   137.9      True\n",
      "| 63    |  0.01927 |   0.01852 |   140.0      False\n",
      "| 64    |  0.01920 |   0.01847 |   142.0      False\n",
      "| 65    |  0.01918 |   0.01838 |   144.1      False\n",
      "| 66    |  0.01925 |   0.01839 |   146.2      False\n",
      "| 67    |  0.01924 |   0.01820 |   149.2      False\n",
      "| 68    |  0.01923 |   0.01835 |   151.4      False\n",
      "| 69    |  0.01917 |   0.01843 |   153.9      False\n",
      "| 70    |  0.01916 |   0.02024 |   156.1      False\n",
      "| 71    |  0.01945 |   0.01824 |   158.2      False\n",
      "| 72    |  0.01929 |   0.01854 |   160.5      False\n",
      "| 73    |  0.01920 |   0.01835 |   162.6      False\n",
      "| 74    |  0.01917 |   0.01839 |   164.7      False\n",
      "| 75    |  0.01919 |   0.01883 |   166.8      False\n",
      "| 76    |  0.01928 |   0.01822 |   169.1      False\n",
      "| 77    |  0.01917 |   0.01877 |   171.6      False\n",
      "| 78    |  0.01923 |   0.01847 |   173.7      False\n",
      "| 79    |  0.01929 |   0.01820 |   175.8      False\n",
      "| 80    |  0.01918 |   0.01846 |   177.9      False\n",
      "| 81    |  0.01912 |   0.01980 |   180.0      False\n",
      "| 82    |  0.01916 |   0.01838 |   182.5      False\n",
      "| 83    |  0.01923 |   0.01808 |   184.6      True\n",
      "| 84    |  0.01909 |   0.01827 |   186.6      False\n",
      "| 85    |  0.01916 |   0.01839 |   188.8      False\n",
      "| 86    |  0.01909 |   0.01841 |   190.8      False\n",
      "| 87    |  0.01918 |   0.01822 |   193.1      False\n",
      "| 88    |  0.01918 |   0.01824 |   195.2      False\n",
      "| 89    |  0.01905 |   0.01853 |   197.3      False\n",
      "| 90    |  0.01914 |   0.01831 |   199.4      False\n",
      "| 91    |  0.01907 |   0.01821 |   201.9      False\n",
      "| 92    |  0.01922 |   0.01846 |   204.2      False\n",
      "| 93    |  0.01926 |   0.01835 |   206.3      False\n",
      "| 94    |  0.01906 |   0.01849 |   208.8      False\n",
      "| 95    |  0.01914 |   0.01841 |   211.2      False\n",
      "| 96    |  0.01925 |   0.01855 |   213.6      False\n",
      "| 97    |  0.01926 |   0.01833 |   216.0      False\n",
      "| 98    |  0.01920 |   0.01821 |   218.3      False\n",
      "| 99    |  0.01906 |   0.01841 |   220.5      False\n",
      "| 100   |  0.01909 |   0.01829 |   222.6      False\n",
      "| 101   |  0.01900 |   0.01840 |   224.9      False\n",
      "| 102   |  0.01890 |   0.01838 |   227.2      False\n",
      "| 103   |  0.01896 |   0.01835 |   229.4      False\n",
      "| 104   |  0.01896 |   0.01856 |   231.6      False\n",
      "| 105   |  0.01894 |   0.01833 |   234.0      False\n",
      "| 106   |  0.01891 |   0.01827 |   236.5      False\n",
      "| 107   |  0.01891 |   0.01851 |   238.6      False\n",
      "| 108   |  0.01895 |   0.01817 |   240.7      False\n",
      "| 109   |  0.01899 |   0.01836 |   242.9      False\n",
      "| 110   |  0.01902 |   0.01840 |   245.2      False\n",
      "| 111   |  0.01918 |   0.01824 |   247.5      False\n",
      "| 112   |  0.01901 |   0.01840 |   249.6      False\n",
      "| 113   |  0.01885 |   0.01871 |   251.6      False\n",
      "| 114   |  0.01897 |   0.01848 |   253.8      False\n",
      "| 115   |  0.01897 |   0.01841 |   255.8      False\n",
      "| 116   |  0.01892 |   0.01865 |   258.1      False\n",
      "| 117   |  0.01913 |   0.01862 |   260.4      False\n",
      "| 118   |  0.01906 |   0.01843 |   262.4      False\n",
      "| 119   |  0.01887 |   0.01825 |   264.5      False\n",
      "| 120   |  0.01899 |   0.01846 |   266.9      False\n",
      "| 121   |  0.01903 |   0.01798 |   269.5      True\n",
      "| 122   |  0.01908 |   0.01899 |   271.9      False\n",
      "| 123   |  0.01902 |   0.01822 |   274.3      False\n",
      "| 124   |  0.01900 |   0.01824 |   276.7      False\n",
      "| 125   |  0.01892 |   0.01858 |   278.8      False\n",
      "| 126   |  0.01899 |   0.01838 |   281.2      False\n",
      "| 127   |  0.01895 |   0.01844 |   283.3      False\n",
      "| 128   |  0.01896 |   0.01822 |   285.3      False\n",
      "| 129   |  0.01888 |   0.01839 |   287.4      False\n",
      "| 130   |  0.01901 |   0.01834 |   289.5      False\n",
      "| 131   |  0.01908 |   0.01825 |   291.9      False\n",
      "| 132   |  0.01905 |   0.01831 |   293.9      False\n",
      "| 133   |  0.01897 |   0.01830 |   295.9      False\n",
      "| 134   |  0.01887 |   0.01869 |   298.3      False\n",
      "| 135   |  0.01890 |   0.01825 |   300.3      False\n",
      "| 136   |  0.01892 |   0.01827 |   302.7      False\n",
      "| 137   |  0.01893 |   0.01836 |   304.7      False\n",
      "| 138   |  0.01894 |   0.01841 |   306.7      False\n",
      "| 139   |  0.01898 |   0.01881 |   308.9      False\n",
      "| 140   |  0.01884 |   0.01832 |   310.9      False\n",
      "| 141   |  0.01882 |   0.01824 |   313.2      False\n",
      "| 142   |  0.01876 |   0.01821 |   315.3      False\n",
      "| 143   |  0.01894 |   0.01839 |   317.3      False\n",
      "| 144   |  0.01910 |   0.01865 |   319.4      False\n",
      "| 145   |  0.01897 |   0.01870 |   321.5      False\n",
      "| 146   |  0.01911 |   0.01830 |   323.6      False\n",
      "| 147   |  0.01892 |   0.01837 |   325.8      False\n",
      "| 148   |  0.01889 |   0.01838 |   328.0      False\n",
      "| 149   |  0.01885 |   0.01845 |   330.6      False\n",
      "| 150   |  0.01899 |   0.01938 |   333.0      False\n",
      "| 151   |  0.01911 |   0.01824 |   335.1      False\n",
      "| 152   |  0.01888 |   0.01854 |   337.5      False\n",
      "| 153   |  0.01888 |   0.01848 |   339.7      False\n",
      "| 154   |  0.01884 |   0.01821 |   341.8      False\n",
      "| 155   |  0.01878 |   0.01862 |   343.9      False\n",
      "| 156   |  0.01880 |   0.01891 |   346.1      False\n",
      "| 157   |  0.01892 |   0.01841 |   348.3      False\n",
      "| 158   |  0.01879 |   0.01834 |   350.4      False\n",
      "| 159   |  0.01879 |   0.01848 |   352.5      False\n",
      "| 160   |  0.01884 |   0.01832 |   354.5      False\n",
      "| 161   |  0.01882 |   0.01836 |   356.6      False\n",
      "| 162   |  0.01883 |   0.01844 |   358.9      False\n",
      "| 163   |  0.01889 |   0.01841 |   360.9      False\n",
      "| 164   |  0.01878 |   0.01838 |   363.4      False\n",
      "| 165   |  0.01870 |   0.01877 |   365.4      False\n",
      "| 166   |  0.01880 |   0.01842 |   367.5      False\n",
      "| 167   |  0.01887 |   0.01855 |   369.7      False\n",
      "| 168   |  0.01880 |   0.01837 |   371.9      False\n",
      "| 169   |  0.01876 |   0.01837 |   374.0      False\n",
      "| 170   |  0.01876 |   0.01884 |   376.0      False\n",
      "| 171   |  0.01894 |   0.01847 |   378.1      False\n",
      "Early stopping occured at epoch 171\n",
      "Training done in 378.062 seconds.\n",
      "---------------------------------------\n",
      "Successfully saved model at tabnet_raw_step1_fold4_0.zip\n",
      "validation fold 4 : 0.017978976226074948\n",
      "FOLDS :  5\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 50 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     |  0.38595 |   0.05275 |   2.0        True\n",
      "| 2     |  0.03299 |   0.02853 |   4.1        True\n",
      "| 3     |  0.02635 |   0.02204 |   6.2        True\n",
      "| 4     |  0.02462 |   0.02153 |   8.7        True\n",
      "| 5     |  0.02415 |   0.02124 |   11.2       True\n",
      "| 6     |  0.02378 |   0.02105 |   13.3       True\n",
      "| 7     |  0.02343 |   0.02064 |   15.9       True\n",
      "| 8     |  0.02308 |   0.02044 |   18.0       True\n",
      "| 9     |  0.02297 |   0.02028 |   20.0       True\n",
      "| 10    |  0.02266 |   0.02065 |   22.8       False\n",
      "| 11    |  0.02244 |   0.02073 |   24.8       False\n",
      "| 12    |  0.02223 |   0.01977 |   26.9       True\n",
      "| 13    |  0.02205 |   0.01980 |   29.0       False\n",
      "| 14    |  0.02164 |   0.01987 |   31.0       False\n",
      "| 15    |  0.02139 |   0.01936 |   33.3       True\n",
      "| 16    |  0.02121 |   0.01911 |   35.4       True\n",
      "| 17    |  0.02112 |   0.01955 |   37.4       False\n",
      "| 18    |  0.02091 |   0.01910 |   39.5       True\n",
      "| 19    |  0.02097 |   0.02124 |   41.6       False\n",
      "| 20    |  0.02080 |   0.01962 |   43.8       False\n",
      "| 21    |  0.02070 |   0.02084 |   46.0       False\n",
      "| 22    |  0.02059 |   0.02010 |   48.3       False\n",
      "| 23    |  0.02047 |   0.01865 |   50.4       True\n",
      "| 24    |  0.02047 |   0.01928 |   52.5       False\n",
      "| 25    |  0.02038 |   0.02081 |   54.9       False\n",
      "| 26    |  0.02019 |   0.02019 |   57.0       False\n",
      "| 27    |  0.02024 |   0.01887 |   59.1       False\n",
      "| 28    |  0.02017 |   0.02091 |   61.1       False\n",
      "| 29    |  0.02014 |   0.01861 |   63.2       True\n",
      "| 30    |  0.02012 |   0.01839 |   65.5       True\n",
      "| 31    |  0.02004 |   0.01885 |   67.8       False\n",
      "| 32    |  0.01996 |   0.01845 |   70.0       False\n",
      "| 33    |  0.01990 |   0.01850 |   72.4       False\n",
      "| 34    |  0.01987 |   0.01829 |   74.5       True\n",
      "| 35    |  0.01985 |   0.01887 |   76.8       False\n",
      "| 36    |  0.01990 |   0.02035 |   79.3       False\n",
      "| 37    |  0.01984 |   0.01828 |   81.4       True\n",
      "| 38    |  0.01978 |   0.01825 |   83.6       True\n",
      "| 39    |  0.01968 |   0.01832 |   85.8       False\n",
      "| 40    |  0.01981 |   0.02045 |   88.1       False\n",
      "| 41    |  0.01962 |   0.01823 |   90.2       True\n",
      "| 42    |  0.01962 |   0.01859 |   92.3       False\n",
      "| 43    |  0.01967 |   0.01819 |   94.4       True\n",
      "| 44    |  0.01970 |   0.01824 |   96.5       False\n",
      "| 45    |  0.01974 |   0.01841 |   98.8       False\n",
      "| 46    |  0.01965 |   0.01811 |   100.9      True\n",
      "| 47    |  0.01950 |   0.01841 |   103.0      False\n",
      "| 48    |  0.01945 |   0.01887 |   105.0      False\n",
      "| 49    |  0.01955 |   0.01825 |   107.1      False\n",
      "| 50    |  0.01956 |   0.01794 |   109.5      True\n",
      "| 51    |  0.01956 |   0.01823 |   111.9      False\n",
      "| 52    |  0.01946 |   0.01830 |   114.0      False\n",
      "| 53    |  0.01950 |   0.01809 |   116.1      False\n",
      "| 54    |  0.01951 |   0.01851 |   118.4      False\n",
      "| 55    |  0.01953 |   0.01808 |   120.7      False\n",
      "| 56    |  0.01947 |   0.01831 |   122.8      False\n",
      "| 57    |  0.01947 |   0.01838 |   124.9      False\n",
      "| 58    |  0.01939 |   0.01838 |   126.9      False\n",
      "| 59    |  0.01936 |   0.01958 |   129.7      False\n",
      "| 60    |  0.01956 |   0.01816 |   132.2      False\n",
      "| 61    |  0.01962 |   0.01905 |   134.4      False\n",
      "| 62    |  0.01942 |   0.01801 |   136.5      False\n",
      "| 63    |  0.01938 |   0.01811 |   138.5      False\n",
      "| 64    |  0.01950 |   0.01830 |   140.6      False\n",
      "| 65    |  0.01942 |   0.01797 |   143.0      False\n",
      "| 66    |  0.01936 |   0.01823 |   145.5      False\n",
      "| 67    |  0.01946 |   0.01812 |   147.5      False\n",
      "| 68    |  0.01938 |   0.01823 |   149.7      False\n",
      "| 69    |  0.01932 |   0.01841 |   151.9      False\n",
      "| 70    |  0.01943 |   0.01802 |   154.1      False\n",
      "| 71    |  0.01937 |   0.01851 |   156.3      False\n",
      "| 72    |  0.01928 |   0.01806 |   158.3      False\n",
      "| 73    |  0.01924 |   0.01832 |   160.3      False\n",
      "| 74    |  0.01955 |   0.01836 |   162.4      False\n",
      "| 75    |  0.01942 |   0.01877 |   164.7      False\n",
      "| 76    |  0.01934 |   0.01806 |   166.8      False\n",
      "| 77    |  0.01930 |   0.01798 |   168.8      False\n",
      "| 78    |  0.01928 |   0.01858 |   170.8      False\n",
      "| 79    |  0.01934 |   0.01815 |   172.9      False\n",
      "| 80    |  0.01932 |   0.01795 |   175.5      False\n",
      "| 81    |  0.01921 |   0.01824 |   177.9      False\n",
      "| 82    |  0.01923 |   0.01813 |   179.9      False\n",
      "| 83    |  0.01919 |   0.01895 |   182.2      False\n",
      "| 84    |  0.01935 |   0.01805 |   184.3      False\n",
      "| 85    |  0.01926 |   0.01840 |   186.5      False\n",
      "| 86    |  0.01925 |   0.01803 |   189.1      False\n",
      "| 87    |  0.01928 |   0.01809 |   191.2      False\n",
      "| 88    |  0.01930 |   0.01845 |   193.5      False\n",
      "| 89    |  0.01920 |   0.01800 |   195.6      False\n",
      "| 90    |  0.01939 |   0.01808 |   197.9      False\n",
      "| 91    |  0.01935 |   0.01835 |   200.1      False\n",
      "| 92    |  0.01929 |   0.01866 |   202.2      False\n",
      "| 93    |  0.01942 |   0.01815 |   204.3      False\n",
      "| 94    |  0.01929 |   0.01885 |   206.4      False\n",
      "| 95    |  0.01937 |   0.01831 |   209.2      False\n",
      "| 96    |  0.01929 |   0.01817 |   211.3      False\n",
      "| 97    |  0.01914 |   0.01820 |   213.5      False\n",
      "| 98    |  0.01918 |   0.01804 |   215.5      False\n",
      "| 99    |  0.01922 |   0.01830 |   217.7      False\n",
      "| 100   |  0.01937 |   0.01817 |   219.9      False\n",
      "Early stopping occured at epoch 100\n",
      "Training done in 219.872 seconds.\n",
      "---------------------------------------\n",
      "Successfully saved model at tabnet_raw_step1_fold5_0.zip\n",
      "validation fold 5 : 0.017936706252213008\n",
      "FOLDS :  6\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 50 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     |  0.39315 |   0.05746 |   2.0        True\n",
      "| 2     |  0.03355 |   0.02603 |   4.0        True\n",
      "| 3     |  0.02664 |   0.02140 |   6.2        True\n",
      "| 4     |  0.02453 |   0.02093 |   8.4        True\n",
      "| 5     |  0.02403 |   0.02069 |   10.5       True\n",
      "| 6     |  0.02378 |   0.02048 |   12.5       True\n",
      "| 7     |  0.02377 |   0.02034 |   14.6       True\n",
      "| 8     |  0.02350 |   0.02018 |   16.7       True\n",
      "| 9     |  0.02334 |   0.01998 |   19.3       True\n",
      "| 10    |  0.02306 |   0.01973 |   21.5       True\n",
      "| 11    |  0.02277 |   0.01956 |   23.8       True\n",
      "| 12    |  0.02252 |   0.01958 |   26.1       False\n",
      "| 13    |  0.02223 |   0.01911 |   28.2       True\n",
      "| 14    |  0.02181 |   0.01879 |   30.3       True\n",
      "| 15    |  0.02160 |   0.01828 |   32.9       True\n",
      "| 16    |  0.02133 |   0.01841 |   35.0       False\n",
      "| 17    |  0.02120 |   0.01852 |   37.0       False\n",
      "| 18    |  0.02097 |   0.01825 |   39.0       True\n",
      "| 19    |  0.02083 |   0.01799 |   41.2       True\n",
      "| 20    |  0.02071 |   0.02057 |   43.2       False\n",
      "| 21    |  0.02077 |   0.01867 |   45.2       False\n",
      "| 22    |  0.02059 |   0.02045 |   47.3       False\n",
      "| 23    |  0.02047 |   0.01814 |   49.3       False\n",
      "| 24    |  0.02047 |   0.02170 |   51.6       False\n",
      "| 25    |  0.02026 |   0.01793 |   53.8       True\n",
      "| 26    |  0.02040 |   0.02101 |   55.9       False\n",
      "| 27    |  0.02035 |   0.01775 |   57.9       True\n",
      "| 28    |  0.02021 |   0.01770 |   60.0       True\n",
      "| 29    |  0.02011 |   0.02047 |   61.9       False\n",
      "| 30    |  0.02014 |   0.01953 |   64.1       False\n",
      "| 31    |  0.02003 |   0.01757 |   66.2       True\n",
      "| 32    |  0.01994 |   0.01967 |   68.1       False\n",
      "| 33    |  0.01991 |   0.01929 |   70.2       False\n",
      "| 34    |  0.01998 |   0.02015 |   72.1       False\n",
      "| 35    |  0.01987 |   0.01755 |   74.3       True\n",
      "| 36    |  0.01984 |   0.01865 |   76.3       False\n",
      "| 37    |  0.01974 |   0.01851 |   78.2       False\n",
      "| 38    |  0.01964 |   0.01777 |   80.2       False\n",
      "| 39    |  0.01966 |   0.01780 |   82.5       False\n",
      "| 40    |  0.01970 |   0.01755 |   84.6       True\n",
      "| 41    |  0.01968 |   0.01724 |   87.5       True\n",
      "| 42    |  0.01979 |   0.01786 |   89.5       False\n",
      "| 43    |  0.01965 |   0.01930 |   91.7       False\n",
      "| 44    |  0.01968 |   0.01846 |   93.7       False\n",
      "| 45    |  0.01963 |   0.01760 |   95.8       False\n",
      "| 46    |  0.01958 |   0.01725 |   97.9       False\n",
      "| 47    |  0.01952 |   0.01748 |   99.9       False\n",
      "| 48    |  0.01962 |   0.01754 |   101.8      False\n",
      "| 49    |  0.01960 |   0.01876 |   103.9      False\n",
      "| 50    |  0.01960 |   0.01777 |   105.8      False\n",
      "| 51    |  0.01966 |   0.01776 |   107.9      False\n",
      "| 52    |  0.01948 |   0.01716 |   110.0      True\n",
      "| 53    |  0.01945 |   0.01756 |   112.0      False\n",
      "| 54    |  0.01944 |   0.01807 |   114.0      False\n",
      "| 55    |  0.01944 |   0.01777 |   116.2      False\n",
      "| 56    |  0.01939 |   0.01836 |   118.5      False\n",
      "| 57    |  0.01950 |   0.01777 |   120.5      False\n",
      "| 58    |  0.01947 |   0.01725 |   122.5      False\n",
      "| 59    |  0.01954 |   0.01720 |   124.5      False\n",
      "| 60    |  0.01939 |   0.01745 |   126.4      False\n",
      "| 61    |  0.01943 |   0.01758 |   128.4      False\n",
      "| 62    |  0.01956 |   0.01736 |   130.6      False\n",
      "| 63    |  0.01926 |   0.01772 |   132.5      False\n",
      "| 64    |  0.01946 |   0.01744 |   134.5      False\n",
      "| 65    |  0.01930 |   0.01803 |   136.5      False\n",
      "| 66    |  0.01934 |   0.01741 |   138.5      False\n",
      "| 67    |  0.01942 |   0.01805 |   140.6      False\n",
      "| 68    |  0.01938 |   0.01773 |   142.7      False\n",
      "| 69    |  0.01936 |   0.01747 |   144.6      False\n",
      "| 70    |  0.01945 |   0.01777 |   147.3      False\n",
      "| 71    |  0.01935 |   0.01710 |   149.6      True\n",
      "| 72    |  0.01932 |   0.01748 |   152.0      False\n",
      "| 73    |  0.01949 |   0.01763 |   154.0      False\n",
      "| 74    |  0.01941 |   0.01742 |   156.0      False\n",
      "| 75    |  0.01939 |   0.01775 |   158.0      False\n",
      "| 76    |  0.01919 |   0.01735 |   159.9      False\n",
      "| 77    |  0.01922 |   0.01732 |   162.1      False\n",
      "| 78    |  0.01944 |   0.01738 |   164.2      False\n",
      "| 79    |  0.01942 |   0.01756 |   166.1      False\n",
      "| 80    |  0.01932 |   0.01834 |   168.2      False\n",
      "| 81    |  0.01925 |   0.01738 |   170.1      False\n",
      "| 82    |  0.01939 |   0.01809 |   172.1      False\n",
      "| 83    |  0.01928 |   0.01734 |   174.3      False\n",
      "| 84    |  0.01925 |   0.01733 |   176.2      False\n",
      "| 85    |  0.01928 |   0.01734 |   178.3      False\n",
      "| 86    |  0.01923 |   0.01718 |   180.8      False\n",
      "| 87    |  0.01926 |   0.01726 |   182.7      False\n",
      "| 88    |  0.01925 |   0.01715 |   184.8      False\n",
      "| 89    |  0.01929 |   0.01721 |   186.8      False\n",
      "| 90    |  0.01942 |   0.01740 |   188.7      False\n",
      "| 91    |  0.01920 |   0.01715 |   190.7      False\n",
      "| 92    |  0.01916 |   0.01740 |   192.7      False\n",
      "| 93    |  0.01921 |   0.01708 |   194.7      True\n",
      "| 94    |  0.01923 |   0.01715 |   196.8      False\n",
      "| 95    |  0.01925 |   0.01750 |   198.8      False\n",
      "| 96    |  0.01934 |   0.01787 |   200.7      False\n",
      "| 97    |  0.01923 |   0.01711 |   202.6      False\n",
      "| 98    |  0.01917 |   0.01727 |   204.6      False\n",
      "| 99    |  0.01918 |   0.01742 |   207.2      False\n",
      "| 100   |  0.01925 |   0.01768 |   209.4      False\n",
      "| 101   |  0.01917 |   0.01715 |   212.0      False\n",
      "| 102   |  0.01903 |   0.01706 |   214.1      True\n",
      "| 103   |  0.01910 |   0.01729 |   216.2      False\n",
      "| 104   |  0.01926 |   0.01753 |   218.4      False\n",
      "| 105   |  0.01915 |   0.01729 |   220.3      False\n",
      "| 106   |  0.01910 |   0.01713 |   222.2      False\n",
      "| 107   |  0.01910 |   0.01713 |   224.3      False\n",
      "| 108   |  0.01924 |   0.01733 |   226.2      False\n",
      "| 109   |  0.01919 |   0.01769 |   228.3      False\n",
      "| 110   |  0.01919 |   0.01755 |   230.3      False\n",
      "| 111   |  0.01916 |   0.01712 |   232.2      False\n",
      "| 112   |  0.01908 |   0.01737 |   234.2      False\n",
      "| 113   |  0.01912 |   0.01732 |   236.2      False\n",
      "| 114   |  0.01905 |   0.01711 |   238.2      False\n",
      "| 115   |  0.01901 |   0.01730 |   240.4      False\n",
      "| 116   |  0.01907 |   0.01831 |   242.6      False\n",
      "| 117   |  0.01913 |   0.01730 |   244.8      False\n",
      "| 118   |  0.01908 |   0.01721 |   246.7      False\n",
      "| 119   |  0.01908 |   0.01743 |   248.7      False\n",
      "| 120   |  0.01908 |   0.01715 |   250.8      False\n",
      "| 121   |  0.01914 |   0.01707 |   252.8      False\n",
      "| 122   |  0.01924 |   0.01740 |   254.8      False\n",
      "| 123   |  0.01921 |   0.01781 |   256.7      False\n",
      "| 124   |  0.01922 |   0.01724 |   258.6      False\n",
      "| 125   |  0.01915 |   0.01730 |   260.6      False\n",
      "| 126   |  0.01903 |   0.01778 |   262.7      False\n",
      "| 127   |  0.01908 |   0.01740 |   264.6      False\n",
      "| 128   |  0.01910 |   0.01720 |   266.9      False\n",
      "| 129   |  0.01896 |   0.01718 |   269.1      False\n",
      "| 130   |  0.01909 |   0.01709 |   271.1      False\n",
      "| 131   |  0.01916 |   0.01723 |   273.5      False\n",
      "| 132   |  0.01912 |   0.01738 |   275.7      False\n",
      "| 133   |  0.01903 |   0.01751 |   277.8      False\n",
      "| 134   |  0.01929 |   0.01720 |   279.8      False\n",
      "| 135   |  0.01919 |   0.01718 |   281.7      False\n",
      "| 136   |  0.01911 |   0.01743 |   283.8      False\n",
      "| 137   |  0.01922 |   0.01882 |   285.8      False\n",
      "| 138   |  0.01932 |   0.01723 |   287.7      False\n",
      "| 139   |  0.01914 |   0.01716 |   289.6      False\n",
      "| 140   |  0.01905 |   0.01736 |   291.6      False\n",
      "| 141   |  0.01917 |   0.01782 |   293.5      False\n",
      "| 142   |  0.01921 |   0.01729 |   295.6      False\n",
      "| 143   |  0.01913 |   0.01696 |   297.6      True\n",
      "| 144   |  0.01913 |   0.01746 |   299.5      False\n",
      "| 145   |  0.01917 |   0.01714 |   301.5      False\n",
      "| 146   |  0.01917 |   0.01728 |   303.4      False\n",
      "| 147   |  0.01903 |   0.01738 |   305.8      False\n",
      "| 148   |  0.01908 |   0.01734 |   307.7      False\n",
      "| 149   |  0.01909 |   0.01716 |   310.0      False\n",
      "| 150   |  0.01922 |   0.01776 |   311.9      False\n",
      "| 151   |  0.01926 |   0.01711 |   313.9      False\n",
      "| 152   |  0.01910 |   0.01711 |   316.0      False\n",
      "| 153   |  0.01906 |   0.01725 |   317.9      False\n",
      "| 154   |  0.01914 |   0.01727 |   319.9      False\n",
      "| 155   |  0.01897 |   0.01731 |   321.8      False\n",
      "| 156   |  0.01897 |   0.01731 |   323.8      False\n",
      "| 157   |  0.01902 |   0.01716 |   326.0      False\n",
      "| 158   |  0.01887 |   0.01729 |   328.3      False\n",
      "| 159   |  0.01902 |   0.01728 |   330.4      False\n",
      "| 160   |  0.01903 |   0.01739 |   332.3      False\n",
      "| 161   |  0.01904 |   0.01719 |   334.4      False\n",
      "| 162   |  0.01909 |   0.01748 |   336.5      False\n",
      "| 163   |  0.01900 |   0.01713 |   338.7      False\n",
      "| 164   |  0.01889 |   0.01718 |   341.0      False\n",
      "| 165   |  0.01884 |   0.01722 |   342.9      False\n",
      "| 166   |  0.01885 |   0.01732 |   344.8      False\n",
      "| 167   |  0.01897 |   0.01782 |   346.7      False\n",
      "| 168   |  0.01902 |   0.01714 |   348.8      False\n",
      "| 169   |  0.01892 |   0.01738 |   350.8      False\n",
      "| 170   |  0.01903 |   0.01748 |   352.8      False\n",
      "| 171   |  0.01932 |   0.01734 |   354.7      False\n",
      "| 172   |  0.01901 |   0.01734 |   356.6      False\n",
      "| 173   |  0.01892 |   0.01734 |   358.6      False\n",
      "| 174   |  0.01886 |   0.01751 |   360.7      False\n",
      "| 175   |  0.01897 |   0.01728 |   362.6      False\n",
      "| 176   |  0.01896 |   0.01860 |   364.6      False\n",
      "| 177   |  0.01898 |   0.01731 |   366.6      False\n",
      "| 178   |  0.01889 |   0.01750 |   368.7      False\n",
      "| 179   |  0.01904 |   0.01752 |   370.9      False\n",
      "| 180   |  0.01893 |   0.01728 |   372.8      False\n",
      "| 181   |  0.01885 |   0.01727 |   375.0      False\n",
      "| 182   |  0.01883 |   0.01719 |   377.0      False\n",
      "| 183   |  0.01897 |   0.01726 |   378.9      False\n",
      "| 184   |  0.01894 |   0.01750 |   380.9      False\n",
      "| 185   |  0.01890 |   0.01739 |   383.1      False\n",
      "| 186   |  0.01890 |   0.01751 |   385.0      False\n",
      "| 187   |  0.01893 |   0.01784 |   387.3      False\n",
      "| 188   |  0.01905 |   0.01747 |   389.4      False\n",
      "| 189   |  0.01891 |   0.01722 |   391.3      False\n",
      "| 190   |  0.01892 |   0.01701 |   393.3      False\n",
      "| 191   |  0.01890 |   0.01749 |   395.5      False\n",
      "| 192   |  0.01909 |   0.01756 |   397.4      False\n",
      "| 193   |  0.01909 |   0.01725 |   399.5      False\n",
      "Early stopping occured at epoch 193\n",
      "Training done in 399.533 seconds.\n",
      "---------------------------------------\n",
      "Successfully saved model at tabnet_raw_step1_fold6_0.zip\n",
      "validation fold 6 : 0.016955487434051533\n",
      "FOLDS :  7\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 50 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     |  0.38265 |   0.05034 |   1.9        True\n",
      "| 2     |  0.03253 |   0.02621 |   4.3        True\n",
      "| 3     |  0.02592 |   0.02127 |   6.4        True\n",
      "| 4     |  0.02438 |   0.02101 |   8.3        True\n",
      "| 5     |  0.02404 |   0.02077 |   10.3       True\n",
      "| 6     |  0.02374 |   0.02061 |   12.3       True\n",
      "| 7     |  0.02359 |   0.02058 |   14.4       True\n",
      "| 8     |  0.02344 |   0.02081 |   16.4       False\n",
      "| 9     |  0.02338 |   0.02035 |   18.4       True\n",
      "| 10    |  0.02302 |   0.02033 |   20.3       True\n",
      "| 11    |  0.02261 |   0.01988 |   22.3       True\n",
      "| 12    |  0.02225 |   0.02024 |   24.4       False\n",
      "| 13    |  0.02206 |   0.01932 |   26.4       True\n",
      "| 14    |  0.02183 |   0.01908 |   28.4       True\n",
      "| 15    |  0.02161 |   0.01924 |   30.5       False\n",
      "| 16    |  0.02144 |   0.01955 |   32.4       False\n",
      "| 17    |  0.02124 |   0.01884 |   34.3       True\n",
      "| 18    |  0.02108 |   0.02068 |   36.8       False\n",
      "| 19    |  0.02096 |   0.01982 |   38.7       False\n",
      "| 20    |  0.02084 |   0.02012 |   40.7       False\n",
      "| 21    |  0.02062 |   0.02001 |   42.7       False\n",
      "| 22    |  0.02054 |   0.02027 |   44.9       False\n",
      "| 23    |  0.02047 |   0.01997 |   47.3       False\n",
      "| 24    |  0.02052 |   0.01880 |   49.5       True\n",
      "| 25    |  0.02049 |   0.02232 |   51.5       False\n",
      "| 26    |  0.02044 |   0.01829 |   53.7       True\n",
      "| 27    |  0.02034 |   0.01813 |   55.7       True\n",
      "| 28    |  0.02026 |   0.01880 |   57.9       False\n",
      "| 29    |  0.02021 |   0.02000 |   59.9       False\n",
      "| 30    |  0.02009 |   0.02092 |   62.0       False\n",
      "| 31    |  0.02005 |   0.01915 |   64.1       False\n",
      "| 32    |  0.02018 |   0.01983 |   66.0       False\n",
      "| 33    |  0.02015 |   0.01794 |   68.3       True\n",
      "| 34    |  0.01997 |   0.01892 |   70.5       False\n",
      "| 35    |  0.02000 |   0.01859 |   72.4       False\n",
      "| 36    |  0.02001 |   0.01816 |   74.4       False\n",
      "| 37    |  0.01991 |   0.01800 |   76.4       False\n",
      "| 38    |  0.01975 |   0.01902 |   78.3       False\n",
      "| 39    |  0.02000 |   0.01804 |   80.5       False\n",
      "| 40    |  0.01999 |   0.01858 |   82.4       False\n",
      "| 41    |  0.01990 |   0.01801 |   84.4       False\n",
      "| 42    |  0.01982 |   0.01984 |   86.3       False\n",
      "| 43    |  0.01977 |   0.01799 |   88.2       False\n",
      "| 44    |  0.01970 |   0.01871 |   90.4       False\n",
      "| 45    |  0.01963 |   0.01805 |   92.4       False\n",
      "| 46    |  0.01966 |   0.01785 |   94.6       True\n",
      "| 47    |  0.01972 |   0.01795 |   96.5       False\n",
      "| 48    |  0.01979 |   0.01781 |   98.4       True\n",
      "| 49    |  0.01964 |   0.01991 |   100.8      False\n",
      "| 50    |  0.01975 |   0.01784 |   102.9      False\n",
      "| 51    |  0.01973 |   0.01864 |   105.2      False\n",
      "| 52    |  0.01969 |   0.01784 |   107.4      False\n",
      "| 53    |  0.01966 |   0.01868 |   109.4      False\n",
      "| 54    |  0.01951 |   0.01833 |   111.4      False\n",
      "| 55    |  0.01956 |   0.01792 |   113.7      False\n",
      "| 56    |  0.01967 |   0.01901 |   115.7      False\n",
      "| 57    |  0.01957 |   0.01770 |   117.6      True\n",
      "| 58    |  0.01955 |   0.01783 |   119.5      False\n",
      "| 59    |  0.01964 |   0.01812 |   121.6      False\n",
      "| 60    |  0.01948 |   0.01812 |   123.7      False\n",
      "| 61    |  0.01938 |   0.01774 |   125.9      False\n",
      "| 62    |  0.01957 |   0.01791 |   127.8      False\n",
      "| 63    |  0.01950 |   0.01872 |   129.8      False\n",
      "| 64    |  0.01956 |   0.01786 |   131.9      False\n",
      "| 65    |  0.01969 |   0.01838 |   134.1      False\n",
      "| 66    |  0.01952 |   0.01859 |   136.3      False\n",
      "| 67    |  0.01951 |   0.01796 |   138.3      False\n",
      "| 68    |  0.01960 |   0.01790 |   140.3      False\n",
      "| 69    |  0.01952 |   0.01780 |   142.3      False\n",
      "| 70    |  0.01965 |   0.01797 |   144.3      False\n",
      "| 71    |  0.01947 |   0.01788 |   146.5      False\n",
      "| 72    |  0.01938 |   0.01765 |   148.5      True\n",
      "| 73    |  0.01943 |   0.01787 |   150.4      False\n",
      "| 74    |  0.01944 |   0.01764 |   152.5      True\n",
      "| 75    |  0.01937 |   0.01899 |   154.4      False\n",
      "| 76    |  0.01968 |   0.01815 |   156.8      False\n",
      "| 77    |  0.01941 |   0.01887 |   158.8      False\n",
      "| 78    |  0.01937 |   0.01807 |   160.7      False\n",
      "| 79    |  0.01936 |   0.01807 |   162.7      False\n",
      "| 80    |  0.01947 |   0.01787 |   165.4      False\n",
      "| 81    |  0.01933 |   0.01899 |   167.5      False\n",
      "| 82    |  0.01945 |   0.01778 |   169.8      False\n",
      "| 83    |  0.01944 |   0.01786 |   171.8      False\n",
      "| 84    |  0.01947 |   0.01763 |   173.8      True\n",
      "| 85    |  0.01936 |   0.01803 |   175.8      False\n",
      "| 86    |  0.01925 |   0.01811 |   177.8      False\n",
      "| 87    |  0.01938 |   0.01784 |   179.9      False\n",
      "| 88    |  0.01938 |   0.01816 |   181.8      False\n",
      "| 89    |  0.01953 |   0.01793 |   183.8      False\n",
      "| 90    |  0.01942 |   0.01787 |   185.7      False\n",
      "| 91    |  0.01939 |   0.01772 |   187.9      False\n",
      "| 92    |  0.01933 |   0.01786 |   190.1      False\n",
      "| 93    |  0.01931 |   0.01778 |   192.0      False\n",
      "| 94    |  0.01924 |   0.01798 |   194.0      False\n",
      "| 95    |  0.01945 |   0.01807 |   195.9      False\n",
      "| 96    |  0.01953 |   0.01774 |   198.2      False\n",
      "| 97    |  0.01947 |   0.01829 |   200.3      False\n",
      "| 98    |  0.01950 |   0.01780 |   202.3      False\n",
      "| 99    |  0.01932 |   0.01772 |   204.2      False\n",
      "| 100   |  0.01919 |   0.01797 |   206.2      False\n",
      "| 101   |  0.01934 |   0.01791 |   208.2      False\n",
      "| 102   |  0.01927 |   0.01793 |   210.1      False\n",
      "| 103   |  0.01918 |   0.01770 |   212.3      False\n",
      "| 104   |  0.01921 |   0.01780 |   214.2      False\n",
      "| 105   |  0.01918 |   0.01784 |   216.1      False\n",
      "| 106   |  0.01923 |   0.01818 |   218.1      False\n",
      "| 107   |  0.01919 |   0.01778 |   220.2      False\n",
      "| 108   |  0.01916 |   0.01804 |   222.3      False\n",
      "| 109   |  0.01913 |   0.01832 |   224.7      False\n",
      "| 110   |  0.01922 |   0.01770 |   226.7      False\n",
      "| 111   |  0.01925 |   0.01795 |   229.0      False\n",
      "| 112   |  0.01916 |   0.01783 |   231.3      False\n",
      "| 113   |  0.01915 |   0.01945 |   233.3      False\n",
      "| 114   |  0.01949 |   0.01776 |   235.4      False\n",
      "| 115   |  0.01933 |   0.01798 |   237.4      False\n",
      "| 116   |  0.01924 |   0.01798 |   239.3      False\n",
      "| 117   |  0.01929 |   0.01796 |   241.3      False\n",
      "| 118   |  0.01924 |   0.01868 |   243.3      False\n",
      "| 119   |  0.01924 |   0.01793 |   245.4      False\n",
      "| 120   |  0.01931 |   0.01800 |   247.3      False\n",
      "| 121   |  0.01924 |   0.01768 |   249.3      False\n",
      "| 122   |  0.01918 |   0.01791 |   251.4      False\n",
      "| 123   |  0.01931 |   0.01837 |   253.3      False\n",
      "| 124   |  0.01932 |   0.01814 |   255.6      False\n",
      "| 125   |  0.01934 |   0.01755 |   257.5      True\n",
      "| 126   |  0.01930 |   0.01757 |   259.4      False\n",
      "| 127   |  0.01918 |   0.01769 |   261.6      False\n",
      "| 128   |  0.01920 |   0.01778 |   263.7      False\n",
      "| 129   |  0.01919 |   0.01781 |   265.6      False\n",
      "| 130   |  0.01919 |   0.01757 |   267.8      False\n",
      "| 131   |  0.01906 |   0.01783 |   269.7      False\n",
      "| 132   |  0.01914 |   0.01767 |   271.7      False\n",
      "| 133   |  0.01932 |   0.01782 |   273.7      False\n",
      "| 134   |  0.01910 |   0.01764 |   275.6      False\n",
      "| 135   |  0.01910 |   0.01778 |   277.7      False\n",
      "| 136   |  0.01922 |   0.01770 |   279.8      False\n",
      "| 137   |  0.01916 |   0.01774 |   281.8      False\n",
      "| 138   |  0.01921 |   0.01767 |   284.0      False\n",
      "| 139   |  0.01940 |   0.01778 |   286.2      False\n",
      "| 140   |  0.01934 |   0.01765 |   288.5      False\n",
      "| 141   |  0.01923 |   0.01774 |   290.6      False\n",
      "| 142   |  0.01925 |   0.01764 |   292.7      False\n",
      "| 143   |  0.01915 |   0.01777 |   294.9      False\n",
      "| 144   |  0.01914 |   0.01755 |   296.9      False\n",
      "| 145   |  0.01915 |   0.01794 |   298.9      False\n",
      "| 146   |  0.01921 |   0.01783 |   301.0      False\n",
      "| 147   |  0.01934 |   0.01788 |   302.9      False\n",
      "| 148   |  0.01919 |   0.01773 |   305.0      False\n",
      "| 149   |  0.01924 |   0.01788 |   306.9      False\n",
      "| 150   |  0.01920 |   0.01752 |   308.8      True\n",
      "| 151   |  0.01897 |   0.01759 |   311.0      False\n",
      "| 152   |  0.01907 |   0.01775 |   312.9      False\n",
      "| 153   |  0.01907 |   0.01771 |   315.0      False\n",
      "| 154   |  0.01897 |   0.01809 |   317.0      False\n",
      "| 155   |  0.01903 |   0.01797 |   318.9      False\n",
      "| 156   |  0.01913 |   0.01759 |   320.8      False\n",
      "| 157   |  0.01907 |   0.01767 |   323.1      False\n",
      "| 158   |  0.01906 |   0.02025 |   325.0      False\n",
      "| 159   |  0.01934 |   0.01788 |   327.2      False\n",
      "| 160   |  0.01910 |   0.01784 |   329.2      False\n",
      "| 161   |  0.01909 |   0.01757 |   331.2      False\n",
      "| 162   |  0.01904 |   0.01791 |   333.3      False\n",
      "| 163   |  0.01901 |   0.01807 |   335.3      False\n",
      "| 164   |  0.01897 |   0.01795 |   337.2      False\n",
      "| 165   |  0.01905 |   0.01769 |   339.1      False\n",
      "| 166   |  0.01907 |   0.01760 |   341.1      False\n",
      "| 167   |  0.01902 |   0.01778 |   343.1      False\n",
      "| 168   |  0.01895 |   0.01758 |   345.8      False\n",
      "| 169   |  0.01896 |   0.01763 |   348.1      False\n",
      "| 170   |  0.01887 |   0.01787 |   350.2      False\n",
      "| 171   |  0.01886 |   0.01770 |   352.1      False\n",
      "| 172   |  0.01893 |   0.01781 |   354.2      False\n",
      "| 173   |  0.01894 |   0.01790 |   356.3      False\n",
      "| 174   |  0.01910 |   0.01770 |   358.4      False\n",
      "| 175   |  0.01910 |   0.01790 |   360.5      False\n",
      "| 176   |  0.01911 |   0.01790 |   362.4      False\n",
      "| 177   |  0.01907 |   0.01776 |   364.3      False\n",
      "| 178   |  0.01907 |   0.01783 |   366.5      False\n",
      "| 179   |  0.01909 |   0.01775 |   368.5      False\n",
      "| 180   |  0.01895 |   0.01762 |   370.4      False\n",
      "| 181   |  0.01905 |   0.01763 |   372.4      False\n",
      "| 182   |  0.01908 |   0.01770 |   374.3      False\n",
      "| 183   |  0.01902 |   0.01777 |   376.4      False\n",
      "| 184   |  0.01901 |   0.01770 |   378.5      False\n",
      "| 185   |  0.01898 |   0.01782 |   380.6      False\n",
      "| 186   |  0.01895 |   0.01772 |   382.5      False\n",
      "| 187   |  0.01893 |   0.01782 |   384.6      False\n",
      "| 188   |  0.01906 |   0.01768 |   386.5      False\n",
      "| 189   |  0.01899 |   0.01771 |   388.6      False\n",
      "| 190   |  0.01892 |   0.01756 |   390.8      False\n",
      "| 191   |  0.01899 |   0.01784 |   392.8      False\n",
      "| 192   |  0.01903 |   0.01775 |   394.8      False\n",
      "| 193   |  0.01899 |   0.01761 |   396.7      False\n",
      "| 194   |  0.01895 |   0.01806 |   398.9      False\n",
      "| 195   |  0.01894 |   0.01769 |   400.8      False\n",
      "| 196   |  0.01892 |   0.01790 |   402.8      False\n",
      "| 197   |  0.01889 |   0.01766 |   405.1      False\n",
      "| 198   |  0.01902 |   0.01840 |   407.1      False\n",
      "| 199   |  0.01924 |   0.01896 |   409.2      False\n",
      "| 200   |  0.01912 |   0.01756 |   411.9      False\n",
      "Early stopping occured at epoch 200\n",
      "Training done in 411.867 seconds.\n",
      "---------------------------------------\n",
      "Successfully saved model at tabnet_raw_step1_fold7_0.zip\n",
      "validation fold 7 : 0.01751639734296969\n",
      "FOLDS :  8\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 50 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     |  0.39921 |   0.05465 |   2.0        True\n",
      "| 2     |  0.03421 |   0.02701 |   3.9        True\n",
      "| 3     |  0.02746 |   0.02115 |   5.8        True\n",
      "| 4     |  0.02492 |   0.02068 |   8.0        True\n",
      "| 5     |  0.02433 |   0.02053 |   10.2       True\n",
      "| 6     |  0.02411 |   0.02031 |   12.1       True\n",
      "| 7     |  0.02385 |   0.02011 |   14.1       True\n",
      "| 8     |  0.02363 |   0.02000 |   16.0       True\n",
      "| 9     |  0.02348 |   0.01989 |   18.2       True\n",
      "| 10    |  0.02318 |   0.01965 |   20.1       True\n",
      "| 11    |  0.02286 |   0.01944 |   22.0       True\n",
      "| 12    |  0.02251 |   0.01898 |   24.1       True\n",
      "| 13    |  0.02229 |   0.01900 |   26.0       False\n",
      "| 14    |  0.02198 |   0.01869 |   27.9       True\n",
      "| 15    |  0.02171 |   0.01882 |   30.4       False\n",
      "| 16    |  0.02145 |   0.01823 |   32.3       True\n",
      "| 17    |  0.02132 |   0.01921 |   34.2       False\n",
      "| 18    |  0.02121 |   0.01838 |   36.2       False\n",
      "| 19    |  0.02100 |   0.01912 |   38.1       False\n",
      "| 20    |  0.02099 |   0.01792 |   40.4       True\n",
      "| 21    |  0.02084 |   0.01916 |   42.6       False\n",
      "| 22    |  0.02079 |   0.01824 |   44.6       False\n",
      "| 23    |  0.02079 |   0.01868 |   46.7       False\n",
      "| 24    |  0.02069 |   0.01894 |   48.7       False\n",
      "| 25    |  0.02058 |   0.01796 |   51.3       False\n",
      "| 26    |  0.02053 |   0.02070 |   53.6       False\n",
      "| 27    |  0.02041 |   0.01789 |   55.5       True\n",
      "| 28    |  0.02040 |   0.01876 |   57.5       False\n",
      "| 29    |  0.02035 |   0.01964 |   59.6       False\n",
      "| 30    |  0.02026 |   0.01959 |   61.9       False\n",
      "| 31    |  0.02033 |   0.01836 |   63.9       False\n",
      "| 32    |  0.02031 |   0.01845 |   65.8       False\n",
      "| 33    |  0.02019 |   0.01946 |   67.7       False\n",
      "| 34    |  0.02019 |   0.01923 |   69.6       False\n",
      "| 35    |  0.02016 |   0.01784 |   71.6       True\n",
      "| 36    |  0.02009 |   0.01947 |   74.1       False\n",
      "| 37    |  0.02012 |   0.01752 |   76.0       True\n",
      "| 38    |  0.02012 |   0.01795 |   77.9       False\n",
      "| 39    |  0.02001 |   0.01733 |   79.9       True\n",
      "| 40    |  0.02001 |   0.01764 |   81.8       False\n",
      "| 41    |  0.02005 |   0.01746 |   84.0       False\n",
      "| 42    |  0.01999 |   0.01970 |   85.9       False\n",
      "| 43    |  0.01995 |   0.01747 |   87.9       False\n",
      "| 44    |  0.01989 |   0.01776 |   89.8       False\n",
      "| 45    |  0.01982 |   0.02009 |   91.7       False\n",
      "| 46    |  0.01979 |   0.01752 |   94.0       False\n",
      "| 47    |  0.01977 |   0.01725 |   96.1       True\n",
      "| 48    |  0.01980 |   0.01985 |   98.0       False\n",
      "| 49    |  0.01980 |   0.01736 |   100.0      False\n",
      "| 50    |  0.01973 |   0.01727 |   101.9      False\n",
      "| 51    |  0.01977 |   0.01727 |   103.9      False\n",
      "| 52    |  0.01971 |   0.01750 |   106.0      False\n",
      "| 53    |  0.01962 |   0.01717 |   108.2      True\n",
      "| 54    |  0.01977 |   0.01801 |   110.3      False\n",
      "| 55    |  0.01978 |   0.01720 |   112.6      False\n",
      "| 56    |  0.01967 |   0.01723 |   114.6      False\n",
      "| 57    |  0.01970 |   0.01731 |   116.6      False\n",
      "| 58    |  0.01963 |   0.01757 |   118.8      False\n",
      "| 59    |  0.01965 |   0.01765 |   120.8      False\n",
      "| 60    |  0.01991 |   0.01726 |   122.8      False\n",
      "| 61    |  0.01976 |   0.01730 |   125.0      False\n",
      "| 62    |  0.01965 |   0.01731 |   126.9      False\n",
      "| 63    |  0.01967 |   0.01754 |   129.0      False\n",
      "| 64    |  0.01961 |   0.01724 |   131.0      False\n",
      "| 65    |  0.01957 |   0.01718 |   132.9      False\n",
      "| 66    |  0.01956 |   0.01743 |   134.9      False\n",
      "| 67    |  0.01959 |   0.01702 |   136.8      True\n",
      "| 68    |  0.01970 |   0.01895 |   139.0      False\n",
      "| 69    |  0.01954 |   0.01740 |   141.2      False\n",
      "| 70    |  0.01957 |   0.01728 |   143.1      False\n",
      "| 71    |  0.01949 |   0.01740 |   145.1      False\n",
      "| 72    |  0.01951 |   0.01746 |   147.0      False\n",
      "| 73    |  0.01946 |   0.01729 |   149.0      False\n",
      "| 74    |  0.01948 |   0.01747 |   151.1      False\n",
      "| 75    |  0.01960 |   0.01754 |   153.1      False\n",
      "| 76    |  0.01959 |   0.01734 |   155.0      False\n",
      "| 77    |  0.01951 |   0.01746 |   157.2      False\n",
      "| 78    |  0.01947 |   0.01716 |   159.2      False\n",
      "| 79    |  0.01946 |   0.01738 |   161.3      False\n",
      "| 80    |  0.01950 |   0.01745 |   163.3      False\n",
      "| 81    |  0.01947 |   0.01723 |   165.3      False\n",
      "| 82    |  0.01946 |   0.01738 |   167.2      False\n",
      "| 83    |  0.01949 |   0.01727 |   169.2      False\n",
      "| 84    |  0.01941 |   0.01720 |   171.5      False\n",
      "| 85    |  0.01943 |   0.01731 |   174.4      False\n",
      "| 86    |  0.01936 |   0.01799 |   176.4      False\n",
      "| 87    |  0.01945 |   0.01713 |   178.5      False\n",
      "| 88    |  0.01935 |   0.01757 |   180.5      False\n",
      "| 89    |  0.01951 |   0.01754 |   182.6      False\n",
      "| 90    |  0.01942 |   0.01986 |   184.8      False\n",
      "| 91    |  0.01983 |   0.01737 |   186.7      False\n",
      "| 92    |  0.01958 |   0.01718 |   188.8      False\n",
      "| 93    |  0.01940 |   0.01768 |   190.8      False\n",
      "| 94    |  0.01947 |   0.01740 |   192.8      False\n",
      "| 95    |  0.01966 |   0.01718 |   194.9      False\n",
      "| 96    |  0.01941 |   0.01729 |   196.9      False\n",
      "| 97    |  0.01940 |   0.01726 |   198.8      False\n",
      "| 98    |  0.01941 |   0.01756 |   200.7      False\n",
      "| 99    |  0.01944 |   0.01725 |   202.7      False\n",
      "| 100   |  0.01942 |   0.01716 |   205.1      False\n",
      "| 101   |  0.01931 |   0.01766 |   207.3      False\n",
      "| 102   |  0.01924 |   0.01719 |   209.3      False\n",
      "| 103   |  0.01919 |   0.01745 |   211.2      False\n",
      "| 104   |  0.01931 |   0.01780 |   213.2      False\n",
      "| 105   |  0.01936 |   0.01719 |   215.3      False\n",
      "| 106   |  0.01924 |   0.01731 |   217.4      False\n",
      "| 107   |  0.01932 |   0.01728 |   219.3      False\n",
      "| 108   |  0.01928 |   0.01765 |   221.5      False\n",
      "| 109   |  0.01931 |   0.01719 |   223.5      False\n",
      "| 110   |  0.01919 |   0.01721 |   225.4      False\n",
      "| 111   |  0.01930 |   0.01692 |   227.6      True\n",
      "| 112   |  0.01916 |   0.01737 |   229.6      False\n",
      "| 113   |  0.01918 |   0.01719 |   232.1      False\n",
      "| 114   |  0.01917 |   0.01713 |   234.2      False\n",
      "| 115   |  0.01917 |   0.01703 |   236.2      False\n",
      "| 116   |  0.01926 |   0.01709 |   238.7      False\n",
      "| 117   |  0.01921 |   0.01730 |   240.8      False\n",
      "| 118   |  0.01926 |   0.01721 |   242.8      False\n",
      "| 119   |  0.01935 |   0.01716 |   244.9      False\n",
      "| 120   |  0.01929 |   0.01707 |   246.9      False\n",
      "| 121   |  0.01937 |   0.01746 |   249.1      False\n",
      "| 122   |  0.01935 |   0.01759 |   251.1      False\n",
      "| 123   |  0.01926 |   0.01713 |   253.3      False\n",
      "| 124   |  0.01920 |   0.01721 |   255.3      False\n",
      "| 125   |  0.01923 |   0.01726 |   257.3      False\n",
      "| 126   |  0.01933 |   0.01711 |   259.4      False\n",
      "| 127   |  0.01917 |   0.01750 |   261.5      False\n",
      "| 128   |  0.01917 |   0.01729 |   263.5      False\n",
      "| 129   |  0.01923 |   0.01736 |   265.5      False\n",
      "| 130   |  0.01934 |   0.01727 |   267.5      False\n",
      "| 131   |  0.01925 |   0.01723 |   269.8      False\n",
      "| 132   |  0.01913 |   0.01756 |   272.0      False\n",
      "| 133   |  0.01910 |   0.01723 |   274.1      False\n",
      "| 134   |  0.01915 |   0.01743 |   276.3      False\n",
      "| 135   |  0.01925 |   0.01703 |   278.3      False\n",
      "| 136   |  0.01922 |   0.01844 |   280.3      False\n",
      "| 137   |  0.01929 |   0.01717 |   282.7      False\n",
      "| 138   |  0.01933 |   0.01715 |   284.9      False\n",
      "| 139   |  0.01924 |   0.01706 |   286.9      False\n",
      "| 140   |  0.01914 |   0.01708 |   289.0      False\n",
      "| 141   |  0.01923 |   0.01732 |   291.4      False\n",
      "| 142   |  0.01951 |   0.01709 |   293.6      False\n",
      "| 143   |  0.01928 |   0.01723 |   295.9      False\n",
      "| 144   |  0.01919 |   0.01755 |   297.9      False\n",
      "| 145   |  0.01914 |   0.01701 |   300.1      False\n",
      "| 146   |  0.01912 |   0.01704 |   302.4      False\n",
      "| 147   |  0.01922 |   0.01745 |   304.6      False\n",
      "| 148   |  0.01919 |   0.01695 |   306.6      False\n",
      "| 149   |  0.01921 |   0.01701 |   308.6      False\n",
      "| 150   |  0.01931 |   0.01744 |   310.7      False\n",
      "| 151   |  0.01915 |   0.01724 |   312.8      False\n",
      "| 152   |  0.01914 |   0.01720 |   315.0      False\n",
      "| 153   |  0.01912 |   0.01730 |   317.1      False\n",
      "| 154   |  0.01910 |   0.01719 |   319.1      False\n",
      "| 155   |  0.01917 |   0.01784 |   321.0      False\n",
      "| 156   |  0.01927 |   0.01792 |   323.0      False\n",
      "| 157   |  0.01903 |   0.01726 |   325.1      False\n",
      "| 158   |  0.01912 |   0.01721 |   327.4      False\n",
      "| 159   |  0.01903 |   0.01723 |   329.3      False\n",
      "| 160   |  0.01900 |   0.01714 |   331.3      False\n",
      "| 161   |  0.01895 |   0.01716 |   333.5      False\n",
      "Early stopping occured at epoch 161\n",
      "Training done in 333.536 seconds.\n",
      "---------------------------------------\n",
      "Successfully saved model at tabnet_raw_step1_fold8_0.zip\n",
      "validation fold 8 : 0.01692219076817177\n",
      "FOLDS :  9\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 50 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     |  0.39458 |   0.06118 |   2.3        True\n",
      "| 2     |  0.03263 |   0.02752 |   4.4        True\n",
      "| 3     |  0.02623 |   0.02304 |   6.4        True\n",
      "| 4     |  0.02473 |   0.02251 |   8.5        True\n",
      "| 5     |  0.02422 |   0.02164 |   10.5       True\n",
      "| 6     |  0.02410 |   0.02163 |   12.8       True\n",
      "| 7     |  0.02389 |   0.02143 |   15.2       True\n",
      "| 8     |  0.02357 |   0.02113 |   17.2       True\n",
      "| 9     |  0.02337 |   0.02088 |   19.3       True\n",
      "| 10    |  0.02302 |   0.02054 |   21.6       True\n",
      "| 11    |  0.02271 |   0.02050 |   23.7       True\n",
      "| 12    |  0.02244 |   0.02074 |   26.0       False\n",
      "| 13    |  0.02214 |   0.02061 |   28.0       False\n",
      "| 14    |  0.02184 |   0.02064 |   30.2       False\n",
      "| 15    |  0.02167 |   0.02065 |   32.3       False\n",
      "| 16    |  0.02143 |   0.01916 |   34.5       True\n",
      "| 17    |  0.02122 |   0.01904 |   36.5       True\n",
      "| 18    |  0.02109 |   0.01888 |   38.5       True\n",
      "| 19    |  0.02092 |   0.01883 |   40.5       True\n",
      "| 20    |  0.02078 |   0.02087 |   42.5       False\n",
      "| 21    |  0.02067 |   0.01865 |   44.9       True\n",
      "| 22    |  0.02062 |   0.02051 |   46.9       False\n",
      "| 23    |  0.02058 |   0.01844 |   48.9       True\n",
      "| 24    |  0.02046 |   0.01906 |   50.9       False\n",
      "| 25    |  0.02044 |   0.02064 |   52.8       False\n",
      "| 26    |  0.02032 |   0.01850 |   54.8       False\n",
      "| 27    |  0.02017 |   0.02056 |   57.0       False\n",
      "| 28    |  0.02016 |   0.02023 |   59.0       False\n",
      "| 29    |  0.02009 |   0.01861 |   61.0       False\n",
      "| 30    |  0.02002 |   0.01974 |   63.3       False\n",
      "| 31    |  0.01989 |   0.02019 |   65.3       False\n",
      "| 32    |  0.01987 |   0.02089 |   67.5       False\n",
      "| 33    |  0.01994 |   0.02294 |   69.4       False\n",
      "| 34    |  0.01976 |   0.01941 |   71.5       False\n",
      "| 35    |  0.01978 |   0.01985 |   73.5       False\n",
      "| 36    |  0.01965 |   0.01868 |   76.1       False\n",
      "| 37    |  0.01951 |   0.01820 |   78.1       True\n",
      "| 38    |  0.01958 |   0.02011 |   80.5       False\n",
      "| 39    |  0.01957 |   0.01825 |   82.6       False\n",
      "| 40    |  0.01951 |   0.01823 |   84.6       False\n",
      "| 41    |  0.01941 |   0.01821 |   86.6       False\n",
      "| 42    |  0.01953 |   0.01809 |   88.7       True\n",
      "| 43    |  0.01941 |   0.02018 |   90.9       False\n",
      "| 44    |  0.01948 |   0.01871 |   93.1       False\n",
      "| 45    |  0.01947 |   0.02006 |   95.3       False\n",
      "| 46    |  0.01950 |   0.01855 |   97.3       False\n",
      "| 47    |  0.01960 |   0.01867 |   99.3       False\n",
      "| 48    |  0.01951 |   0.01899 |   101.5      False\n",
      "| 49    |  0.01940 |   0.01978 |   103.5      False\n",
      "| 50    |  0.01944 |   0.01842 |   105.5      False\n",
      "| 51    |  0.01942 |   0.01815 |   107.7      False\n",
      "| 52    |  0.01932 |   0.01807 |   109.6      True\n",
      "| 53    |  0.01948 |   0.01815 |   111.9      False\n",
      "| 54    |  0.01937 |   0.01815 |   113.8      False\n",
      "| 55    |  0.01925 |   0.01840 |   115.8      False\n",
      "| 56    |  0.01926 |   0.01877 |   117.9      False\n",
      "| 57    |  0.01937 |   0.01810 |   119.8      False\n",
      "| 58    |  0.01937 |   0.01868 |   121.9      False\n",
      "| 59    |  0.01929 |   0.01843 |   124.0      False\n",
      "| 60    |  0.01931 |   0.01797 |   126.0      True\n",
      "| 61    |  0.01936 |   0.01811 |   128.3      False\n",
      "| 62    |  0.01935 |   0.01814 |   130.3      False\n",
      "| 63    |  0.01928 |   0.01803 |   132.3      False\n",
      "| 64    |  0.01929 |   0.01817 |   134.7      False\n",
      "| 65    |  0.01922 |   0.01823 |   136.8      False\n",
      "| 66    |  0.01922 |   0.01801 |   139.1      False\n",
      "| 67    |  0.01930 |   0.01832 |   141.3      False\n",
      "| 68    |  0.01933 |   0.01942 |   143.3      False\n",
      "| 69    |  0.01933 |   0.01822 |   145.5      False\n",
      "| 70    |  0.01926 |   0.01811 |   147.5      False\n",
      "| 71    |  0.01925 |   0.01831 |   149.5      False\n",
      "| 72    |  0.01928 |   0.01865 |   151.4      False\n",
      "| 73    |  0.01919 |   0.01802 |   153.4      False\n",
      "| 74    |  0.01924 |   0.01839 |   155.6      False\n",
      "| 75    |  0.01926 |   0.01843 |   157.6      False\n",
      "| 76    |  0.01915 |   0.01803 |   159.5      False\n",
      "| 77    |  0.01920 |   0.01817 |   161.8      False\n",
      "| 78    |  0.01920 |   0.01811 |   163.8      False\n",
      "| 79    |  0.01911 |   0.01817 |   165.8      False\n",
      "| 80    |  0.01914 |   0.01825 |   168.0      False\n",
      "| 81    |  0.01962 |   0.01820 |   170.2      False\n",
      "| 82    |  0.01947 |   0.01818 |   172.1      False\n",
      "| 83    |  0.01935 |   0.01833 |   174.1      False\n",
      "| 84    |  0.01916 |   0.01914 |   176.1      False\n",
      "| 85    |  0.01920 |   0.01792 |   178.3      True\n",
      "| 86    |  0.01918 |   0.01809 |   180.3      False\n",
      "| 87    |  0.01917 |   0.01809 |   182.2      False\n",
      "| 88    |  0.01937 |   0.01881 |   184.1      False\n",
      "| 89    |  0.01925 |   0.01819 |   186.1      False\n",
      "| 90    |  0.01918 |   0.02011 |   188.3      False\n",
      "| 91    |  0.01910 |   0.01811 |   190.2      False\n",
      "| 92    |  0.01921 |   0.01863 |   192.3      False\n",
      "| 93    |  0.01918 |   0.01851 |   194.8      False\n",
      "| 94    |  0.01909 |   0.01798 |   197.2      False\n",
      "| 95    |  0.01914 |   0.01823 |   199.3      False\n",
      "| 96    |  0.01917 |   0.01828 |   201.9      False\n",
      "| 97    |  0.01935 |   0.01829 |   203.9      False\n",
      "| 98    |  0.01919 |   0.01791 |   206.0      True\n",
      "| 99    |  0.01915 |   0.01833 |   207.9      False\n",
      "| 100   |  0.01921 |   0.01890 |   210.0      False\n",
      "| 101   |  0.01913 |   0.01845 |   212.1      False\n",
      "| 102   |  0.01901 |   0.01802 |   214.2      False\n",
      "| 103   |  0.01901 |   0.01808 |   216.1      False\n",
      "| 104   |  0.01896 |   0.01802 |   218.1      False\n",
      "| 105   |  0.01900 |   0.01810 |   220.1      False\n",
      "| 106   |  0.01899 |   0.01813 |   222.3      False\n",
      "| 107   |  0.01899 |   0.01846 |   224.3      False\n",
      "| 108   |  0.01897 |   0.01818 |   226.5      False\n",
      "| 109   |  0.01904 |   0.01815 |   228.6      False\n",
      "| 110   |  0.01903 |   0.01813 |   230.6      False\n",
      "| 111   |  0.01898 |   0.01795 |   232.7      False\n",
      "| 112   |  0.01902 |   0.01822 |   234.8      False\n",
      "| 113   |  0.01890 |   0.01821 |   236.9      False\n",
      "| 114   |  0.01902 |   0.01828 |   238.8      False\n",
      "| 115   |  0.01896 |   0.01805 |   240.8      False\n",
      "| 116   |  0.01902 |   0.01845 |   242.9      False\n",
      "| 117   |  0.01896 |   0.01808 |   245.0      False\n",
      "| 118   |  0.01887 |   0.01836 |   246.9      False\n",
      "| 119   |  0.01891 |   0.01819 |   248.9      False\n",
      "| 120   |  0.01899 |   0.01815 |   250.8      False\n",
      "| 121   |  0.01894 |   0.01837 |   252.8      False\n",
      "| 122   |  0.01907 |   0.01835 |   255.4      False\n",
      "| 123   |  0.01896 |   0.01798 |   257.4      False\n",
      "| 124   |  0.01887 |   0.01823 |   259.7      False\n",
      "| 125   |  0.01893 |   0.01828 |   262.3      False\n",
      "| 126   |  0.01895 |   0.01995 |   264.3      False\n",
      "| 127   |  0.01906 |   0.01832 |   267.0      False\n",
      "| 128   |  0.01914 |   0.01839 |   269.1      False\n",
      "| 129   |  0.01903 |   0.01823 |   271.0      False\n",
      "| 130   |  0.01903 |   0.01818 |   273.0      False\n",
      "| 131   |  0.01896 |   0.01790 |   275.1      True\n",
      "| 132   |  0.01891 |   0.01845 |   277.3      False\n",
      "| 133   |  0.01901 |   0.01801 |   279.2      False\n",
      "| 134   |  0.01891 |   0.01804 |   281.2      False\n",
      "| 135   |  0.01891 |   0.01814 |   283.2      False\n",
      "| 136   |  0.01902 |   0.01795 |   285.2      False\n",
      "| 137   |  0.01887 |   0.01811 |   287.4      False\n",
      "| 138   |  0.01879 |   0.01824 |   289.4      False\n",
      "| 139   |  0.01905 |   0.01813 |   291.4      False\n",
      "| 140   |  0.01907 |   0.01797 |   293.7      False\n",
      "| 141   |  0.01893 |   0.01861 |   295.7      False\n",
      "| 142   |  0.01889 |   0.01826 |   298.1      False\n",
      "| 143   |  0.01894 |   0.01835 |   300.1      False\n",
      "| 144   |  0.01896 |   0.01824 |   302.1      False\n",
      "| 145   |  0.01893 |   0.01820 |   304.0      False\n",
      "| 146   |  0.01886 |   0.01806 |   306.0      False\n",
      "| 147   |  0.01877 |   0.01853 |   308.0      False\n",
      "| 148   |  0.01890 |   0.01825 |   310.2      False\n",
      "| 149   |  0.01905 |   0.01813 |   312.2      False\n",
      "| 150   |  0.01893 |   0.01827 |   314.1      False\n",
      "| 151   |  0.01899 |   0.01789 |   316.5      True\n",
      "| 152   |  0.01887 |   0.01834 |   318.6      False\n",
      "| 153   |  0.01875 |   0.01820 |   321.0      False\n",
      "| 154   |  0.01875 |   0.01814 |   323.2      False\n",
      "| 155   |  0.01879 |   0.01872 |   325.5      False\n",
      "| 156   |  0.01881 |   0.01810 |   327.6      False\n",
      "| 157   |  0.01872 |   0.01842 |   329.9      False\n",
      "| 158   |  0.01876 |   0.01863 |   332.2      False\n",
      "| 159   |  0.01873 |   0.01873 |   334.1      False\n",
      "| 160   |  0.01870 |   0.01816 |   336.1      False\n",
      "| 161   |  0.01881 |   0.01838 |   338.2      False\n",
      "| 162   |  0.01874 |   0.01809 |   340.1      False\n",
      "| 163   |  0.01870 |   0.01807 |   342.3      False\n",
      "| 164   |  0.01876 |   0.01880 |   344.3      False\n",
      "| 165   |  0.01880 |   0.01820 |   346.2      False\n",
      "| 166   |  0.01877 |   0.01812 |   348.2      False\n",
      "| 167   |  0.01884 |   0.01823 |   350.2      False\n",
      "| 168   |  0.01883 |   0.01793 |   352.2      False\n",
      "| 169   |  0.01871 |   0.01816 |   354.3      False\n",
      "| 170   |  0.01884 |   0.01822 |   356.4      False\n",
      "| 171   |  0.01876 |   0.01846 |   358.7      False\n",
      "| 172   |  0.01871 |   0.01843 |   361.0      False\n",
      "| 173   |  0.01877 |   0.01835 |   363.0      False\n",
      "| 174   |  0.01893 |   0.01804 |   365.2      False\n",
      "| 175   |  0.01882 |   0.01830 |   367.2      False\n",
      "| 176   |  0.01874 |   0.01809 |   369.2      False\n",
      "| 177   |  0.01883 |   0.01838 |   371.2      False\n",
      "| 178   |  0.01890 |   0.01832 |   373.1      False\n",
      "| 179   |  0.01901 |   0.01812 |   375.9      False\n",
      "| 180   |  0.01886 |   0.01809 |   378.1      False\n",
      "| 181   |  0.01879 |   0.01823 |   380.1      False\n",
      "| 182   |  0.01881 |   0.01792 |   382.2      False\n",
      "| 183   |  0.01878 |   0.01812 |   384.2      False\n",
      "| 184   |  0.01885 |   0.01800 |   386.4      False\n",
      "| 185   |  0.01884 |   0.01839 |   388.6      False\n",
      "| 186   |  0.01881 |   0.01835 |   390.8      False\n",
      "| 187   |  0.01877 |   0.01816 |   393.0      False\n",
      "| 188   |  0.01876 |   0.01802 |   395.1      False\n",
      "| 189   |  0.01875 |   0.01815 |   397.2      False\n",
      "| 190   |  0.01878 |   0.01811 |   399.2      False\n",
      "| 191   |  0.01873 |   0.01794 |   401.3      False\n",
      "| 192   |  0.01867 |   0.01808 |   403.2      False\n",
      "| 193   |  0.01880 |   0.01826 |   405.2      False\n",
      "| 194   |  0.01886 |   0.01817 |   407.2      False\n",
      "| 195   |  0.01879 |   0.01803 |   409.3      False\n",
      "| 196   |  0.01870 |   0.01820 |   411.3      False\n",
      "| 197   |  0.01874 |   0.01831 |   413.3      False\n",
      "| 198   |  0.01893 |   0.01843 |   415.2      False\n",
      "| 199   |  0.01877 |   0.01809 |   417.2      False\n",
      "| 200   |  0.01868 |   0.01820 |   419.5      False\n",
      "Training done in 419.487 seconds.\n",
      "---------------------------------------\n",
      "Successfully saved model at tabnet_raw_step1_fold9_0.zip\n",
      "validation fold 9 : 0.017890024295888245\n"
     ]
    }
   ],
   "source": [
    "if cfg.strategy == \"KFOLD\":\n",
    "    oof_preds_all = []\n",
    "    oof_targets_all = []\n",
    "    scores_all =  []\n",
    "    scores_auc_all= []\n",
    "    preds_test = []\n",
    "    for seed in range(cfg.num_ensembling):\n",
    "        print(\"## SEED : \", seed)\n",
    "        #mskf = MultilabelStratifiedKFold(n_splits=cfg.SPLITS, random_state=cfg.seed+seed, shuffle=True)\n",
    "        oof_preds = []\n",
    "        oof_targets = []\n",
    "        scores = []\n",
    "        scores_auc = []\n",
    "        p = []\n",
    "        #for j, (train_idx, val_idx) in enumerate(mskf.split(np.zeros(len(cat_tr)), targets_tr)):\n",
    "        for j in range(cfg.SPLITS):\n",
    "            print(\"FOLDS : \", j)\n",
    "\n",
    "            ## model\n",
    "            X_train, y_train = torch.as_tensor(train_data[train_data['kfold'] != j].drop(columns = ['kfold']).values), torch.as_tensor(targets_tr[targets_tr['kfold'] != j].drop(columns = ['kfold']).values)\n",
    "            X_val, y_val = torch.as_tensor(train_data[train_data['kfold'] == j].drop(columns = ['kfold']).values), torch.as_tensor(targets_tr[targets_tr['kfold'] == j].drop(columns = ['kfold']).values)\n",
    "            model = TabNetRegressor(n_d=24, n_a=24, n_steps=1, gamma=1.3, lambda_sparse=0, cat_dims=cfg.cat_dims, cat_emb_dim=cfg.cat_emb_dim, cat_idxs=cfg.cats_idx, optimizer_fn=torch.optim.Adam,\n",
    "                                   optimizer_params=dict(lr=2e-2, weight_decay=1e-5), mask_type='entmax', device_name=cfg.device, scheduler_params=dict(milestones=[100,150], gamma=0.9), scheduler_fn=torch.optim.lr_scheduler.MultiStepLR)\n",
    "            #'sparsemax'\n",
    "            \n",
    "            model.fit(X_train=X_train, y_train=y_train, X_valid=X_val, y_valid=y_val,max_epochs=cfg.EPOCHS, patience=50, batch_size=1024, virtual_batch_size=128,\n",
    "                    num_workers=0, drop_last=False, loss_fn=torch.nn.functional.binary_cross_entropy_with_logits, label_smoothing = 0.001)\n",
    "            model.load_best_model()\n",
    "            preds = model.predict(X_val)\n",
    "            preds = torch.sigmoid(torch.as_tensor(preds)).detach().cpu().numpy()\n",
    "            score = log_loss_multi(y_val, preds)\n",
    "            name = cfg.save_name + f\"_fold{j}_{seed}\"\n",
    "            model.save_model(name)\n",
    "            ## preds on test\n",
    "            temp = model.predict(test_data)\n",
    "            p.append(torch.sigmoid(torch.as_tensor(temp)).detach().cpu().numpy())\n",
    "            ## save oof to compute the CV later\n",
    "            oof_preds.append(preds)\n",
    "            oof_targets.append(y_val)\n",
    "            scores.append(score)\n",
    "            scores_auc.append(auc_multi(y_val,preds))\n",
    "            print(f\"validation fold {j} : {score}\")\n",
    "            \n",
    "        p = np.stack(p)\n",
    "        preds_test.append(p)    \n",
    "        oof_preds_all.append(np.concatenate(oof_preds))\n",
    "        oof_targets_all.append(np.concatenate(oof_targets))\n",
    "        scores_all.append(np.array(scores))\n",
    "        scores_auc_all.append(np.array(scores_auc))\n",
    "    preds_test = np.stack(preds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T21:35:45.016461Z",
     "iopub.status.busy": "2020-11-26T21:35:45.015396Z",
     "iopub.status.idle": "2020-11-26T21:35:45.082929Z",
     "shell.execute_reply": "2020-11-26T21:35:45.083631Z"
    },
    "papermill": {
     "duration": 0.810603,
     "end_time": "2020-11-26T21:35:45.083851",
     "exception": false,
     "start_time": "2020-11-26T21:35:44.273248",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score fold :  0.017585692541827493\n",
      "auc mean :  0.457787884513296\n"
     ]
    }
   ],
   "source": [
    "if cfg.strategy == \"KFOLD\":\n",
    "\n",
    "    for i in range(cfg.num_ensembling): \n",
    "        print(\"CV score fold : \", log_loss_multi(oof_targets_all[i], oof_preds_all[i]))\n",
    "        print(\"auc mean : \", sum(scores_auc_all[i])/len(scores_auc_all[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T21:35:46.258284Z",
     "iopub.status.busy": "2020-11-26T21:35:46.257352Z",
     "iopub.status.idle": "2020-11-26T21:35:46.261238Z",
     "shell.execute_reply": "2020-11-26T21:35:46.261911Z"
    },
    "papermill": {
     "duration": 0.593036,
     "end_time": "2020-11-26T21:35:46.262050",
     "exception": false,
     "start_time": "2020-11-26T21:35:45.669014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.0177897 , 0.01783891, 0.01768235, 0.01735805, 0.01797898,\n",
      "       0.01793671, 0.01695549, 0.0175164 , 0.01692219, 0.01789002])]\n"
     ]
    }
   ],
   "source": [
    "print(scores_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T21:35:47.472257Z",
     "iopub.status.busy": "2020-11-26T21:35:47.471621Z",
     "iopub.status.idle": "2020-11-26T21:35:47.481708Z",
     "shell.execute_reply": "2020-11-26T21:35:47.481167Z"
    },
    "papermill": {
     "duration": 0.596277,
     "end_time": "2020-11-26T21:35:47.481822",
     "exception": false,
     "start_time": "2020-11-26T21:35:46.885545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabNetRegressor(cat_dims=[3, 2], cat_emb_dim=[1, 1], cat_idxs=[0, 1],\n",
      "                device_name='cuda', input_dim=874, lambda_sparse=0,\n",
      "                mask_type='entmax', n_a=24, n_d=24, n_steps=1,\n",
      "                optimizer_params={'lr': 0.02, 'weight_decay': 1e-05},\n",
      "                output_dim=206,\n",
      "                scheduler_fn=<class 'torch.optim.lr_scheduler.MultiStepLR'>,\n",
      "                scheduler_params={'gamma': 0.9, 'milestones': [100, 150]})\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.590067,
     "end_time": "2020-11-26T21:35:48.654813",
     "exception": false,
     "start_time": "2020-11-26T21:35:48.064746",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "p_min = 0.0005\n",
    "p_max = 0.9995\n",
    "\n",
    "submission[columns] = np.clip(preds_test.mean(1).mean(0), p_min, p_max)\n",
    "submission.loc[test_features['cp_type']=='ctl_vehicle', submission.columns[1:]] = 0\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 3528.403563,
   "end_time": "2020-11-26T21:35:49.759872",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-11-26T20:37:01.356309",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
