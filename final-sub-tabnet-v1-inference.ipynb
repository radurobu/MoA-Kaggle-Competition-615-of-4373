{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032034,
     "end_time": "2020-11-27T19:17:43.928055",
     "exception": false,
     "start_time": "2020-11-27T19:17:43.896021",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Source \n",
    "\n",
    "- Pytorch 1.6 : https://pytorch.org/docs/stable/\n",
    "- iterative-stratification : https://github.com/trent-b/iterative-stratification for stratified K fold multilabel\n",
    "- TabNet : https://arxiv.org/pdf/1908.07442.pdf https://github.com/dreamquark-ai/tabnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03287,
     "end_time": "2020-11-27T19:17:43.991393",
     "exception": false,
     "start_time": "2020-11-27T19:17:43.958523",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Approach :\n",
    "Inference script : \n",
    "https://www.kaggle.com/ludovick/introduction-to-tabnet-kfold-10-inference\n",
    "\n",
    "The Regressor class of TabNet is used as it allows multilabels outputs.\n",
    "- Stratified K Fold (10 folds) or shufflesplit\n",
    "- BCE Loss\n",
    "- TabNet Regressor Model\n",
    "- Supervised learning\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032996,
     "end_time": "2020-11-27T19:17:44.057151",
     "exception": false,
     "start_time": "2020-11-27T19:17:44.024155",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Radu Approach:\n",
    "* Label Smoothing: 0.001\n",
    "* Folds: 10\n",
    "* Patience: 50\n",
    "* Seed: 3\n",
    "* **REMOVED** Added Frequency Encoding Variables\n",
    "* **REMOVED** PCA for Frequency variables (10 components)\n",
    "* Removed Label Smoothing from vaidation inference\n",
    "* Added Min Max Clipping\n",
    "* **REMOVED** Added Extra_features (mean sum std etc.)\n",
    "* **REMOVED** Drop c62\n",
    "* **REMOVED** Added Kmeans Clustres\n",
    "* Stratified validation by drug id (Chris Deotte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-11-27T19:17:44.129662Z",
     "iopub.status.busy": "2020-11-27T19:17:44.128646Z",
     "iopub.status.idle": "2020-11-27T19:17:44.155762Z",
     "shell.execute_reply": "2020-11-27T19:17:44.156324Z"
    },
    "papermill": {
     "duration": 0.067099,
     "end_time": "2020-11-27T19:17:44.156513",
     "exception": false,
     "start_time": "2020-11-27T19:17:44.089414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/lish-moa/test_features.csv\n",
      "/kaggle/input/lish-moa/train_drug.csv\n",
      "/kaggle/input/lish-moa/train_features.csv\n",
      "/kaggle/input/lish-moa/train_targets_scored.csv\n",
      "/kaggle/input/lish-moa/train_targets_nonscored.csv\n",
      "/kaggle/input/lish-moa/sample_submission.csv\n",
      "/kaggle/input/final-sub-tabnet-v1-training/tabnet_raw_step1_fold2_0.zip\n",
      "/kaggle/input/final-sub-tabnet-v1-training/tabnet_raw_step1_fold0_0.zip\n",
      "/kaggle/input/final-sub-tabnet-v1-training/tabnet_raw_step1_fold7_0.zip\n",
      "/kaggle/input/final-sub-tabnet-v1-training/tabnet_raw_step1_fold6_0.zip\n",
      "/kaggle/input/final-sub-tabnet-v1-training/custom.css\n",
      "/kaggle/input/final-sub-tabnet-v1-training/__notebook__.ipynb\n",
      "/kaggle/input/final-sub-tabnet-v1-training/tabnet_raw_step1_fold1_0.zip\n",
      "/kaggle/input/final-sub-tabnet-v1-training/tabnet_raw_step1_fold9_0.zip\n",
      "/kaggle/input/final-sub-tabnet-v1-training/tabnet_raw_step1_fold3_0.zip\n",
      "/kaggle/input/final-sub-tabnet-v1-training/__results__.html\n",
      "/kaggle/input/final-sub-tabnet-v1-training/__output__.json\n",
      "/kaggle/input/final-sub-tabnet-v1-training/tabnet_raw_step1_fold5_0.zip\n",
      "/kaggle/input/final-sub-tabnet-v1-training/tabnet_raw_step1_fold4_0.zip\n",
      "/kaggle/input/final-sub-tabnet-v1-training/tabnet_raw_step1_fold8_0.zip\n",
      "/kaggle/input/pytorch16gpu/torch-1.6.0cu101-cp37-cp37m-linux_x86_64.whl\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/CHANGELOG.md\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/Dockerfile\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/renovate.json\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/.editorconfig\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/.gitignore\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/.gitatttributes\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/LICENSE\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/README.md\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/Dockerfile_gpu\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/regression_example.ipynb\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/pyproject.toml\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/forest_example.ipynb\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/.dockerignore\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/.flake8\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/census_example.ipynb\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/poetry.lock\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/multi_regression_example.ipynb\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/Makefile\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/.circleci/config.yml\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/pytorch_tabnet/tab_network.py\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/pytorch_tabnet/utils.py\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/pytorch_tabnet/sparsemax.py\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/pytorch_tabnet/tab_model.py\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/pytorch_tabnet/multiclass_utils.py\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/release-script/do-release.sh\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/release-script/prepare-release.sh\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/release-script/Dockerfile_changelog\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/.github/PULL_REQUEST_TEMPLATE.md\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/.github/ISSUE_TEMPLATE/bug_report.md\n",
      "/kaggle/input/tabnetdevelop/tabnet-develop/.github/ISSUE_TEMPLATE/feature_request.md\n",
      "/kaggle/input/iterative-stratification/iterative_stratification-0.1.6-py3-none-any.whl\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.031797,
     "end_time": "2020-11-27T19:17:44.221657",
     "exception": false,
     "start_time": "2020-11-27T19:17:44.189860",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Install package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-11-27T19:17:44.303082Z",
     "iopub.status.busy": "2020-11-27T19:17:44.288621Z",
     "iopub.status.idle": "2020-11-27T19:18:50.708608Z",
     "shell.execute_reply": "2020-11-27T19:18:50.707910Z"
    },
    "papermill": {
     "duration": 66.455163,
     "end_time": "2020-11-27T19:18:50.708726",
     "exception": false,
     "start_time": "2020-11-27T19:17:44.253563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/pytorch16gpu/torch-1.6.0cu101-cp37-cp37m-linux_x86_64.whl\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch==1.6.0cu101) (0.18.2)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch==1.6.0cu101) (1.18.5)\r\n",
      "Installing collected packages: torch\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 1.5.1\r\n",
      "    Uninstalling torch-1.5.1:\r\n",
      "      Successfully uninstalled torch-1.5.1\r\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\r\n",
      "\r\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\r\n",
      "\r\n",
      "kornia 0.3.2 requires torch<1.6.0,>=1.5.0, but you'll have torch 1.6.0+cu101 which is incompatible.\r\n",
      "allennlp 1.0.0 requires torch<1.6.0,>=1.5.0, but you'll have torch 1.6.0+cu101 which is incompatible.\u001b[0m\r\n",
      "Successfully installed torch-1.6.0+cu101\r\n"
     ]
    }
   ],
   "source": [
    "! pip install ../input/pytorch16gpu/torch-1.6.0cu101-cp37-cp37m-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-11-27T19:18:50.780538Z",
     "iopub.status.busy": "2020-11-27T19:18:50.779690Z",
     "iopub.status.idle": "2020-11-27T19:19:18.286856Z",
     "shell.execute_reply": "2020-11-27T19:19:18.285652Z"
    },
    "papermill": {
     "duration": 27.544725,
     "end_time": "2020-11-27T19:19:18.286995",
     "exception": false,
     "start_time": "2020-11-27T19:18:50.742270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/iterative-stratification/iterative_stratification-0.1.6-py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.6) (1.18.5)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.6) (0.23.2)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.6) (1.4.1)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->iterative-stratification==0.1.6) (0.14.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->iterative-stratification==0.1.6) (2.1.0)\r\n",
      "Installing collected packages: iterative-stratification\r\n",
      "Successfully installed iterative-stratification-0.1.6\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ../input/iterative-stratification/iterative_stratification-0.1.6-py3-none-any.whl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.035446,
     "end_time": "2020-11-27T19:19:18.359186",
     "exception": false,
     "start_time": "2020-11-27T19:19:18.323740",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "[](http://)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-11-27T19:19:18.436777Z",
     "iopub.status.busy": "2020-11-27T19:19:18.435831Z",
     "iopub.status.idle": "2020-11-27T19:19:18.438696Z",
     "shell.execute_reply": "2020-11-27T19:19:18.438062Z"
    },
    "papermill": {
     "duration": 0.043709,
     "end_time": "2020-11-27T19:19:18.438804",
     "exception": false,
     "start_time": "2020-11-27T19:19:18.395095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../input/tabnetdevelop/tabnet-develop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036102,
     "end_time": "2020-11-27T19:19:18.510401",
     "exception": false,
     "start_time": "2020-11-27T19:19:18.474299",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is TabNet ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.037183,
     "end_time": "2020-11-27T19:19:18.584193",
     "exception": false,
     "start_time": "2020-11-27T19:19:18.547010",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "TabNet is a Deep Neural Network for tabular data and was designed to learn in a similar way than decision tree based models, in order to have their benefits : ***interpretability*** and ***sparse feature selection***. TabNet uses ***sequential attention to choose which features to reason from at each decision step***, enabling interpretability and better learning (as the learning capacity is used for the most salient/important features). The feature selection is instancewise, so it can be different for each input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.05161,
     "end_time": "2020-11-27T19:19:18.674216",
     "exception": false,
     "start_time": "2020-11-27T19:19:18.622606",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "![Example of features selections with multiples steps](https://miro.medium.com/max/700/0*xGrnkDqPkDmC_MWS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.038781,
     "end_time": "2020-11-27T19:19:18.750254",
     "exception": false,
     "start_time": "2020-11-27T19:19:18.711473",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "TabNet can use *categoricals* and *numericals features*. There are no global normalization of the inputs, instead of that, a batch normalization is applied. The features obtained is then used at each step. Two parts can be identified at each step (fig a) :\n",
    "- **<font size=\"3\">the feature selection</font>** : the feature selection is based on a **learnable mask M[i]** which allows soft selection of the important feature. The mask is obtained with the help of **an attentive transformer** (fig d), using the processed features from the previous step a[i-1]. A sparsemax function is used to encourage the sparsity : \n",
    "**M[i] = sparsemax(P[i-1] . $h_i$ a[i-1])** where P[i] is the prior scale term and allow to control how much a feature has been used in previous steps; $h_i$ is a trainable function (FC layer) . (More details in the paper)\n",
    "- **<font size=\"3\">the feature processing</font>** : the features are processed with a feature transformer block (fig c) and then split for the decision step outputs and the information for the subsequence step, such as : \n",
    "**[ d[i], a[i]] = $f_i$ (M[i] . f)** with f the inputs data \"normalized\" by the BN, **M[i]** the learnable mask computed by the attentive transformer, $f_i$ the feature transformer block. **a[i]** is then used in the next step by the attentive transformer, **d[i]** is used to compute the overall decision embedding $d_{out}$, such as **$d_{out}$ = $\\sum_{i=0}^{steps}(ReLU(d[i])$** . \n",
    "\n",
    "The output is then computed such as : **Outputs = F( $W_{final}$ $d_{out}$ )** with *F activation function (softmax, sigmoid, etc) and $W_{final}$ a FC layer.*\n",
    "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F2234817%2Fa08525247089146ce7e1bfdcfe256a2f%2FArchitecture.PNG?generation=1590581898108061&alt=media)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.035888,
     "end_time": "2020-11-27T19:19:18.822987",
     "exception": false,
     "start_time": "2020-11-27T19:19:18.787099",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# For this competition : \n",
    "TabNet can have multiple advantages in this competition:\n",
    "- it can use gpu, so the training can be quite fast, if we limits the number of epochs and steps\n",
    "- it can use multilabel prediction, so only one model can be trained contrary to boosting methods.\n",
    "\n",
    "Even if the model does not give you the best results in CV/LB compared to others approaches, it can still be a good model to add in an ensembling because it is quite fast to train and can add different capabilities, so make the ensembling more exhaustif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-11-27T19:19:18.904950Z",
     "iopub.status.busy": "2020-11-27T19:19:18.904114Z",
     "iopub.status.idle": "2020-11-27T19:19:20.541266Z",
     "shell.execute_reply": "2020-11-27T19:19:20.541895Z"
    },
    "papermill": {
     "duration": 1.683256,
     "end_time": "2020-11-27T19:19:20.542033",
     "exception": false,
     "start_time": "2020-11-27T19:19:18.858777",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-27T19:19:20.656294Z",
     "iopub.status.busy": "2020-11-27T19:19:20.654494Z",
     "iopub.status.idle": "2020-11-27T19:19:21.102230Z",
     "shell.execute_reply": "2020-11-27T19:19:21.101238Z"
    },
    "papermill": {
     "duration": 0.508423,
     "end_time": "2020-11-27T19:19:21.102382",
     "exception": false,
     "start_time": "2020-11-27T19:19:20.593959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        \n",
    "seed_everything(62)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.038327,
     "end_time": "2020-11-27T19:19:21.179078",
     "exception": false,
     "start_time": "2020-11-27T19:19:21.140751",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-27T19:19:21.264603Z",
     "iopub.status.busy": "2020-11-27T19:19:21.263582Z",
     "iopub.status.idle": "2020-11-27T19:19:27.872494Z",
     "shell.execute_reply": "2020-11-27T19:19:27.871733Z"
    },
    "papermill": {
     "duration": 6.655936,
     "end_time": "2020-11-27T19:19:27.872631",
     "exception": false,
     "start_time": "2020-11-27T19:19:21.216695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/lish-moa/train_features.csv')\n",
    "train_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n",
    "train_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n",
    "test_features = pd.read_csv('../input/lish-moa/test_features.csv')\n",
    "submission = pd.read_csv('../input/lish-moa/sample_submission.csv')\n",
    "\n",
    "remove_vehicle = True\n",
    "\n",
    "if remove_vehicle:\n",
    "    train_features = train.loc[train['cp_type']=='trt_cp'].reset_index(drop=True)\n",
    "    train_targets_scored = train_targets_scored.loc[train['cp_type']=='trt_cp'].reset_index(drop=True)\n",
    "    train_targets_nonscored = train_targets_nonscored.loc[train['cp_type']=='trt_cp'].reset_index(drop=True)\n",
    "else:\n",
    "    train_features = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-27T19:19:28.002055Z",
     "iopub.status.busy": "2020-11-27T19:19:28.000981Z",
     "iopub.status.idle": "2020-11-27T19:19:28.074182Z",
     "shell.execute_reply": "2020-11-27T19:19:28.075000Z"
    },
    "papermill": {
     "duration": 0.137598,
     "end_time": "2020-11-27T19:19:28.075205",
     "exception": false,
     "start_time": "2020-11-27T19:19:27.937607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>cp_type</th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-1</th>\n",
       "      <th>g-2</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-5</th>\n",
       "      <th>...</th>\n",
       "      <th>c-90</th>\n",
       "      <th>c-91</th>\n",
       "      <th>c-92</th>\n",
       "      <th>c-93</th>\n",
       "      <th>c-94</th>\n",
       "      <th>c-95</th>\n",
       "      <th>c-96</th>\n",
       "      <th>c-97</th>\n",
       "      <th>c-98</th>\n",
       "      <th>c-99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_000644bb2</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D1</td>\n",
       "      <td>1.0620</td>\n",
       "      <td>0.5577</td>\n",
       "      <td>-0.2479</td>\n",
       "      <td>-0.6208</td>\n",
       "      <td>-0.1944</td>\n",
       "      <td>-1.0120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2862</td>\n",
       "      <td>0.2584</td>\n",
       "      <td>0.8076</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>-0.1912</td>\n",
       "      <td>0.6584</td>\n",
       "      <td>-0.3981</td>\n",
       "      <td>0.2139</td>\n",
       "      <td>0.3801</td>\n",
       "      <td>0.4176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_000779bfc</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>72</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.0743</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.2991</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>1.0190</td>\n",
       "      <td>0.5207</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4265</td>\n",
       "      <td>0.7543</td>\n",
       "      <td>0.4708</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.2957</td>\n",
       "      <td>0.4899</td>\n",
       "      <td>0.1522</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>0.6077</td>\n",
       "      <td>0.7371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_000a6266a</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>48</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.6280</td>\n",
       "      <td>0.5817</td>\n",
       "      <td>1.5540</td>\n",
       "      <td>-0.0764</td>\n",
       "      <td>-0.0323</td>\n",
       "      <td>1.2390</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.7250</td>\n",
       "      <td>-0.6297</td>\n",
       "      <td>0.6103</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>-1.3240</td>\n",
       "      <td>-0.3174</td>\n",
       "      <td>-0.6417</td>\n",
       "      <td>-0.2187</td>\n",
       "      <td>-1.4080</td>\n",
       "      <td>0.6931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_0015fd391</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>48</td>\n",
       "      <td>D1</td>\n",
       "      <td>-0.5138</td>\n",
       "      <td>-0.2491</td>\n",
       "      <td>-0.2656</td>\n",
       "      <td>0.5288</td>\n",
       "      <td>4.0620</td>\n",
       "      <td>-0.8095</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0990</td>\n",
       "      <td>-0.6441</td>\n",
       "      <td>-5.6300</td>\n",
       "      <td>-1.3780</td>\n",
       "      <td>-0.8632</td>\n",
       "      <td>-1.2880</td>\n",
       "      <td>-1.6210</td>\n",
       "      <td>-0.8784</td>\n",
       "      <td>-0.3876</td>\n",
       "      <td>-0.8154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_001626bd3</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>72</td>\n",
       "      <td>D2</td>\n",
       "      <td>-0.3254</td>\n",
       "      <td>-0.4009</td>\n",
       "      <td>0.9700</td>\n",
       "      <td>0.6919</td>\n",
       "      <td>1.4180</td>\n",
       "      <td>-0.8244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.6670</td>\n",
       "      <td>1.0690</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>-0.3031</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>0.2885</td>\n",
       "      <td>-0.3786</td>\n",
       "      <td>0.7125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21943</th>\n",
       "      <td>id_fff8c2444</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>72</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.1608</td>\n",
       "      <td>-1.0500</td>\n",
       "      <td>0.2551</td>\n",
       "      <td>-0.2239</td>\n",
       "      <td>-0.2431</td>\n",
       "      <td>0.4256</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0789</td>\n",
       "      <td>0.3538</td>\n",
       "      <td>0.0558</td>\n",
       "      <td>0.3377</td>\n",
       "      <td>-0.4753</td>\n",
       "      <td>-0.2504</td>\n",
       "      <td>-0.7415</td>\n",
       "      <td>0.8413</td>\n",
       "      <td>-0.4259</td>\n",
       "      <td>0.2434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21944</th>\n",
       "      <td>id_fffb1ceed</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D2</td>\n",
       "      <td>0.1394</td>\n",
       "      <td>-0.0636</td>\n",
       "      <td>-0.1112</td>\n",
       "      <td>-0.5080</td>\n",
       "      <td>-0.4713</td>\n",
       "      <td>0.7201</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1969</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>-0.8121</td>\n",
       "      <td>0.3434</td>\n",
       "      <td>0.5372</td>\n",
       "      <td>-0.3246</td>\n",
       "      <td>0.0631</td>\n",
       "      <td>0.9171</td>\n",
       "      <td>0.5258</td>\n",
       "      <td>0.4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21945</th>\n",
       "      <td>id_fffb70c0c</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D2</td>\n",
       "      <td>-1.3260</td>\n",
       "      <td>0.3478</td>\n",
       "      <td>-0.3743</td>\n",
       "      <td>0.9905</td>\n",
       "      <td>-0.7178</td>\n",
       "      <td>0.6621</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4286</td>\n",
       "      <td>0.4426</td>\n",
       "      <td>0.0423</td>\n",
       "      <td>-0.3195</td>\n",
       "      <td>-0.8086</td>\n",
       "      <td>-0.9798</td>\n",
       "      <td>-0.2084</td>\n",
       "      <td>-0.1224</td>\n",
       "      <td>-0.2715</td>\n",
       "      <td>0.3689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21946</th>\n",
       "      <td>id_fffcb9e7c</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.6660</td>\n",
       "      <td>0.2324</td>\n",
       "      <td>0.4392</td>\n",
       "      <td>0.2044</td>\n",
       "      <td>0.8531</td>\n",
       "      <td>-0.0343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1105</td>\n",
       "      <td>0.4258</td>\n",
       "      <td>-0.2012</td>\n",
       "      <td>0.1506</td>\n",
       "      <td>1.5230</td>\n",
       "      <td>0.7101</td>\n",
       "      <td>0.1732</td>\n",
       "      <td>0.7015</td>\n",
       "      <td>-0.6290</td>\n",
       "      <td>0.0740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21947</th>\n",
       "      <td>id_ffffdd77b</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>72</td>\n",
       "      <td>D1</td>\n",
       "      <td>-0.8598</td>\n",
       "      <td>1.0240</td>\n",
       "      <td>-0.1361</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>-0.3611</td>\n",
       "      <td>-3.6750</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.3890</td>\n",
       "      <td>-1.7450</td>\n",
       "      <td>-6.6300</td>\n",
       "      <td>-4.0950</td>\n",
       "      <td>-7.3860</td>\n",
       "      <td>-1.4160</td>\n",
       "      <td>-3.5770</td>\n",
       "      <td>-0.4775</td>\n",
       "      <td>-2.1500</td>\n",
       "      <td>-4.2520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21948 rows × 876 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             sig_id cp_type  cp_time cp_dose     g-0     g-1     g-2     g-3  \\\n",
       "0      id_000644bb2  trt_cp       24      D1  1.0620  0.5577 -0.2479 -0.6208   \n",
       "1      id_000779bfc  trt_cp       72      D1  0.0743  0.4087  0.2991  0.0604   \n",
       "2      id_000a6266a  trt_cp       48      D1  0.6280  0.5817  1.5540 -0.0764   \n",
       "3      id_0015fd391  trt_cp       48      D1 -0.5138 -0.2491 -0.2656  0.5288   \n",
       "4      id_001626bd3  trt_cp       72      D2 -0.3254 -0.4009  0.9700  0.6919   \n",
       "...             ...     ...      ...     ...     ...     ...     ...     ...   \n",
       "21943  id_fff8c2444  trt_cp       72      D1  0.1608 -1.0500  0.2551 -0.2239   \n",
       "21944  id_fffb1ceed  trt_cp       24      D2  0.1394 -0.0636 -0.1112 -0.5080   \n",
       "21945  id_fffb70c0c  trt_cp       24      D2 -1.3260  0.3478 -0.3743  0.9905   \n",
       "21946  id_fffcb9e7c  trt_cp       24      D1  0.6660  0.2324  0.4392  0.2044   \n",
       "21947  id_ffffdd77b  trt_cp       72      D1 -0.8598  1.0240 -0.1361  0.7952   \n",
       "\n",
       "          g-4     g-5  ...    c-90    c-91    c-92    c-93    c-94    c-95  \\\n",
       "0     -0.1944 -1.0120  ...  0.2862  0.2584  0.8076  0.5523 -0.1912  0.6584   \n",
       "1      1.0190  0.5207  ... -0.4265  0.7543  0.4708  0.0230  0.2957  0.4899   \n",
       "2     -0.0323  1.2390  ... -0.7250 -0.6297  0.6103  0.0223 -1.3240 -0.3174   \n",
       "3      4.0620 -0.8095  ... -2.0990 -0.6441 -5.6300 -1.3780 -0.8632 -1.2880   \n",
       "4      1.4180 -0.8244  ...  0.0042  0.0048  0.6670  1.0690  0.5523 -0.3031   \n",
       "...       ...     ...  ...     ...     ...     ...     ...     ...     ...   \n",
       "21943 -0.2431  0.4256  ...  0.0789  0.3538  0.0558  0.3377 -0.4753 -0.2504   \n",
       "21944 -0.4713  0.7201  ...  0.1969  0.0262 -0.8121  0.3434  0.5372 -0.3246   \n",
       "21945 -0.7178  0.6621  ...  0.4286  0.4426  0.0423 -0.3195 -0.8086 -0.9798   \n",
       "21946  0.8531 -0.0343  ... -0.1105  0.4258 -0.2012  0.1506  1.5230  0.7101   \n",
       "21947 -0.3611 -3.6750  ... -3.3890 -1.7450 -6.6300 -4.0950 -7.3860 -1.4160   \n",
       "\n",
       "         c-96    c-97    c-98    c-99  \n",
       "0     -0.3981  0.2139  0.3801  0.4176  \n",
       "1      0.1522  0.1241  0.6077  0.7371  \n",
       "2     -0.6417 -0.2187 -1.4080  0.6931  \n",
       "3     -1.6210 -0.8784 -0.3876 -0.8154  \n",
       "4      0.1094  0.2885 -0.3786  0.7125  \n",
       "...       ...     ...     ...     ...  \n",
       "21943 -0.7415  0.8413 -0.4259  0.2434  \n",
       "21944  0.0631  0.9171  0.5258  0.4680  \n",
       "21945 -0.2084 -0.1224 -0.2715  0.3689  \n",
       "21946  0.1732  0.7015 -0.6290  0.0740  \n",
       "21947 -3.5770 -0.4775 -2.1500 -4.2520  \n",
       "\n",
       "[21948 rows x 876 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-27T19:19:28.208816Z",
     "iopub.status.busy": "2020-11-27T19:19:28.207921Z",
     "iopub.status.idle": "2020-11-27T19:19:28.806602Z",
     "shell.execute_reply": "2020-11-27T19:19:28.807143Z"
    },
    "papermill": {
     "duration": 0.669435,
     "end_time": "2020-11-27T19:19:28.807303",
     "exception": false,
     "start_time": "2020-11-27T19:19:28.137868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove 0 columns\n",
      "remove 71 columns\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ratio for each label\n",
    "\n",
    "def get_ratio_labels(df):\n",
    "    columns = list(df.columns)\n",
    "    columns.pop(0)\n",
    "    ratios = []\n",
    "    toremove = []\n",
    "    for c in columns:\n",
    "        counts = df[c].value_counts()\n",
    "        if len(counts) != 1:\n",
    "            ratios.append(counts[0]/counts[1])\n",
    "        else:\n",
    "            toremove.append(c)\n",
    "    print(f\"remove {len(toremove)} columns\")\n",
    "    \n",
    "    for t in toremove:\n",
    "        columns.remove(t)\n",
    "    return columns, np.array(ratios).astype(np.int32)\n",
    "\n",
    "columns, ratios = get_ratio_labels(train_targets_scored)\n",
    "columns_nonscored, ratios_nonscored = get_ratio_labels(train_targets_nonscored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-27T19:19:28.891085Z",
     "iopub.status.busy": "2020-11-27T19:19:28.890465Z",
     "iopub.status.idle": "2020-11-27T19:19:28.894647Z",
     "shell.execute_reply": "2020-11-27T19:19:28.895129Z"
    },
    "papermill": {
     "duration": 0.047437,
     "end_time": "2020-11-27T19:19:28.895245",
     "exception": false,
     "start_time": "2020-11-27T19:19:28.847808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "drop_c62=False\n",
    "if drop_c62:\n",
    "    train_features = train_features.drop(['c-62'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.172808,
     "end_time": "2020-11-27T19:19:29.131840",
     "exception": false,
     "start_time": "2020-11-27T19:19:28.959032",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-27T19:19:29.266227Z",
     "iopub.status.busy": "2020-11-27T19:19:29.263045Z",
     "iopub.status.idle": "2020-11-27T19:19:29.501995Z",
     "shell.execute_reply": "2020-11-27T19:19:29.503229Z"
    },
    "papermill": {
     "duration": 0.310623,
     "end_time": "2020-11-27T19:19:29.503448",
     "exception": false,
     "start_time": "2020-11-27T19:19:29.192825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "extra_features_=False\n",
    "add_clusters_= False\n",
    "\n",
    "def create_cluster(train, test, features, kind = 'g', n_clusters = 35):\n",
    "    train_ = train[features].copy()\n",
    "    test_ = test[features].copy()\n",
    "    data = pd.concat([train_, test_], axis = 0)\n",
    "    kmeans = KMeans(n_clusters = n_clusters, random_state = 62).fit(data)\n",
    "    train[f'clusters_{kind}'] = kmeans.labels_[:train.shape[0]]\n",
    "    test[f'clusters_{kind}'] = kmeans.labels_[train.shape[0]:]\n",
    "    #train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n",
    "    #test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n",
    "    return train, test\n",
    "    \n",
    "def transform_data(train, test, col, normalize=True, removed_vehicle=False, extra_features=False, add_clusters=False):\n",
    "    \"\"\"\n",
    "        the first 3 columns represents categories, the others numericals features\n",
    "    \"\"\"\n",
    "    mapping = {\"cp_type\":{\"trt_cp\": 0, \"ctl_vehicle\":1},\n",
    "               \"cp_time\":{48:0, 72:1, 24:2},\n",
    "               \"cp_dose\":{\"D1\":0, \"D2\":1}}\n",
    "    \n",
    "    if extra_features:\n",
    "        features_g = list(train.columns[4:774])\n",
    "        features_c = list(train.columns[776:876])\n",
    "\n",
    "        for df in train, test:\n",
    "            df['g_sum'] = df[features_g].sum(axis = 1)\n",
    "            df['g_mean'] = df[features_g].mean(axis = 1)\n",
    "            df['g_std'] = df[features_g].std(axis = 1)\n",
    "            df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n",
    "            df['g_skew'] = df[features_g].skew(axis = 1)\n",
    "            df['c_sum'] = df[features_c].sum(axis = 1)\n",
    "            df['c_mean'] = df[features_c].mean(axis = 1)\n",
    "            df['c_std'] = df[features_c].std(axis = 1)\n",
    "            df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n",
    "            df['c_skew'] = df[features_c].skew(axis = 1)\n",
    "            df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n",
    "            df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n",
    "            df['gc_std'] = df[features_g + features_c].std(axis = 1)\n",
    "            df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n",
    "            df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n",
    "        \n",
    "    if add_clusters:\n",
    "        features_g = list(train.columns[4:774])\n",
    "        features_c = list(train.columns[776:876])\n",
    "    \n",
    "        train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = 35)\n",
    "        train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = 5)\n",
    "        \n",
    "    \n",
    "    if removed_vehicle:\n",
    "        categories_tr = np.stack([ train[c].apply(lambda x: mapping[c][x]).values for c in col[1:3]], axis=1)\n",
    "        categories_test = np.stack([ test[c].apply(lambda x: mapping[c][x]).values for c in col[1:3]], axis=1)\n",
    "    else:\n",
    "        categories_tr = np.stack([ train[c].apply(lambda x: mapping[c][x]).values for c in col[:3]], axis=1)\n",
    "        categories_test = np.stack([ test[c].apply(lambda x: mapping[c][x]).values for c in col[:3]], axis=1)\n",
    "    if add_clusters:\n",
    "        categories_tr = np.append(categories_tr, train[['clusters_g', 'clusters_c']].values, axis=1)\n",
    "        categories_test = np.append(categories_test, test[['clusters_g', 'clusters_c']].values, axis=1)\n",
    "        \n",
    "    #categories_tr = np.append(categories_tr, train[['sig_id']].values, axis=1) \n",
    "    #categories_test = np.append(categories_test, test[['sig_id']].values, axis=1)\n",
    "    \n",
    "    max_ = 10.\n",
    "    min_ = -10.\n",
    "   \n",
    "    if removed_vehicle:\n",
    "        numerical_tr = train[col[3:]].values\n",
    "        numerical_test = test[col[3:]].values\n",
    "    else:\n",
    "        numerical_tr = train[col[3:]].values\n",
    "        numerical_test = test[col[3:]].values\n",
    "    if normalize:\n",
    "        numerical_tr = (numerical_tr-min_)/(max_ - min_)\n",
    "        numerical_test = (numerical_test-min_)/(max_ - min_)\n",
    "    return categories_tr, categories_test, numerical_tr, numerical_test\n",
    "if extra_features_:\n",
    "    col_features = list(train_features.columns)[1:] + ['g_sum', 'g_mean', 'g_std', 'g_kurt', 'g_skew', 'c_sum', 'c_mean', 'c_std', 'c_kurt', 'c_skew', 'gc_sum', 'gc_mean', 'gc_std', 'gc_kurt', 'gc_skew']  \n",
    "else:\n",
    "    col_features = list(train_features.columns)[1:]\n",
    "cat_tr, cat_test, numerical_tr, numerical_test = transform_data(train_features, test_features, col_features, normalize=False, removed_vehicle=remove_vehicle, extra_features=extra_features_, add_clusters=add_clusters_)\n",
    "targets_tr = train_targets_scored[columns].values.astype(np.float32)\n",
    "targets2_tr = train_targets_nonscored[columns_nonscored].values.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-27T19:19:29.632204Z",
     "iopub.status.busy": "2020-11-27T19:19:29.629068Z",
     "iopub.status.idle": "2020-11-27T19:19:29.770090Z",
     "shell.execute_reply": "2020-11-27T19:19:29.771162Z"
    },
    "papermill": {
     "duration": 0.210927,
     "end_time": "2020-11-27T19:19:29.771370",
     "exception": false,
     "start_time": "2020-11-27T19:19:29.560443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21948, 874)\n",
      "(3982, 874)\n"
     ]
    }
   ],
   "source": [
    "freq_encodeing = False\n",
    "if freq_encodeing:\n",
    "    df_numerical_tr = pd.DataFrame(numerical_tr)\n",
    "    df_numerical_test = pd.DataFrame(numerical_test)\n",
    "    df_numerical_count_tr = pd.DataFrame()\n",
    "    df_numerical_count_test = pd.DataFrame()\n",
    "    df_numerical_tr_map = pd.DataFrame()\n",
    "    df_numerical_test_map = pd.DataFrame()\n",
    "\n",
    "    def myround(x, prec=2, base=.05):\n",
    "      return round(base * round(float(x)/base),prec)\n",
    "\n",
    "    for col in df_numerical_tr.columns:\n",
    "        df_numerical_count_tr.loc[:,col] = df_numerical_tr.loc[:,col].apply(lambda x: myround(x)).value_counts()\n",
    "        df_numerical_tr_map.loc[:,col] = df_numerical_tr.loc[:,col].apply(lambda x: myround(x)).map(df_numerical_count_tr.loc[:,col])\n",
    "        df_numerical_count_test.loc[:,col] = df_numerical_test.loc[:,col].apply(lambda x: myround(x)).value_counts()\n",
    "        df_numerical_test_map.loc[:,col] = df_numerical_test.loc[:,col].apply(lambda x: myround(x)).map(df_numerical_count_test.loc[:,col])\n",
    "    df_numerical_tr_map = df_numerical_tr_map.fillna(0)\n",
    "    df_numerical_test_map = df_numerical_test_map.fillna(0)\n",
    "\n",
    "    train_data = np.concatenate([cat_tr, numerical_tr, df_numerical_tr_map.values], axis=1)\n",
    "    print(train_data.shape)\n",
    "    test_data = np.concatenate([cat_test, numerical_test, df_numerical_test_map.values], axis=1)\n",
    "    print(test_data.shape)\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    train_data[:,cat_tr.shape[1]:] = scaler.fit_transform(train_data[:,cat_tr.shape[1]:])\n",
    "    test_data[:,cat_tr.shape[1]:]= scaler.transform(test_data[:,cat_tr.shape[1]:])\n",
    "else:\n",
    "    train_data = np.concatenate([cat_tr, numerical_tr], axis=1)\n",
    "    print(train_data.shape)\n",
    "    test_data = np.concatenate([cat_test, numerical_test], axis=1)\n",
    "    print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-27T19:19:29.895633Z",
     "iopub.status.busy": "2020-11-27T19:19:29.893789Z",
     "iopub.status.idle": "2020-11-27T19:19:29.896581Z",
     "shell.execute_reply": "2020-11-27T19:19:29.897153Z"
    },
    "papermill": {
     "duration": 0.067409,
     "end_time": "2020-11-27T19:19:29.897276",
     "exception": false,
     "start_time": "2020-11-27T19:19:29.829867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Trans_PCA=False\n",
    "if Trans_PCA:\n",
    "    from sklearn.decomposition import PCA\n",
    "    n_comp = 10\n",
    "    PCAF = PCA(n_components=n_comp, random_state=62)\n",
    "    numerical_tr_PCA = PCAF.fit_transform(train_data[:,-df_numerical_tr_map.shape[1]:])\n",
    "    numerical_test_PCA = PCAF.transform(test_data[:,-df_numerical_tr_map.shape[1]:])\n",
    "    \n",
    "    train_data = train_data[:,:-df_numerical_tr_map.shape[1]]\n",
    "    test_data = test_data[:,:-df_numerical_tr_map.shape[1]]\n",
    "    \n",
    "    train_data = np.concatenate([train_data, numerical_tr_PCA], axis=1)\n",
    "    print(train_data.shape)\n",
    "    test_data = np.concatenate([test_data, numerical_test_PCA], axis=1)\n",
    "    print(test_data.shape)\n",
    "    print(PCAF.explained_variance_ratio_)\n",
    "    print(PCAF.explained_variance_ratio_.sum())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.040011,
     "end_time": "2020-11-27T19:19:29.978158",
     "exception": false,
     "start_time": "2020-11-27T19:19:29.938147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039744,
     "end_time": "2020-11-27T19:19:30.069479",
     "exception": false,
     "start_time": "2020-11-27T19:19:30.029735",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-27T19:19:30.153250Z",
     "iopub.status.busy": "2020-11-27T19:19:30.152625Z",
     "iopub.status.idle": "2020-11-27T19:19:30.157108Z",
     "shell.execute_reply": "2020-11-27T19:19:30.156585Z"
    },
    "papermill": {
     "duration": 0.048689,
     "end_time": "2020-11-27T19:19:30.157200",
     "exception": false,
     "start_time": "2020-11-27T19:19:30.108511",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "           \n",
    "def inference_fn(model, X ,verbose=True):\n",
    "    with torch.no_grad():\n",
    "        y_preds = model.predict( X )\n",
    "        y_preds = torch.sigmoid(torch.as_tensor(y_preds)).numpy()\n",
    "    return y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-27T19:19:30.267522Z",
     "iopub.status.busy": "2020-11-27T19:19:30.266405Z",
     "iopub.status.idle": "2020-11-27T19:19:30.268393Z",
     "shell.execute_reply": "2020-11-27T19:19:30.268928Z"
    },
    "papermill": {
     "duration": 0.057918,
     "end_time": "2020-11-27T19:19:30.269054",
     "exception": false,
     "start_time": "2020-11-27T19:19:30.211136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_loss_score(actual, predicted,  eps=1e-15):\n",
    "\n",
    "        \"\"\"\n",
    "        :param predicted:   The predicted probabilities as floats between 0-1\n",
    "        :param actual:      The binary labels. Either 0 or 1.\n",
    "        :param eps:         Log(0) is equal to infinity, so we need to offset our predicted values slightly by eps from 0 or 1\n",
    "        :return:            The logarithmic loss between between the predicted probability assigned to the possible outcomes for item i, and the actual outcome.\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        p1 = actual * np.log(predicted+eps)\n",
    "        p0 = (1-actual) * np.log(1-predicted+eps)\n",
    "        loss = p0 + p1\n",
    "\n",
    "        return -loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-27T19:19:30.364939Z",
     "iopub.status.busy": "2020-11-27T19:19:30.363238Z",
     "iopub.status.idle": "2020-11-27T19:19:30.365918Z",
     "shell.execute_reply": "2020-11-27T19:19:30.366523Z"
    },
    "papermill": {
     "duration": 0.054369,
     "end_time": "2020-11-27T19:19:30.366639",
     "exception": false,
     "start_time": "2020-11-27T19:19:30.312270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_loss_multi(y_true, y_pred):\n",
    "    M = y_true.shape[1]\n",
    "    results = np.zeros(M)\n",
    "    for i in range(M):\n",
    "        results[i] = log_loss_score(y_true[:,i], y_pred[:,i])\n",
    "    return results.mean()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-27T19:19:30.457266Z",
     "iopub.status.busy": "2020-11-27T19:19:30.455238Z",
     "iopub.status.idle": "2020-11-27T19:19:30.458037Z",
     "shell.execute_reply": "2020-11-27T19:19:30.458601Z"
    },
    "papermill": {
     "duration": 0.051857,
     "end_time": "2020-11-27T19:19:30.458749",
     "exception": false,
     "start_time": "2020-11-27T19:19:30.406892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_targets(targets):\n",
    "    ### check if targets are all binary in training set\n",
    "    \n",
    "    for i in range(targets.shape[1]):\n",
    "        if len(np.unique(targets[:,i])) != 2:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-27T19:19:30.547843Z",
     "iopub.status.busy": "2020-11-27T19:19:30.547030Z",
     "iopub.status.idle": "2020-11-27T19:19:30.551096Z",
     "shell.execute_reply": "2020-11-27T19:19:30.550585Z"
    },
    "papermill": {
     "duration": 0.051576,
     "end_time": "2020-11-27T19:19:30.551193",
     "exception": false,
     "start_time": "2020-11-27T19:19:30.499617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def auc_multi(y_true, y_pred):\n",
    "    M = y_true.shape[1]\n",
    "    results = np.zeros(M)\n",
    "    for i in range(M):\n",
    "        try:\n",
    "            results[i] = roc_auc_score(y_true[:,i], y_pred[:,i])\n",
    "        except:\n",
    "            pass\n",
    "    return results.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039383,
     "end_time": "2020-11-27T19:19:30.630672",
     "exception": false,
     "start_time": "2020-11-27T19:19:30.591289",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.038618,
     "end_time": "2020-11-27T19:19:30.708811",
     "exception": false,
     "start_time": "2020-11-27T19:19:30.670193",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Cross Entrpoy with Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-11-27T19:19:30.804820Z",
     "iopub.status.busy": "2020-11-27T19:19:30.799045Z",
     "iopub.status.idle": "2020-11-27T19:19:30.948027Z",
     "shell.execute_reply": "2020-11-27T19:19:30.946741Z"
    },
    "papermill": {
     "duration": 0.199026,
     "end_time": "2020-11-27T19:19:30.948155",
     "exception": false,
     "start_time": "2020-11-27T19:19:30.749129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## TABNET\n",
    "#from pytorch_tabnet.tab_model import TabModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.sparse import csc_matrix\n",
    "import time\n",
    "from abc import abstractmethod\n",
    "from pytorch_tabnet import tab_network\n",
    "from pytorch_tabnet.multiclass_utils import unique_labels\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, accuracy_score\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from pytorch_tabnet.utils import (PredictDataset,\n",
    "                                  create_dataloaders,\n",
    "                                  create_explain_matrix)\n",
    "from sklearn.base import BaseEstimator\n",
    "from torch.utils.data import DataLoader\n",
    "from copy import deepcopy\n",
    "import io\n",
    "import json\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "\n",
    "class TabModel(BaseEstimator):\n",
    "    def __init__(self, n_d=8, n_a=8, n_steps=3, gamma=1.3, cat_idxs=[], cat_dims=[], cat_emb_dim=1,\n",
    "                 n_independent=2, n_shared=2, epsilon=1e-15,  momentum=0.02,\n",
    "                 lambda_sparse=1e-3, seed=0,\n",
    "                 clip_value=1, verbose=1,\n",
    "                 optimizer_fn=torch.optim.Adam,\n",
    "                 optimizer_params=dict(lr=2e-2),\n",
    "                 scheduler_params=None, scheduler_fn=None,\n",
    "                 mask_type=\"sparsemax\",\n",
    "                 input_dim=None, output_dim=None,\n",
    "                 device_name='auto',\n",
    "                 label_smoothing = 0.001):\n",
    "        \"\"\" Class for TabNet model\n",
    "        Parameters\n",
    "        ----------\n",
    "            device_name: str\n",
    "                'cuda' if running on GPU, 'cpu' if not, 'auto' to autodetect\n",
    "        \"\"\"\n",
    "\n",
    "        self.n_d = n_d\n",
    "        self.n_a = n_a\n",
    "        self.n_steps = n_steps\n",
    "        self.gamma = gamma\n",
    "        self.cat_idxs = cat_idxs\n",
    "        self.cat_dims = cat_dims\n",
    "        self.cat_emb_dim = cat_emb_dim\n",
    "        self.n_independent = n_independent\n",
    "        self.n_shared = n_shared\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.lambda_sparse = lambda_sparse\n",
    "        self.clip_value = clip_value\n",
    "        self.verbose = verbose\n",
    "        self.optimizer_fn = optimizer_fn\n",
    "        self.optimizer_params = optimizer_params\n",
    "        self.device_name = device_name\n",
    "        self.scheduler_params = scheduler_params\n",
    "        self.scheduler_fn = scheduler_fn\n",
    "        self.mask_type = mask_type\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.batch_size = 1024\n",
    "        \n",
    "        #Adaugat de Radu:\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "        self.seed = seed\n",
    "        torch.manual_seed(self.seed)\n",
    "        # Defining device\n",
    "        if device_name == 'auto':\n",
    "            if torch.cuda.is_available():\n",
    "                device_name = 'cuda'\n",
    "            else:\n",
    "                device_name = 'cpu'\n",
    "        self.device = torch.device(device_name)\n",
    "        print(f\"Device used : {self.device}\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def construct_loaders(self, X_train, y_train, X_valid, y_valid,\n",
    "                          weights, batch_size, num_workers, drop_last):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        train_dataloader, valid_dataloader : torch.DataLoader, torch.DataLoader\n",
    "            Training and validation dataloaders\n",
    "        -------\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define construct_loaders to use this base class')\n",
    "\n",
    "    def init_network(\n",
    "                     self,\n",
    "                     input_dim,\n",
    "                     output_dim,\n",
    "                     n_d,\n",
    "                     n_a,\n",
    "                     n_steps,\n",
    "                     gamma,\n",
    "                     cat_idxs,\n",
    "                     cat_dims,\n",
    "                     cat_emb_dim,\n",
    "                     n_independent,\n",
    "                     n_shared,\n",
    "                     epsilon,\n",
    "                     virtual_batch_size,\n",
    "                     momentum,\n",
    "                     device_name,\n",
    "                     mask_type,\n",
    "                     ):\n",
    "        self.network = tab_network.TabNet(\n",
    "            input_dim,\n",
    "            output_dim,\n",
    "            n_d=n_d,\n",
    "            n_a=n_a,\n",
    "            n_steps=n_steps,\n",
    "            gamma=gamma,\n",
    "            cat_idxs=cat_idxs,\n",
    "            cat_dims=cat_dims,\n",
    "            cat_emb_dim=cat_emb_dim,\n",
    "            n_independent=n_independent,\n",
    "            n_shared=n_shared,\n",
    "            epsilon=epsilon,\n",
    "            virtual_batch_size=virtual_batch_size,\n",
    "            momentum=momentum,\n",
    "            device_name=device_name,\n",
    "            mask_type=mask_type).to(self.device)\n",
    "\n",
    "        self.reducing_matrix = create_explain_matrix(\n",
    "            self.network.input_dim,\n",
    "            self.network.cat_emb_dim,\n",
    "            self.network.cat_idxs,\n",
    "            self.network.post_embed_dim)\n",
    "\n",
    "    def fit(self, X_train, y_train, X_valid=None, y_valid=None, loss_fn=None,\n",
    "            weights=0, max_epochs=100, patience=10, batch_size=1024,\n",
    "            virtual_batch_size=128, num_workers=0, drop_last=False, label_smoothing=0.001):\n",
    "        \"\"\"Train a neural network stored in self.network\n",
    "        Using train_dataloader for training data and\n",
    "        valid_dataloader for validation.\n",
    "        Parameters\n",
    "        ----------\n",
    "            X_train: np.ndarray\n",
    "                Train set\n",
    "            y_train : np.array\n",
    "                Train targets\n",
    "            X_train: np.ndarray\n",
    "                Train set\n",
    "            y_train : np.array\n",
    "                Train targets\n",
    "            weights : bool or dictionnary\n",
    "                0 for no balancing\n",
    "                1 for automated balancing\n",
    "                dict for custom weights per class\n",
    "            max_epochs : int\n",
    "                Maximum number of epochs during training\n",
    "            patience : int\n",
    "                Number of consecutive non improving epoch before early stopping\n",
    "            batch_size : int\n",
    "                Training batch size\n",
    "            virtual_batch_size : int\n",
    "                Batch size for Ghost Batch Normalization (virtual_batch_size < batch_size)\n",
    "            num_workers : int\n",
    "                Number of workers used in torch.utils.data.DataLoader\n",
    "            drop_last : bool\n",
    "                Whether to drop last batch during training\n",
    "        \"\"\"\n",
    "        # update model name\n",
    "\n",
    "        self.update_fit_params(X_train, y_train, X_valid, y_valid, loss_fn,\n",
    "                               weights, max_epochs, patience, batch_size,\n",
    "                               virtual_batch_size, num_workers, drop_last, label_smoothing)\n",
    "\n",
    "        train_dataloader, valid_dataloader = self.construct_loaders(X_train,\n",
    "                                                                    y_train,\n",
    "                                                                    X_valid,\n",
    "                                                                    y_valid,\n",
    "                                                                    self.updated_weights,\n",
    "                                                                    self.batch_size,\n",
    "                                                                    self.num_workers,\n",
    "                                                                    self.drop_last)\n",
    "\n",
    "        self.init_network(\n",
    "            input_dim=self.input_dim,\n",
    "            output_dim=self.output_dim,\n",
    "            n_d=self.n_d,\n",
    "            n_a=self.n_a,\n",
    "            n_steps=self.n_steps,\n",
    "            gamma=self.gamma,\n",
    "            cat_idxs=self.cat_idxs,\n",
    "            cat_dims=self.cat_dims,\n",
    "            cat_emb_dim=self.cat_emb_dim,\n",
    "            n_independent=self.n_independent,\n",
    "            n_shared=self.n_shared,\n",
    "            epsilon=self.epsilon,\n",
    "            virtual_batch_size=self.virtual_batch_size,\n",
    "            momentum=self.momentum,\n",
    "            device_name=self.device_name,\n",
    "            mask_type=self.mask_type\n",
    "        )\n",
    "\n",
    "        self.optimizer = self.optimizer_fn(self.network.parameters(),\n",
    "                                           **self.optimizer_params)\n",
    "\n",
    "        if self.scheduler_fn:\n",
    "            self.scheduler = self.scheduler_fn(self.optimizer, **self.scheduler_params)\n",
    "        else:\n",
    "            self.scheduler = None\n",
    "\n",
    "        self.losses_train = []\n",
    "        self.losses_valid = []\n",
    "        self.learning_rates = []\n",
    "        self.metrics_train = []\n",
    "        self.metrics_valid = []\n",
    "\n",
    "        if self.verbose > 0:\n",
    "            print(\"Will train until validation stopping metric\",\n",
    "                  f\"hasn't improved in {self.patience} rounds.\")\n",
    "            msg_epoch = f'| EPOCH |  train  |   valid  | total time (s)'\n",
    "            print('---------------------------------------')\n",
    "            print(msg_epoch)\n",
    "\n",
    "        total_time = 0\n",
    "        while (self.epoch < self.max_epochs and\n",
    "               self.patience_counter < self.patience):\n",
    "            starting_time = time.time()\n",
    "            # updates learning rate history\n",
    "            self.learning_rates.append(self.optimizer.param_groups[-1][\"lr\"])\n",
    "\n",
    "            fit_metrics = self.fit_epoch(train_dataloader, valid_dataloader)\n",
    "\n",
    "            # leaving it here, may be used for callbacks later\n",
    "            self.losses_train.append(fit_metrics['train']['loss_avg'])\n",
    "            self.losses_valid.append(fit_metrics['valid']['total_loss'])\n",
    "            self.metrics_train.append(fit_metrics['train']['stopping_loss'])\n",
    "            self.metrics_valid.append(fit_metrics['valid']['stopping_loss'])\n",
    "\n",
    "            stopping_loss = fit_metrics['valid']['stopping_loss']\n",
    "            if stopping_loss < self.best_cost:\n",
    "                self.best_cost = stopping_loss\n",
    "                self.patience_counter = 0\n",
    "                # Saving model\n",
    "                self.best_network = deepcopy(self.network)\n",
    "                has_improved = True\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "                has_improved=False\n",
    "            self.epoch += 1\n",
    "            total_time += time.time() - starting_time\n",
    "            if self.verbose > 0:\n",
    "                if self.epoch % self.verbose == 0:\n",
    "                    separator = \"|\"\n",
    "                    msg_epoch = f\"| {self.epoch:<5} | \"\n",
    "                    msg_epoch += f\" {fit_metrics['train']['stopping_loss']:.5f}\"\n",
    "                    msg_epoch += f' {separator:<2} '\n",
    "                    msg_epoch += f\" {fit_metrics['valid']['stopping_loss']:.5f}\"\n",
    "                    msg_epoch += f' {separator:<2} '\n",
    "                    msg_epoch += f\" {np.round(total_time, 1):<10}\"\n",
    "                    msg_epoch += f\" {has_improved}\"\n",
    "                    print(msg_epoch)\n",
    "\n",
    "        if self.verbose > 0:\n",
    "            if self.patience_counter == self.patience:\n",
    "                print(f\"Early stopping occured at epoch {self.epoch}\")\n",
    "            print(f\"Training done in {total_time:.3f} seconds.\")\n",
    "            print('---------------------------------------')\n",
    "\n",
    "        self.history = {\"train\": {\"loss\": self.losses_train,\n",
    "                                  \"metric\": self.metrics_train,\n",
    "                                  \"lr\": self.learning_rates},\n",
    "                        \"valid\": {\"loss\": self.losses_valid,\n",
    "                                  \"metric\": self.metrics_valid}}\n",
    "        # load best models post training\n",
    "        self.load_best_model()\n",
    "\n",
    "        # compute feature importance once the best model is defined\n",
    "        self._compute_feature_importances(train_dataloader)\n",
    "\n",
    "    def save_model(self, path):\n",
    "        \"\"\"\n",
    "        Saving model with two distinct files.\n",
    "        \"\"\"\n",
    "        saved_params = {}\n",
    "        for key, val in self.get_params().items():\n",
    "            if isinstance(val, type):\n",
    "                # Don't save torch specific params\n",
    "                continue\n",
    "            else:\n",
    "                saved_params[key] = val\n",
    "\n",
    "        # Create folder\n",
    "        Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Save models params\n",
    "        with open(Path(path).joinpath(\"model_params.json\"), \"w\", encoding=\"utf8\") as f:\n",
    "            json.dump(saved_params, f)\n",
    "\n",
    "        # Save state_dict\n",
    "        torch.save(self.network.state_dict(), Path(path).joinpath(\"network.pt\"))\n",
    "        shutil.make_archive(path, 'zip', path)\n",
    "        shutil.rmtree(path)\n",
    "        print(f\"Successfully saved model at {path}.zip\")\n",
    "        return f\"{path}.zip\"\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "\n",
    "        try:\n",
    "            with zipfile.ZipFile(filepath) as z:\n",
    "                with z.open(\"model_params.json\") as f:\n",
    "                    loaded_params = json.load(f)\n",
    "                with z.open(\"network.pt\") as f:\n",
    "                    try:\n",
    "                        saved_state_dict = torch.load(f)\n",
    "                    except io.UnsupportedOperation:\n",
    "                        # In Python <3.7, the returned file object is not seekable (which at least\n",
    "                        # some versions of PyTorch require) - so we'll try buffering it in to a\n",
    "                        # BytesIO instead:\n",
    "                        saved_state_dict = torch.load(io.BytesIO(f.read()))\n",
    "        except KeyError:\n",
    "            raise KeyError(\"Your zip file is missing at least one component\")\n",
    "\n",
    "        self.__init__(**loaded_params)\n",
    "\n",
    "        self.init_network(\n",
    "            input_dim=self.input_dim,\n",
    "            output_dim=self.output_dim,\n",
    "            n_d=self.n_d,\n",
    "            n_a=self.n_a,\n",
    "            n_steps=self.n_steps,\n",
    "            gamma=self.gamma,\n",
    "            cat_idxs=self.cat_idxs,\n",
    "            cat_dims=self.cat_dims,\n",
    "            cat_emb_dim=self.cat_emb_dim,\n",
    "            n_independent=self.n_independent,\n",
    "            n_shared=self.n_shared,\n",
    "            epsilon=self.epsilon,\n",
    "            virtual_batch_size=1024,\n",
    "            momentum=self.momentum,\n",
    "            device_name=self.device_name,\n",
    "            mask_type=self.mask_type\n",
    "        )\n",
    "        self.network.load_state_dict(saved_state_dict)\n",
    "        self.network.eval()\n",
    "        return\n",
    "\n",
    "    def fit_epoch(self, train_dataloader, valid_dataloader):\n",
    "        \"\"\"\n",
    "        Evaluates and updates network for one epoch.\n",
    "        Parameters\n",
    "        ----------\n",
    "            train_dataloader: a :class: `torch.utils.data.Dataloader`\n",
    "                DataLoader with train set\n",
    "            valid_dataloader: a :class: `torch.utils.data.Dataloader`\n",
    "                DataLoader with valid set\n",
    "        \"\"\"\n",
    "        train_metrics = self.train_epoch(train_dataloader)\n",
    "        valid_metrics = self.predict_epoch(valid_dataloader)\n",
    "\n",
    "        fit_metrics = {'train': train_metrics,\n",
    "                       'valid': valid_metrics}\n",
    "\n",
    "        return fit_metrics\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_epoch(self, train_loader):\n",
    "        \"\"\"\n",
    "        Trains one epoch of the network in self.network\n",
    "        Parameters\n",
    "        ----------\n",
    "            train_loader: a :class: `torch.utils.data.Dataloader`\n",
    "                DataLoader with train set\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define train_epoch to use this base class')\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_batch(self, data, targets):\n",
    "        \"\"\"\n",
    "        Trains one batch of data\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.tensor`\n",
    "                Target data\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define train_batch to use this base class')\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict_epoch(self, loader):\n",
    "        \"\"\"\n",
    "        Validates one epoch of the network in self.network\n",
    "        Parameters\n",
    "        ----------\n",
    "            loader: a :class: `torch.utils.data.Dataloader`\n",
    "                    DataLoader with validation set\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define predict_epoch to use this base class')\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict_batch(self, data, targets):\n",
    "        \"\"\"\n",
    "        Make predictions on a batch (valid)\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            batch_outs: dict\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define predict_batch to use this base class')\n",
    "\n",
    "    def load_best_model(self):\n",
    "        if self.best_network is not None:\n",
    "            self.network = self.best_network\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on a batch (valid)\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            predictions: np.array\n",
    "                Predictions of the regression problem or the last class\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define predict to use this base class')\n",
    "\n",
    "    def explain(self, X):\n",
    "        \"\"\"\n",
    "        Return local explanation\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            M_explain: matrix\n",
    "                Importance per sample, per columns.\n",
    "            masks: matrix\n",
    "                Sparse matrix showing attention masks used by network.\n",
    "        \"\"\"\n",
    "        self.network.eval()\n",
    "\n",
    "        dataloader = DataLoader(PredictDataset(X),\n",
    "                                batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        for batch_nb, data in enumerate(dataloader):\n",
    "            data = data.to(self.device).float()\n",
    "\n",
    "            M_explain, masks = self.network.forward_masks(data)\n",
    "            for key, value in masks.items():\n",
    "                masks[key] = csc_matrix.dot(value.cpu().detach().numpy(),\n",
    "                                            self.reducing_matrix)\n",
    "\n",
    "            if batch_nb == 0:\n",
    "                res_explain = csc_matrix.dot(M_explain.cpu().detach().numpy(),\n",
    "                                             self.reducing_matrix)\n",
    "                res_masks = masks\n",
    "            else:\n",
    "                res_explain = np.vstack([res_explain,\n",
    "                                         csc_matrix.dot(M_explain.cpu().detach().numpy(),\n",
    "                                                        self.reducing_matrix)])\n",
    "                for key, value in masks.items():\n",
    "                    res_masks[key] = np.vstack([res_masks[key], value])\n",
    "        return res_explain, res_masks\n",
    "\n",
    "    def _compute_feature_importances(self, loader):\n",
    "        self.network.eval()\n",
    "        feature_importances_ = np.zeros((self.network.post_embed_dim))\n",
    "        for data, targets in loader:\n",
    "            data = data.to(self.device).float()\n",
    "            M_explain, masks = self.network.forward_masks(data)\n",
    "            feature_importances_ += M_explain.sum(dim=0).cpu().detach().numpy()\n",
    "\n",
    "        feature_importances_ = csc_matrix.dot(feature_importances_,\n",
    "                                              self.reducing_matrix)\n",
    "        self.feature_importances_ = feature_importances_ / np.sum(feature_importances_)\n",
    "        \n",
    "        \n",
    "class TabNetRegressor(TabModel):\n",
    "\n",
    "    def construct_loaders(self, X_train, y_train, X_valid, y_valid, weights,\n",
    "                          batch_size, num_workers, drop_last):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        train_dataloader, valid_dataloader : torch.DataLoader, torch.DataLoader\n",
    "            Training and validation dataloaders\n",
    "        -------\n",
    "        \"\"\"\n",
    "        if isinstance(weights, int):\n",
    "            if weights == 1:\n",
    "                raise ValueError(\"Please provide a list of weights for regression.\")\n",
    "        if isinstance(weights, dict):\n",
    "            raise ValueError(\"Please provide a list of weights for regression.\")\n",
    "\n",
    "        train_dataloader, valid_dataloader = create_dataloaders(X_train,\n",
    "                                                                y_train,\n",
    "                                                                X_valid,\n",
    "                                                                y_valid,\n",
    "                                                                weights,\n",
    "                                                                batch_size,\n",
    "                                                                num_workers,\n",
    "                                                                drop_last)\n",
    "        return train_dataloader, valid_dataloader\n",
    "\n",
    "    def update_fit_params(self, X_train, y_train, X_valid, y_valid, loss_fn,\n",
    "                          weights, max_epochs, patience,\n",
    "                          batch_size, virtual_batch_size, num_workers, drop_last,label_smoothing):\n",
    "\n",
    "        if loss_fn is None:\n",
    "            self.loss_fn = torch.nn.functional.mse_loss\n",
    "        else:\n",
    "            self.loss_fn = loss_fn\n",
    "\n",
    "        assert X_train.shape[1] == X_valid.shape[1], \"Dimension mismatch X_train X_valid\"\n",
    "        self.input_dim = X_train.shape[1]\n",
    "\n",
    "        if len(y_train.shape) == 1:\n",
    "            raise ValueError(\"\"\"Please apply reshape(-1, 1) to your targets\n",
    "                                if doing single regression.\"\"\")\n",
    "        assert y_train.shape[1] == y_valid.shape[1], \"Dimension mismatch y_train y_valid\"\n",
    "        self.output_dim = y_train.shape[1]\n",
    "\n",
    "        self.updated_weights = weights\n",
    "\n",
    "        self.max_epochs = max_epochs\n",
    "        self.patience = patience\n",
    "        self.batch_size = batch_size\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        # Initialize counters and histories.\n",
    "        self.patience_counter = 0\n",
    "        self.epoch = 0\n",
    "        self.best_cost = np.inf\n",
    "        self.num_workers = num_workers\n",
    "        self.drop_last = drop_last\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "    def train_epoch(self, train_loader):\n",
    "        \"\"\"\n",
    "        Trains one epoch of the network in self.network\n",
    "        Parameters\n",
    "        ----------\n",
    "            train_loader: a :class: `torch.utils.data.Dataloader`\n",
    "                DataLoader with train set\n",
    "        \"\"\"\n",
    "\n",
    "        self.network.train()\n",
    "        y_preds = []\n",
    "        ys = []\n",
    "        total_loss = 0\n",
    "\n",
    "        for data, targets in train_loader:\n",
    "            batch_outs = self.train_batch(data, targets)\n",
    "            y_preds.append(batch_outs[\"y_preds\"].cpu().detach().numpy())\n",
    "            ys.append(batch_outs[\"y\"].cpu().detach().numpy())\n",
    "            total_loss += batch_outs[\"loss\"]\n",
    "\n",
    "        y_preds = np.vstack(y_preds)\n",
    "        ys = np.vstack(ys)\n",
    "\n",
    "        #stopping_loss = mean_squared_error(y_true=ys, y_pred=y_preds)\n",
    "        stopping_loss =log_loss_multi(ys, torch.sigmoid(torch.as_tensor(y_preds)).numpy()  )\n",
    "        total_loss = total_loss / len(train_loader)\n",
    "        epoch_metrics = {'loss_avg': total_loss,\n",
    "                         'stopping_loss': total_loss,\n",
    "                         }\n",
    "\n",
    "        if self.scheduler is not None:\n",
    "            self.scheduler.step()\n",
    "        return epoch_metrics\n",
    "\n",
    "    def train_batch(self, data, targets):\n",
    "        \"\"\"\n",
    "        Trains one batch of data\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.tensor`\n",
    "                Target data\n",
    "        \"\"\"\n",
    "        self.network.train()\n",
    "        data = data.to(self.device).float()\n",
    "\n",
    "        targets = targets.to(self.device).float()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        output, M_loss = self.network(data)\n",
    "        \n",
    "        #Added by Radu:\n",
    "        y_smo = targets * (1 - self.label_smoothing) + 0.5 * self.label_smoothing\n",
    "\n",
    "        loss = self.loss_fn(output, y_smo)\n",
    "        \n",
    "        loss -= self.lambda_sparse*M_loss\n",
    "\n",
    "        loss.backward()\n",
    "        if self.clip_value:\n",
    "            clip_grad_norm_(self.network.parameters(), self.clip_value)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        loss_value = loss.item()\n",
    "        batch_outs = {'loss': loss_value,\n",
    "                      'y_preds': output,\n",
    "                      'y': targets}\n",
    "        return batch_outs\n",
    "\n",
    "    def predict_epoch(self, loader):\n",
    "        \"\"\"\n",
    "        Validates one epoch of the network in self.network\n",
    "        Parameters\n",
    "        ----------\n",
    "            loader: a :class: `torch.utils.data.Dataloader`\n",
    "                    DataLoader with validation set\n",
    "        \"\"\"\n",
    "        y_preds = []\n",
    "        ys = []\n",
    "        self.network.eval()\n",
    "        total_loss = 0\n",
    "\n",
    "        for data, targets in loader:\n",
    "            batch_outs = self.predict_batch(data, targets)\n",
    "            total_loss += batch_outs[\"loss\"]\n",
    "            y_preds.append(batch_outs[\"y_preds\"].cpu().detach().numpy())\n",
    "            ys.append(batch_outs[\"y\"].cpu().detach().numpy())\n",
    "\n",
    "        y_preds = np.vstack(y_preds)\n",
    "        ys = np.vstack(ys)\n",
    "\n",
    "        stopping_loss =log_loss_multi(ys, torch.sigmoid(torch.as_tensor(y_preds)).numpy()  ) #mean_squared_error(y_true=ys, y_pred=y_preds)\n",
    "\n",
    "        total_loss = total_loss / len(loader)\n",
    "        epoch_metrics = {'total_loss': total_loss,\n",
    "                         'stopping_loss': stopping_loss}\n",
    "\n",
    "        return epoch_metrics\n",
    "\n",
    "    def predict_batch(self, data, targets):\n",
    "        \"\"\"\n",
    "        Make predictions on a batch (valid)\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            batch_outs: dict\n",
    "        \"\"\"\n",
    "        self.network.eval()\n",
    "        data = data.to(self.device).float()\n",
    "        targets = targets.to(self.device).float()\n",
    "\n",
    "        output, M_loss = self.network(data)\n",
    "       \n",
    "        #y_smo = targets * (1 - self.label_smoothing) + 0.5 * self.label_smoothing\n",
    "        loss = self.loss_fn(output, targets)\n",
    "        #print(self.loss_fn, loss)\n",
    "        loss -= self.lambda_sparse*M_loss\n",
    "        #print(loss)\n",
    "        loss_value = loss.item()\n",
    "        batch_outs = {'loss': loss_value,\n",
    "                      'y_preds': output,\n",
    "                      'y': targets}\n",
    "        return batch_outs\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on a batch (valid)\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            predictions: np.array\n",
    "                Predictions of the regression problem\n",
    "        \"\"\"\n",
    "        self.network.eval()\n",
    "        dataloader = DataLoader(PredictDataset(X),\n",
    "                                batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        results = []\n",
    "        for batch_nb, data in enumerate(dataloader):\n",
    "            data = data.to(self.device).float()\n",
    "\n",
    "            output, M_loss = self.network(data)\n",
    "            predictions = output.cpu().detach().numpy()\n",
    "            results.append(predictions)\n",
    "        res = np.vstack(results)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.044054,
     "end_time": "2020-11-27T19:19:31.032558",
     "exception": false,
     "start_time": "2020-11-27T19:19:30.988504",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-27T19:19:31.121246Z",
     "iopub.status.busy": "2020-11-27T19:19:31.119858Z",
     "iopub.status.idle": "2020-11-27T19:19:31.123552Z",
     "shell.execute_reply": "2020-11-27T19:19:31.124066Z"
    },
    "papermill": {
     "duration": 0.05019,
     "end_time": "2020-11-27T19:19:31.124195",
     "exception": false,
     "start_time": "2020-11-27T19:19:31.074005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold, MultilabelStratifiedShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-27T19:19:31.216634Z",
     "iopub.status.busy": "2020-11-27T19:19:31.215075Z",
     "iopub.status.idle": "2020-11-27T19:19:31.220285Z",
     "shell.execute_reply": "2020-11-27T19:19:31.219559Z"
    },
    "papermill": {
     "duration": 0.055641,
     "end_time": "2020-11-27T19:19:31.220404",
     "exception": false,
     "start_time": "2020-11-27T19:19:31.164763",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        self.num_class = targets_tr.shape[1]\n",
    "        self.verbose=False\n",
    "        self.seed = 64\n",
    "        self.SPLITS= 10\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.EPOCHS = 200\n",
    "        self.num_ensembling = 1\n",
    "        # Parameters model\n",
    "        self.cat_emb_dim=[1] * cat_tr.shape[1] #to choose\n",
    "        self.cats_idx = list(range(cat_tr.shape[1]))\n",
    "        self.cat_dims = [len(np.unique(cat_tr[:, i])) for i in range(cat_tr.shape[1])]\n",
    "        #self.num_numericals= numerical_tr.shape[1]\n",
    "    \n",
    "        # save\n",
    "        self.save_name = \"../input/final-sub-tabnet-v1-training/tabnet_raw_step1\"\n",
    "        \n",
    "        self.strategy = \"KFOLD\" # \n",
    "cfg = Config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-27T19:19:31.306118Z",
     "iopub.status.busy": "2020-11-27T19:19:31.305229Z",
     "iopub.status.idle": "2020-11-27T19:19:31.307892Z",
     "shell.execute_reply": "2020-11-27T19:19:31.308429Z"
    },
    "papermill": {
     "duration": 0.047093,
     "end_time": "2020-11-27T19:19:31.308565",
     "exception": false,
     "start_time": "2020-11-27T19:19:31.261472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#X_test = np.concatenate([cat_test, numerical_test ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-27T19:19:31.422773Z",
     "iopub.status.busy": "2020-11-27T19:19:31.421246Z",
     "iopub.status.idle": "2020-11-27T19:19:32.196063Z",
     "shell.execute_reply": "2020-11-27T19:19:32.197322Z"
    },
    "papermill": {
     "duration": 0.843839,
     "end_time": "2020-11-27T19:19:32.197532",
     "exception": false,
     "start_time": "2020-11-27T19:19:31.353693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass shuffle=True, random_state=64 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass shuffle=True, random_state=64 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "SEED = 64\n",
    "NFOLDS = cfg.SPLITS\n",
    "\n",
    "# LOAD LIBRARIES (from PIP or Kaggle Dataset) \n",
    "from sklearn.model_selection import KFold\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "# LOAD FILES\n",
    "scored = train_targets_scored.copy()\n",
    "drug = pd.read_csv('/kaggle/input/lish-moa/train_drug.csv')\n",
    "targets = scored.columns[1:]\n",
    "scored = scored.merge(drug, on='sig_id', how='left') \n",
    "\n",
    "# LOCATE DRUGS\n",
    "vc = scored.drug_id.value_counts()\n",
    "vc1 = vc.loc[vc<=18].index.sort_values()\n",
    "vc2 = vc.loc[vc>18].index.sort_values()\n",
    "\n",
    "# STRATIFY DRUGS 18X OR LESS\n",
    "dct1 = {}; dct2 = {}\n",
    "skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, \n",
    "          random_state=SEED)\n",
    "tmp = scored.groupby('drug_id')[targets].mean().loc[vc1]\n",
    "for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n",
    "    dd = {k:fold for k in tmp.index[idxV].values}\n",
    "    dct1.update(dd)\n",
    "\n",
    "# STRATIFY DRUGS MORE THAN 18X\n",
    "skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, \n",
    "          random_state=SEED)\n",
    "tmp = scored.loc[scored.drug_id.isin(vc2)].reset_index(drop=True)\n",
    "for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n",
    "    dd = {k:fold for k in tmp.sig_id[idxV].values}\n",
    "    dct2.update(dd)\n",
    "\n",
    "# ASSIGN FOLDS\n",
    "scored['kfold'] = scored.drug_id.map(dct1)\n",
    "scored.loc[scored.kfold.isna(),'kfold'] = scored.loc[scored.kfold.isna(),'sig_id'].map(dct2)\n",
    "scored.kfold = scored.kfold.astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-27T19:19:32.327749Z",
     "iopub.status.busy": "2020-11-27T19:19:32.326891Z",
     "iopub.status.idle": "2020-11-27T19:19:32.330519Z",
     "shell.execute_reply": "2020-11-27T19:19:32.328616Z"
    },
    "papermill": {
     "duration": 0.071795,
     "end_time": "2020-11-27T19:19:32.330644",
     "exception": false,
     "start_time": "2020-11-27T19:19:32.258849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21948, 209)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scored.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-27T19:19:32.457622Z",
     "iopub.status.busy": "2020-11-27T19:19:32.456980Z",
     "iopub.status.idle": "2020-11-27T19:19:32.589194Z",
     "shell.execute_reply": "2020-11-27T19:19:32.588646Z"
    },
    "papermill": {
     "duration": 0.19962,
     "end_time": "2020-11-27T19:19:32.589306",
     "exception": false,
     "start_time": "2020-11-27T19:19:32.389686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = pd.merge(pd.DataFrame(train_data), scored[['kfold']], how='left', left_index=True, right_index=True)\n",
    "targets_tr = pd.merge(pd.DataFrame(targets_tr), scored[['kfold']], how='left', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-27T19:19:32.681620Z",
     "iopub.status.busy": "2020-11-27T19:19:32.680871Z",
     "iopub.status.idle": "2020-11-27T19:19:32.687435Z",
     "shell.execute_reply": "2020-11-27T19:19:32.686951Z"
    },
    "papermill": {
     "duration": 0.055278,
     "end_time": "2020-11-27T19:19:32.687532",
     "exception": false,
     "start_time": "2020-11-27T19:19:32.632254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for seed in range(2):\n",
    "    print(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-27T19:19:32.793264Z",
     "iopub.status.busy": "2020-11-27T19:19:32.778138Z",
     "iopub.status.idle": "2020-11-27T19:19:44.140191Z",
     "shell.execute_reply": "2020-11-27T19:19:44.139612Z"
    },
    "papermill": {
     "duration": 11.411736,
     "end_time": "2020-11-27T19:19:44.140357",
     "exception": false,
     "start_time": "2020-11-27T19:19:32.728621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## SEED :  0\n",
      "FOLDS :  0\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "validation fold 0 : 0.017789696181052844\n",
      "FOLDS :  1\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "validation fold 1 : 0.017838912915994494\n",
      "FOLDS :  2\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "validation fold 2 : 0.017682345672476656\n",
      "FOLDS :  3\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "validation fold 3 : 0.017358052611545414\n",
      "FOLDS :  4\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "validation fold 4 : 0.017978976226074948\n",
      "FOLDS :  5\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "validation fold 5 : 0.017936706252213008\n",
      "FOLDS :  6\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "validation fold 6 : 0.016955487434051533\n",
      "FOLDS :  7\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "validation fold 7 : 0.01751639734296969\n",
      "FOLDS :  8\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "validation fold 8 : 0.01692219076817177\n",
      "FOLDS :  9\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "validation fold 9 : 0.017890024295888245\n"
     ]
    }
   ],
   "source": [
    "if cfg.strategy == \"KFOLD\":\n",
    "    oof_preds_all = []\n",
    "    oof_targets_all = []\n",
    "    scores_all =  []\n",
    "    scores_auc_all= []\n",
    "    preds_test = []\n",
    "    for seed in range(cfg.num_ensembling):\n",
    "        print(\"## SEED : \", seed)\n",
    "        #mskf = MultilabelStratifiedKFold(n_splits=cfg.SPLITS, random_state=cfg.seed+seed, shuffle=True)\n",
    "        oof_preds = []\n",
    "        oof_targets = []\n",
    "        scores = []\n",
    "        scores_auc = []\n",
    "        p = []\n",
    "        #for j, (train_idx, val_idx) in enumerate(mskf.split(np.zeros(len(cat_tr)), targets_tr)):\n",
    "        for j in range(cfg.SPLITS):\n",
    "            print(\"FOLDS : \", j)\n",
    "\n",
    "            ## model\n",
    "            X_train, y_train = torch.as_tensor(train_data[train_data['kfold'] != j].drop(columns = ['kfold']).values), torch.as_tensor(targets_tr[targets_tr['kfold'] != j].drop(columns = ['kfold']).values)\n",
    "            X_val, y_val = torch.as_tensor(train_data[train_data['kfold'] == j].drop(columns = ['kfold']).values), torch.as_tensor(targets_tr[targets_tr['kfold'] == j].drop(columns = ['kfold']).values)\n",
    "            model = TabNetRegressor(n_d=24, n_a=24, n_steps=1, gamma=1.3, lambda_sparse=0, cat_dims=cfg.cat_dims, cat_emb_dim=cfg.cat_emb_dim, cat_idxs=cfg.cats_idx, optimizer_fn=torch.optim.Adam,\n",
    "                                   optimizer_params=dict(lr=2e-2, weight_decay=1e-5), mask_type='entmax', device_name=cfg.device, scheduler_params=dict(milestones=[100,150], gamma=0.9), scheduler_fn=torch.optim.lr_scheduler.MultiStepLR)\n",
    "            #'sparsemax'\n",
    "            \n",
    "            #model.fit(X_train=X_train, y_train=y_train, X_valid=X_val, y_valid=y_val,max_epochs=cfg.EPOCHS, patience=50, batch_size=1024, virtual_batch_size=128,\n",
    "                    #num_workers=0, drop_last=False, loss_fn=torch.nn.functional.binary_cross_entropy_with_logits, label_smoothing = 0.001)\n",
    "            #model.load_best_model()\n",
    "            \n",
    "            name = cfg.save_name + f\"_fold{j}_{seed}.zip\"\n",
    "            model.load_model(name)\n",
    "            \n",
    "            preds = model.predict(X_val)\n",
    "            preds = torch.sigmoid(torch.as_tensor(preds)).detach().cpu().numpy()\n",
    "            score = log_loss_multi(y_val, preds)\n",
    "            #name = cfg.save_name + f\"_fold{j}_{seed}\"\n",
    "            #model.save_model(name)\n",
    "            ## preds on test\n",
    "            temp = model.predict(test_data)\n",
    "            p.append(torch.sigmoid(torch.as_tensor(temp)).detach().cpu().numpy())\n",
    "            ## save oof to compute the CV later\n",
    "            oof_preds.append(preds)\n",
    "            oof_targets.append(y_val)\n",
    "            scores.append(score)\n",
    "            scores_auc.append(auc_multi(y_val,preds))\n",
    "            print(f\"validation fold {j} : {score}\")\n",
    "            \n",
    "        p = np.stack(p)\n",
    "        preds_test.append(p)    \n",
    "        oof_preds_all.append(np.concatenate(oof_preds))\n",
    "        oof_targets_all.append(np.concatenate(oof_targets))\n",
    "        scores_all.append(np.array(scores))\n",
    "        scores_auc_all.append(np.array(scores_auc))\n",
    "    preds_test = np.stack(preds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-27T19:19:44.246304Z",
     "iopub.status.busy": "2020-11-27T19:19:44.245141Z",
     "iopub.status.idle": "2020-11-27T19:19:44.320064Z",
     "shell.execute_reply": "2020-11-27T19:19:44.320573Z"
    },
    "papermill": {
     "duration": 0.130969,
     "end_time": "2020-11-27T19:19:44.320714",
     "exception": false,
     "start_time": "2020-11-27T19:19:44.189745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score fold :  0.017585692541827493\n",
      "auc mean :  0.457787884513296\n"
     ]
    }
   ],
   "source": [
    "if cfg.strategy == \"KFOLD\":\n",
    "\n",
    "    for i in range(cfg.num_ensembling): \n",
    "        print(\"CV score fold : \", log_loss_multi(oof_targets_all[i], oof_preds_all[i]))\n",
    "        print(\"auc mean : \", sum(scores_auc_all[i])/len(scores_auc_all[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-27T19:19:44.432263Z",
     "iopub.status.busy": "2020-11-27T19:19:44.430303Z",
     "iopub.status.idle": "2020-11-27T19:19:44.435969Z",
     "shell.execute_reply": "2020-11-27T19:19:44.434990Z"
    },
    "papermill": {
     "duration": 0.062164,
     "end_time": "2020-11-27T19:19:44.436134",
     "exception": false,
     "start_time": "2020-11-27T19:19:44.373970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.0177897 , 0.01783891, 0.01768235, 0.01735805, 0.01797898,\n",
      "       0.01793671, 0.01695549, 0.0175164 , 0.01692219, 0.01789002])]\n"
     ]
    }
   ],
   "source": [
    "print(scores_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-27T19:19:44.555121Z",
     "iopub.status.busy": "2020-11-27T19:19:44.554287Z",
     "iopub.status.idle": "2020-11-27T19:19:44.563689Z",
     "shell.execute_reply": "2020-11-27T19:19:44.564416Z"
    },
    "papermill": {
     "duration": 0.071301,
     "end_time": "2020-11-27T19:19:44.564564",
     "exception": false,
     "start_time": "2020-11-27T19:19:44.493263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabNetRegressor(cat_dims=[3, 2], cat_emb_dim=[1, 1], cat_idxs=[0, 1],\n",
      "                device_name='cuda', input_dim=874, lambda_sparse=0,\n",
      "                mask_type='entmax', n_a=24, n_d=24, n_steps=1,\n",
      "                optimizer_params={'lr': 0.02, 'weight_decay': 1e-05},\n",
      "                output_dim=206,\n",
      "                scheduler_params={'gamma': 0.9, 'milestones': [100, 150]})\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-27T19:19:44.684829Z",
     "iopub.status.busy": "2020-11-27T19:19:44.683523Z",
     "iopub.status.idle": "2020-11-27T19:19:47.280046Z",
     "shell.execute_reply": "2020-11-27T19:19:47.279495Z"
    },
    "papermill": {
     "duration": 2.659095,
     "end_time": "2020-11-27T19:19:47.280162",
     "exception": false,
     "start_time": "2020-11-27T19:19:44.621067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "p_min = 0.0005\n",
    "p_max = 0.9995\n",
    "\n",
    "submission[columns] = np.clip(preds_test.mean(1).mean(0), p_min, p_max)\n",
    "submission.loc[test_features['cp_type']=='ctl_vehicle', submission.columns[1:]] = 0\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 128.871236,
   "end_time": "2020-11-27T19:19:48.580833",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-11-27T19:17:39.709597",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
